[
  {
    "objectID": "research/chip-seq/chip-seq.html",
    "href": "research/chip-seq/chip-seq.html",
    "title": "Selecting ChIP-Seq Normalization Methods from the Perspective of their Technical Conditions",
    "section": "",
    "text": "Overview: Chromatin immunoprecipitation with high-throughput sequencing (ChIP-seq) provides insights into both the genomic location occupied by the protein of interest and the difference in DNA occupancy between experimental states. Given that ChIP-seq data are collected experimentally, an important step for determining regions with differential DNA occupancy between states is between-sample normalization. While between-sample normalization is crucial for downstream differential binding analysis, the technical conditions underlying between-sample normalization methods have yet to be examined for ChIP-seq. We identify three important technical conditions underlying ChIP-seq between-sample normalization methods: balanced differential DNA occupancy, equal total DNA occupancy, and equal background binding across states. To illustrate the importance of satisfying the selected normalization method’s technical conditions for downstream differential binding analysis, we simulate ChIP-seq read count data where different combinations of the technical conditions are violated. We then externally verify our simulation results using experimental data. Based on our findings, we suggest that researchers use their understanding of the ChIP-seq experiment at hand to guide their choice of between-sample normalization method. Alternatively, researchers can use a high-confidence peakset, which is the intersection of the differentially bound peaksets obtained from using different between-sample normalization methods. In our two experimental analyses, roughly half of the called peaks were called as differentially bound for every normalization method. High-confidence peaks are less sensitive to one’s choice of between-sample normalization method, and thus could be a more robust basis for identifying genomic regions with differential DNA occupancy between experimental states when there is uncertainty about which technical conditions are satisfied.\nFunded by Pomona College’s The Class of 1971 Summer Undergraduate Research Fund (Summer 2022) and the Kenneth Cooke Summer Research Fellowship (Summer 2024)."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Notebooks",
    "section": "",
    "text": "Common Outcome-Based Algorithmic Fairness Metrics – July 24, 2025 [still working on]\nData Analysis Exam Thoughts and Materials – June 9, 2025\nTidyTuesday Creations – January 30, 2025"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "PDF Version of Sara Colando’s CV"
  },
  {
    "objectID": "cv.html#education",
    "href": "cv.html#education",
    "title": "Curriculum Vitae",
    "section": "Education",
    "text": "Education\nPhD in Statistics 2024 – 2029 (Expected)Carnegie Mellon University, Pittsburgh, PA\nBA in Mathematics and Philosophy May 2024Pomona College, Claremont, CA\n\nGraduated magna cum laude (GPA: 3.99/4.00) and with departmental honors"
  },
  {
    "objectID": "cv.html#publications",
    "href": "cv.html#publications",
    "title": "Curriculum Vitae",
    "section": "Publications",
    "text": "Publications\nColando, S., Hardin, J. Philosophy as Integral to a Data Science Ethics Course, Journal of Statistics and Data Science Education, accepted, 2023. Preprint."
  },
  {
    "objectID": "cv.html#research-experience",
    "href": "cv.html#research-experience",
    "title": "Curriculum Vitae",
    "section": "Research Experience",
    "text": "Research Experience\nPomona College Department of Mathematics and StatisticsResearch Assistant to Dr. Johanna Hardin May 2022 – Aug 2025\n\nAnalyzed between-sample normalization techniques by their technical assumptions to improve differential binding analysis practices for ChIP-Seq and similarly structured types of high-throughput data\nCreated reproducible ChIP-Seq read count simulations using R, Bash, and high-performance computing to examine how violating normalization method assumptions impacts the false discovery rate and power of identifying genomic regions with true differential DNA occupancy.\n\nData Science Ethics Independent Project (Supervisor: Dr. Johanna Hardin) May – Oct 2023\n\nSynthesized resources and pedagogies of existing Data Science Ethics undergraduate courses to connect key ethical concepts to data science practices\nCreated a website to effectively communicate data science ethics concepts and relevant background information to a broad community of decision-makers\nAided in Pomona College’s development of an ethics course for the recently approved data science minor\n\nCarnegie Mellon University Department of Statistics and Data ScienceResearch Assistant to Dr. Ron Yurko May – Aug 2023\n\nPerforming mixture-model clustering analysis to discover trends in movement patterns for injured racehorses\nIdentifying horses who under-raced through residual analysis of a created expected race count model\nEngaged in 8 weeks of lectures about popular statistical analysis techniques, data engineering, and novel data science methodologies employed in sports analytics\n\nSelf-Directed ProjectData Analysis and Visualization CreatorJan 2021 – May 2024\n\nConducting exploratory data analysis in R to improve data visualization and communication skills every week using data provided by the “R for Data Science” Online Learning Community\nHelping students new to R become more familiar with the programming language and troubleshoot errors in 2-hour Pomona TidyTuesday workshop offered each week\n\nUniversity of California, San Diego Halıcıoğlu Data Science Institute Intelligence, Data, Ethics, and Society (IDEAS) Summer Institute Participant Aug 2023\n\nParticipated in workshops taught by experts in domains like data science, AI, philosophy, and law to improve critical and interdisciplinary thinking on data science and AI ethics topics\nCollaborated with three other students on a two-week research project, examining practical methods of implementing responsible data science and AI practices in a California wildfire resource allocation algorithm\n\nUniversity of Michigan School of Public HealthBig Data Summer Institute Research Assistant to Dr. Nikola Banovic June – Aug 2022\n\nCollaborated with three other students to implement a partially Bayesian neural network to quantify and communicate prediction uncertainty in a supervised brain tumor segmentation model via Python’s PyTorch and TensorFlow packages\nParticipated in 6 weeks of lectures about ethical data collection practice in healthcare, probability and statistical theory, and the ethical implications of using statistics and computer science methods in healthcare settings"
  },
  {
    "objectID": "cv.html#mentorship-experience",
    "href": "cv.html#mentorship-experience",
    "title": "Curriculum Vitae",
    "section": "Mentorship Experience",
    "text": "Mentorship Experience\nCarnegie Mellon Department of Statistics and Data Science Graduate Teaching Assistant Sep 2024– Present\n\n\n\nPomona College Department of Mathematics and Statistics Course Mentor and Grader Jan 2021 – May 2024\n\nLed 2-hour mentor sessions each week to help students with homework and course material for Statistical Linear Models (Spring 2024) Introduction to Biostatistics (Spring 2023) and Linear Algebra (Fall 2021)\nWorked with co-mentors and professors to foster inclusivity and collaboration within small and large group environments\nGraded problem sets each week for 30+ students, with personalized feedback, within the professor’s desired time frame for Statistical Linear Models (Spring 2024), Introduction to Biostatistics (Spring 2023), Calculus I (Spring 2022), and Linear Algebra (Spring and Fall 2021)\n\n1-2-1 Summer Bridge Program Teacher’s Assistant June – Sep 2021\n\nLed 1.5-hour meetings for three small cohorts, consisting of incoming Pomona College students, twice per week\nTaught mathematical concepts (e.g., logic, calculus, combinatorics, number theory) to incoming undergraduate students\nOrganized group bonding by preparing icebreakers and virtual games for 40+ students in the cohort\nProvided individualized feedback on cohort members’ submitted problem sets three times a week\n\nPomona College Center for Speaking, Writing, and the Image Course Writing Partner Aug 2023 - Present\n\nProviding meaningful writing feedback to students from diverse writing backgrounds in one-on-one and small group settings; attached writing partner for the Critical Inquiry Seminar (ID1): I Disagree (Fall 2023)\n\nPomona College Department of Biology Introductory Genetics w/ Lab Course Mentor and Grader Aug – Dec 2022\n\nAnswered student questions within lectures twice a week\nCo-hosted mentor sessions to help students with problem sets and course concepts\nCollaborated with the professors to ensure inclusivity, peer collaboration, and student learning were optimized within classroom and mentor session environments\nGraded 30+ problem sets each week and provided individualized feedback to each student"
  },
  {
    "objectID": "cv.html#leadership-and-service",
    "href": "cv.html#leadership-and-service",
    "title": "Curriculum Vitae",
    "section": "Leadership and Service",
    "text": "Leadership and Service\nCarnegie Mellon Department of Statistics and Data Science TeachStat Working Group, Mentorship Committee, Mentor for Undergraduates Aug 2024 – Present\nPomona College Department of Philosophy Search and Selection Committee Member Aug 2023 – Mar 2024\nPomona-Pitzer Athletics Varsity Women’s Cross Country Team Captain Aug – Nov 2023"
  },
  {
    "objectID": "cv.html#presentations",
    "href": "cv.html#presentations",
    "title": "Curriculum Vitae",
    "section": "Presentations",
    "text": "Presentations\n\nColando, S. & Franke, E., “Analyzing Statistics Students’ Writing Before and After the Emergence of Large Language Models” (poster), United States Conference on Teaching Statistics, July 2025\nFranke, E. & Colando, S. “Teaching Data Cleaning and Wrangling with R’s data.table Package” (poster), United States Conference on Teaching Statistics, July 2025\nColando, S. “Predicting Avoidance Ties within Social Networks (ADA Progress Update)”, Networkshop Research Group, Carnegie Mellon University, March 2025\nColando, S. & Franke E. “Computational Efficiency of R’s data.table Package”, StatBytes, Carnegie Mellon University, March 2025\nColando, S. & Hardin, J. “Philosophy within Data Science Ethics Courses”, CAUSE Webinar Series, October 2024\nColando, S., Pipping, J., Wilson K. “Clustering Race Horse Movement Profiles to Discover Trends in Injured Horses”, Carnegie Mellon Sports Analytics Conference 2023, November 2023\nColando, S. “Analyzing Data Science Ethics Pedagogies”, Claremont Center for Mathematical Sciences Poster Session, September 2023\nColando, S. “Analyzing Data Science Ethics Pedagogies”, Intensive Summer Experience Symposium, September 2023\nColando, S., Pipping, J., Wilson K. “Clustering Race Horse Movement Profiles to Discover Trends in Injured Horses”, Summer Undergraduate Research Experience (SURE) 2023 Project Showcase, July 2023\nColando, S. Panelist for “Pomona Funded Summer Experiences Informational Session”, Family Weekend, October 2022\nColando, S. “Analyzing ChIP-Seq Normalization Techniques through the Lens of their Biological Assumptions”, Intensive Summer Experience Symposium, September 2022\nColando, S. “Analyzing ChIP-Seq Normalization Techniques through the Lens of their Biological Assumptions”, Claremont Center for Mathematical Sciences Poster Session, September 2022\nChu, C., Colando, S., Nandi, D., Serrano, X. “Quantifying Uncertainty in a Tumor Segmentation Model”, A Symposium on Big Data, Human Health, and Statistics. July 2022"
  },
  {
    "objectID": "cv.html#honors-and-awards",
    "href": "cv.html#honors-and-awards",
    "title": "Curriculum Vitae",
    "section": "Honors and Awards",
    "text": "Honors and Awards\n\nPassed First-Year PhD Data Analysis Exam with Distinction, Statistics and Data Science Department, Carnegie Mellon University 2025\nBlair Nixon Award, Pomona College 2024\nBruce Jay Levy Senior Prize, Department of Mathematics and Statistics, Pomona College 2024\nW.T. Jones Prize in Philosophy, Department of Philosophy, Pomona College 2024\nPhi Beta Kappa, Southern California Chapter 2024\nSigma Xi, Claremont Colleges Chapter 2024\nPomona College Scholar 2020 – 2024\nCross Country and Track and Field Varsity Letter Recipient, Pomona-Pitzer Athletics 2020 – 2024"
  },
  {
    "objectID": "cv.html#computer-and-language-skills",
    "href": "cv.html#computer-and-language-skills",
    "title": "Curriculum Vitae",
    "section": "Computer and Language Skills",
    "text": "Computer and Language Skills\n\nComputer – Programming languages including R, Python, Unix, and SQL\nLanguage – Spanish (conversational proficiency)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Sara Colando",
    "section": "",
    "text": "Hi, and welcome to my website!\nI am a Ph.D. student in the Department of Statistics and Data Science at Carnegie Mellon University. I graduated magna cum laude from Pomona College with a double major in Mathematics and Philosophy in Spring 2024.\nCurrently, I am working with Nynke Niezink and Eva Jaspers on a continuation of my Advanced Data Analysis project, titled “Why Do Students Avoid Each Other? Investigating Positive and Negative Network Effects.” I am also working on a project with Erin Franke and Alex Reinhart in which we analyze statistics students’ writing before and after the emergence of Large Language Models. I have previously been involved in statistical genomics and data science ethics pedagogy research with Jo Hardin.\nFind out more about my interests and past experiences by browsing this site.\n  Curriculum Vitae"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "TidyTuesday Creations",
    "section": "",
    "text": "TidyTuesday is a weekly community activity put on the the Data Science Learning Community. I try to spend a little time each week creating a data visualization or model with the data posted to the official TidyTuesday GitHub Repository (linked below)."
  },
  {
    "objectID": "projects.html#highlighted-creations",
    "href": "projects.html#highlighted-creations",
    "title": "TidyTuesday Creations",
    "section": "Highlighted Creations",
    "text": "Highlighted Creations\nHere are some of my favorite data visualizations that I have made from TidyTuesday over the past two years. Each title has a link to my code for creating the visualization.\n05/07/2024: Demographics of the Rolling Stone’s Top 500 Albums of All Time in 2003 vs. 2012 vs. 2020 \n\n\n\n\n\n03/05/2024: The Mr. Trash Wheel Fleet’s Collected Garbage over the Years \n\n\n\n\n\n11/28/2023: Dr. Who Distribution of Episode Rankings Based on the Episode Writer \n\n\n\n\n\n10/24/2023: Difference between the Taylor’s Version and the Old Version of Songs from Fearless and Red \n\n\n\n\n\n03/21/2023: Coding Language Creation over the Years \n\n\n\n\n\n12/20/2022: Seattle Weather in 2021 \n\n\n\n\n\n07/05/2022: Changes in Median Rent Prices and Percent of Apartments by Neighborhood in San Francisco"
  },
  {
    "objectID": "projects.html#helpful-resources",
    "href": "projects.html#helpful-resources",
    "title": "TidyTuesday Creations",
    "section": "Helpful Resources",
    "text": "Helpful Resources\nIf you are interested in participating in TidyTuesday yourself, here are some resources that I have found helpful for starting:\n\n1. Finding Inspiration\nWhen beginning TidyTuesdays, I find it super helpful to take inspiration from what others have done with the data either this week or in previous weeks. X (i.e., Twitter), Fossodon, and sometimes even Google searches are a great way of gaining inspiration from others!\n\n\n Search “#tidytuesday” on BlueSky\n\n\nhttps://bsky.app/hashtag/TidyTuesday\n\n\n\n\n Search “#tidytuesday” on Mastodon\n\n\nhttps://fosstodon.org/tags/tidytuesday\n\n\n\n\n2. Loading in the Data\nUsually, I read the data into an .Rmd file using the code block that looks like the following:\n```{r}\ndataset_1 &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2024/some_date/dataset_1.csv')\n```\n\n\n3. Analyzing and Visualizing the Data\nAfter that, the world (or, in this case, data) is your oyster! I primarily use Tidyverse to create my TidyTuesday data visualizations and models, so I appreciate the following cheat sheets:\n\n\n stringr Cheat Sheet PDF\n\n\nhttps://rstudio.github.io/cheatsheets/strings.pdf\n\n\n\n\n dplyr Cheat Sheet PDF\n\n\nhttps://rstudio.github.io/cheatsheets/data-transformation.pdf\n\n\n\n\n GGPlot2 Cheat Sheet PDF\n\n\nhttps://rstudio.github.io/cheatsheets/data-visualization.pdf"
  },
  {
    "objectID": "research/data-ethics/data-ethics.html",
    "href": "research/data-ethics/data-ethics.html",
    "title": "Philosophy within Data Science Ethics Courses",
    "section": "",
    "text": "Overview: There is wide agreement that ethical considerations are a valuable aspect of a data science curriculum, and to that end, many data science programs offer courses in data science ethics. There are not always, however, explicit connections between data science ethics and the centuries-old work on ethics within the discipline of philosophy. Here, we present a framework for bringing together key data science practices with ethical topics. The ethical topics were collated from sixteen data science ethics courses with public-facing syllabi and reading lists. We encourage individuals who are teaching data science ethics to engage with the philosophical literature and its connection to current data science practices, which is rife with potentially morally charged decision points.\n\nColando, S., & Hardin, J. (2024). Philosophy within Data Science Ethics Courses. Journal of Statistics and Data Science Education, 32(4), 361–373. https://doi.org/10.1080/26939169.2024.2394542\n\nFunded by Pomona College’s Evelyn B. Craddock McVicar Memorial Fund (Summer 2023) and the Kenneth Cooke Summer Research Fellowship (Summer 2024)."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am a Ph.D. student in Carnegie Mellon’s Statistics and Data Science department. In May 2024, I graduated magna cum laude from Pomona College with a double major in mathematics and philosophy. Currently, I am working with Nynke Niezink and Eva Jaspers to study structural dependencies in classroom avoidance networks and how a student’s tendency to avoid others and be avoided relates to their position in the friendship network. At Pomona, I primarily worked with Jo Hardin, conducting research in both statistical genomics and data science ethics pedagogy. More information on my current and previous research projects can be found on my research page.\nOutside of research, I enjoy teaching statistics and data science. This Spring, I will be a TA for 36-402: Undergraduate Advanced Data Analysis, which is a capstone course for undergraduate students in CMU’s Statistics & Data Science department. A list of my previous teaching experiences and education-related research projects can be found on my teaching page.\nAnother primary interest of mine is philosophy, particularly (feminist) philosophy of science, ethics, and (social) epistemology. I hope to stay engaged in philosophy and, with that, work to better understand and grapple with the ethical dimensions of statistical technologies while completing my Ph.D. in Statistics. For more of my interests, check out my notebooks page."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About",
    "section": "Education",
    "text": "Education\n Ph.D. in Statistics, Carnegie Mellon University, 2024-2029 (expected)\n BA in Mathematics and Philosophy, Pomona College, 2020-2024"
  },
  {
    "objectID": "about.html#contact",
    "href": "about.html#contact",
    "title": "About",
    "section": "Contact",
    "text": "Contact\n scolando@andrew.cmu.edu"
  },
  {
    "objectID": "extras.html",
    "href": "extras.html",
    "title": "Extras",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nTitle\n\n\n\n\n\n\nAI and Data Science Ethics Writing\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "extras/philosophy-writing/philosophy.html",
    "href": "extras/philosophy-writing/philosophy.html",
    "title": "AI and Data Science Ethics Writing",
    "section": "",
    "text": "Title\n\n\nDescription\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "extras/philosophy-writing/writing/writing-sample-1.html",
    "href": "extras/philosophy-writing/writing/writing-sample-1.html",
    "title": "The Epistemic Flaw of Gerrymandered Biases in Endorsed Algorithms",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "extras/philosophy-writing/writing/writing-sample-1/writing-sample-1.html",
    "href": "extras/philosophy-writing/writing/writing-sample-1/writing-sample-1.html",
    "title": "The Epistemic Flaw of Gerrymandered Biases in Endorsed Algorithms",
    "section": "",
    "text": "Download PDF file.\nImagine that there is a recidivism algorithm being used to determine if an incarcerated man should be granted early parole release. This algorithm has the bias that ‘Black men are more likely to be reconvicted than white men.’ Suppose that this algorithmic bias is statistically accurate; it is, in fact, more likely for Black men to be reconvicted than white men. Still, the algorithm’s disposition to predict that a Black man is more likely to be reconvicted than a white man seems morally problematic. However, could the algorithmic bias (which leads to its morally flawed disposition) also be epistemically flawed, even though it is statistically accurate? In this paper, I elucidate how a bias’s etiology, i.e., the means that give rise to a bias, can influence the epistemic evaluation of an algorithmic bias. In particular, I argue that biases that arise through gerrymandered means in endorsed algorithms are epistemically flawed in virtue of being gerrymandered and endorsed. I begin by explaining what I mean by bias and vindicating that algorithms can have biases. I next adopt philosopher Jessie Munton’s argument that gerrymandered perceptual priors are inherently epistemically flawed in order to offer a sufficient condition for gerrymandered biases and underscore a primary epistemic aim of predictive skill. Using my adopted version of Munton’s argument, I contend that gerrymandered biases in endorsed algorithms increase the likelihood of inaccurate modal predictions, thereby reducing the predictive skill of agents whose decisions are influenced by the algorithm. Such a reduction in predictive skill diminishes one’s ability to gain causal-explanatory knowledge about relevant environments, which is epistemically problematic. Thus, I conclude that gerrymandered biases in endorsed algorithms are epistemically flawed in virtue of being gerrymandered and endorsed\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "extras/philosophy-writing/writing/writing-sample-2/writing-sample-2.html",
    "href": "extras/philosophy-writing/writing/writing-sample-2/writing-sample-2.html",
    "title": "On Calibration Within Groups being Necessary for Algorithmic Fairness",
    "section": "",
    "text": "Download PDF file.\nAs algorithms have become more ubiquitous in decisions with high moral stakes – like deciding who to approve for a loan, who to release early on parole, and which communities to give additional governmental resources to – there has also been a growing concern about what is required for such algorithms to be fair. One criterion that is commonly viewed as necessary for algorithmic fairness within computer science circles is calibration within groups. Per calibration within groups, an algorithm is fair only if for every risk score s and group A, s percent of the individuals in group A who are assigned a risk score s are indeed positive instances. However, in this paper, I argue that calibration within groups is not always necessary for algorithmic fairness, even when it is most charitably conceived. I begin with a primer on algorithmic fairness and then further explain what is meant by calibration within groups. I next explicate Brian Hedden and Robert Long’s positive arguments for why calibration within groups is necessary for algorithmic fairness. I then delve into critiques of such arguments. These critiques lead me to present a modified conception of calibration within groups, which I contend is maximally charitable. Yet, I present a case to demonstrate how an algorithm can violate even the maximally charitable conception of calibration within groups but still fail to be intuitively unfair in virtue of doing so. Thus, I conclude that calibration within groups, even when it is most charitably conceived, is not always necessary for algorithmic fairness.\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "extras/philosophy.html",
    "href": "extras/philosophy.html",
    "title": "AI and Data Science Ethics Writing",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nTitle\n\n\nDescription\n\n\n\n\n\n\nOn Calibration Within Groups being Necessary for Algorithmic Fairness\n\n\n\n\n\n\n\nThe Epistemic Flaw of Gerrymandered Biases in Endorsed Algorithms\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "philosophy-writing/writing/writing-sample-1/writing-sample-1.html",
    "href": "philosophy-writing/writing/writing-sample-1/writing-sample-1.html",
    "title": "The Epistemic Flaw of Gerrymandered Biases in Endorsed Algorithms",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "philosophy-writing/writing/writing-sample-2/writing-sample-2.html",
    "href": "philosophy-writing/writing/writing-sample-2/writing-sample-2.html",
    "title": "The Epistemic Flaw of Gerrymandered Biases in Endorsed Algorithms",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "blogs/applying-grad-school.html",
    "href": "blogs/applying-grad-school.html",
    "title": "Applying to (Bio)statistics PhD Programs",
    "section": "",
    "text": "I applied to primarily (Bio)statistics PhD programs during the 2023-2024 application cycle. Coming from a small liberal arts school, I found any and all resources or tidbits of advice incredibly helpful. That said, I wanted to share my advice as well as some of my application materials in this blog post in case others find it helpful."
  },
  {
    "objectID": "cv.html#publications-and-in-progess-papers",
    "href": "cv.html#publications-and-in-progess-papers",
    "title": "Curriculum Vitae",
    "section": "Publications and In-Progess Papers",
    "text": "Publications and In-Progess Papers\n\nColando, S. & Hardin, J. Selecting ChIP-Seq Normalization Methods from the Perspective of their Technical Conditions. Revise and Resubmit, 2025.\nColando, S. & Hardin, J. (2024). Philosophy within Data Science Ethics Courses. Journal of Statistics and Data Science Education, 32(4), 361–373. https://doi.org/10.1080/26939169.2024.2394542"
  },
  {
    "objectID": "cv.html#teaching-experience",
    "href": "cv.html#teaching-experience",
    "title": "Curriculum Vitae",
    "section": "Teaching Experience",
    "text": "Teaching Experience\nCarnegie Mellon Department of Statistics and Data Science Graduate Teaching Assistant\n\n36-707: Regression Analysis (Fall 2025)\nCarnegie Mellon Sports Analytics Camp, co-led four lectures (Summer 2025)\n36-226: Introduction to Statistical Inference, Head TA (Spring 2025)\n36-600: Overview of Statistical Learning and Modeling (Fall 2024)\n\nPomona College Course Mentor and Grader\n\nMATH158: Statistical Linear Models (Spring 2024)\nMATH058B: Introduction to Biostatistics (Spring 2023)\nMATH060: Linear Algebra (Fall 2021)\nMATH030: Calculus I, Grader Only (Spring 2021)\nID001: First-year Interdisciplinary Seminar: I Disagree (Fall 2023)\nBIOL040: Introductory Genetics with Lab (Fall 2022)\n\n1-2-1 Summer Bridge Program Teacher’s Assistant (Summer 2021)\n\nLed 1.5-hour meetings, twice per week, for three small cohorts of incoming Pomona College students\nOrganized group bonding by preparing icebreakers and virtual games for 40+ students in the program\nMathematical concepts taught: logic, statistics, calculus I and II, combinatorics, and number theory"
  },
  {
    "objectID": "research/horses/horses.html",
    "href": "research/horses/horses.html",
    "title": "Clustering Race Horse Movement Profiles to Discover Trends in Injured Horses",
    "section": "",
    "text": "Overview: Between 2009 and 2021, over 7,200 horses died or were euthanized due to racing-related injuries. Using horse profile data and horse tracking data from the NYRA, we hoped to identify horses who under-raced between 2019 and 2021, cluster movement profiles for horses who raced in 2019 New York races, and discover whether certain profiles were more associated with injured horses.\nBy fitting a negative binomial model on horse profile data and performing residual analysis, we discovered that at least 251 horses under-raced between 2019 and 2021. Additionally, clustering horse movement profiles revealed that a horse’s speed profile is most associated with its injury status: specifically, greater variation in speed is more associated with injury.\nDone in through Carnegie Mellon’s Sports Analytics Camp and via an external partnership with Joseph Appelbaum at the New York Thoroughbred Horsemen’s Association."
  },
  {
    "objectID": "research/tumor-mri/tumor-mri.html",
    "href": "research/tumor-mri/tumor-mri.html",
    "title": "Implementing a Partially Bayesian Neural Network for Brain Tumor Segmentation",
    "section": "",
    "text": "Overview: Radiologists segment tumors from MRI scans to determine treatment plans such as surgical resection or radiation therapy, whereas neural networks can streamline the segmentation process to ensure ideal tumor removal and reduce the burden on radiologists. However, these black-box models currently lack explainability, which leads to a lack of trust from different end users like physicians and patients when used to segment brain tumors. Uncertainty Quantification communicates to stakeholders: (a) if and when they should trust model predictions and (b) how fair these predictions are on sample-wide and patient-specific cases. Therefore, Uncertainty Quantification enhances a model’s transparency by exposing a model’s properties to various stakeholders to better understand, improve, and contest the model’s predictions. In this project, we aim to quantify model uncertainty by using a partially Bayesian neural network to communicate where the model is uncertain of its prediction of a pixel being classified as “tumor” or “non-tumor.” In particular, we aim to address the following questions:\n\nWhere is this model failing, and how is it failing to properly segment the tumor?\nIn what cases is the model certain but still making mistakes in tumor segmentation?\n\nUltimately, we find that the highest uncertainty is at the boundary regions of the model’s predicted tumor location and the false negative and false positive pixels are highly clustered. Further, we discover that there is generally higher certainty for accurately classified pixels as well as greater certainty for false negatives than false positives pixels. From our findings, we suggest future work in collaboration with clinicians to better understand why model fails in specific brain regions, and why false positive and false negative results tend to cluster. Moreover, we suggest that the model performance and uncertainty levels across should be compared various subsets (e.g., different tumor histologies, tissue source sites, patient sex, vital status, etc.) to investigate whether there are trends in differential model performance across different demographic groups.\n\nSummer 2022 Poster PDF on quantifying uncertainty in a tumor segmentation model.\nSummer 2022 Presentation PDF on quantifying uncertainty in a tumor segmentation model\n\nDone through University of Michigan School of Public Health’s Big Data Summer Institute and via a partnership with Dr. Nikola Banovic and Snehal Prabhudesai in University of Michigan School of Computer Science and Engineering."
  },
  {
    "objectID": "blogs/understanding-fairness-metrics.html",
    "href": "blogs/understanding-fairness-metrics.html",
    "title": "Common Outcome-Based Algorithmic Fairness Metrics",
    "section": "",
    "text": "Figure 1: A slightly cynical perspective on outcome-basedalgorithmic fairness metrics from Machines Gone Wrong.\nAlgorithms are becoming an increasingly ubiquitous component of decision-making with high moral stakes, such as loan and mortgage approval, governmental aid allocations, health insurance claim approval, and US bail and sentencing procedures. With the growing use of algorithms in settings with high moral stakes, there has also been a growing concern about algorithmic fairness, or more specifically, whether an algorithm used to make decisions in such settings is (un)fair.\nMany statistical metrics have been developed to assess whether a given algorithm is (un)fair. Since 2022, I have been interested in these measures and how they track with discussions of fairness and justice coming from Philosophy – a field which has been theorizing about fairness and justice for more than two millennia. However, when doing research on statistical metrics of fairness, I realized that the definitions of such metrics tend to be scattered across multiple papers, making it difficult to understand how different metrics compare (or motivate) one another.\nIn this notebook, I provide an overview of common algorithmic fairness metrics coming from Statistics and draw comparison between the described metrics. For simplicity, this notebook focuses on outcome-based measures of fairness (i.e., metrics that evaluate an algorithm for fairness using its outputted predictions). In a future notebook, I hope to delve into more procedural-based algorithm fairness measures. Additionally, since many of the common fairness metrics are for discrete prediction problems, I primarily focus on metrics in this notebook rather than fairness metrics for continuous prediction problems. For example, predicting whether or not someone will be reconvicted rather than the time-until reconviction (and whether that time is censored)."
  },
  {
    "objectID": "blogs/understanding-fairness-metrics.html#calibration-within-groups",
    "href": "blogs/understanding-fairness-metrics.html#calibration-within-groups",
    "title": "Common Outcome-Based Algorithmic Fairness Metrics",
    "section": "2.8 Calibration within Groups",
    "text": "2.8 Calibration within Groups\nCalibration within Groups is a metric specifically for algorithms that output risk scores, i.e., an ordinal prediction for each observation that represents the probability that it belongs to the positive class. The COMPAS algorithm is a canonical example of a risk score algorithm in the algorithmic fairness literature.\nPer Calibration within Groups, an algorithm is fair if for each possible risk score, the (expected) proportion of individuals assigned that risk score who are actually members of the positive class is equal to the risk score and the same for each relevant group. That is:\n\n\\mathbb{P}(\\hat R \\mid \\mathrm{True~Class} = 1, \\mathrm{~Group} = X) = \\hat R\n\nwhere, X \\in \\{A,B, \\dots\\} and \\hat R denotes predicted risk score, possibility transformed to be between 0 and 1 (inclusive).\nSince there are several moving parts for satisfying Calibration within Groups metric, it is perhaps easier to understand through toy examples (see Figure 3).\n\n\n\n\n\n\n\n\nFigure 3: ADD\n\n\n\n\n\nOne motivation for Calibration within Groups is that ADD."
  },
  {
    "objectID": "blogs/understanding-fairness-metrics.html#predictive-parity",
    "href": "blogs/understanding-fairness-metrics.html#predictive-parity",
    "title": "Understanding (and Distinguishing) Common Algorithmic Fairness Metrics",
    "section": "Predictive Parity",
    "text": "Predictive Parity\nHowever, depending on the context of the algorithm, it might be more crucial from the perspective fairness for there to be equal accuracy between groups among those predicted as part of the positive class, not just equal accuracy between the groups in general (i.e. unconditioned on the individual’s predicted class). The fairness metric of Predictive Parity captures this concern. According to Predictive Parity, an algorithm is unfair between relevant groups A and B if:\n\\[\\mathbb{P}(\\mathrm{True~Class = 1|~Pred.~Class =1,~Group = A}) \\neq \\mathbb{P}(\\mathrm{True~Class = 1|~Pred.~Class = 1,~Group = B})\\] Notice how Predictive Parity relates to the algorithm’s Positive Predictive Value (PPV) or Precision add"
  },
  {
    "objectID": "blogs/understanding-fairness-metrics.html#equal-false-positive-rates",
    "href": "blogs/understanding-fairness-metrics.html#equal-false-positive-rates",
    "title": "Common Outcome-Based Algorithmic Fairness Metrics",
    "section": "2.3 Equal False Positive Rates",
    "text": "2.3 Equal False Positive Rates\nLike the Predictive Parity metric, the Equal False Positive Rates metric only equalizes prediction performance among a subset of individuals in the relevant groups. Per the Equal False Positive Rates metric, an algorithm is unfair if the probability that an individual whose true class is negative is predicted as positive differs between the relevant groups. This can be formalized as:\n\\mathbb{P}(\\mathrm{Pred.~Class = 1|~True~Class = 0,~Group = A}) \\neq \\mathbb{P}(\\mathrm{Pred.~Class = 1|~True~Class = 0,~Group = B})\nFor example, using Equal False Positive Rates as a fairness metric would prevent cases where more people in group A are incorrectly identified as recidivists than in group B. In the ProPublica report on the COMPAS algorithm, ADD argue COMPAS is biased, and thus unfair, on the basis on unequal false positive rates between Black and White defendants. Specifically, they found that Black defendants were two times more likely to be falsely labeled as future criminals than white defendants."
  },
  {
    "objectID": "blogs/understanding-fairness-metrics.html#equal-false",
    "href": "blogs/understanding-fairness-metrics.html#equal-false",
    "title": "Understanding (and Distinguishing) Common Algorithmic Fairness Metrics",
    "section": "3 Equal False",
    "text": "3 Equal False"
  },
  {
    "objectID": "blogs/understanding-fairness-metrics.html#equal-false-negative-rates",
    "href": "blogs/understanding-fairness-metrics.html#equal-false-negative-rates",
    "title": "Common Outcome-Based Algorithmic Fairness Metrics",
    "section": "2.4 Equal False Negative Rates",
    "text": "2.4 Equal False Negative Rates\nAlong a similar vein to Equal False Positive Rates is the fairness metric of Equal False Negative Rates, which says that an algorithm is unfair if the probability that an individual whose true class is negative will be predicted as positive differs between relevant groups:\n\\mathbb{P}(\\mathrm{Pred.~Class = 0|~True~Class = 1,~Group = A}) \\neq \\mathbb{P}(\\mathrm{Pred.~Class = 0|~True~Class = 1,~Group = B})\nADD also found that COMPAS had different false negative rates between Black and white defendants. Namely, “white defendants were mislabeled as low risk more often than black defendants”, which they also used to justify their conclusion that the COMPAS algorith was biased and thus unfair CITE."
  },
  {
    "objectID": "blogs/understanding-fairness-metrics.html#equalized-odds",
    "href": "blogs/understanding-fairness-metrics.html#equalized-odds",
    "title": "Common Outcome-Based Algorithmic Fairness Metrics",
    "section": "2.7 Equalized Odds",
    "text": "2.7 Equalized Odds\n\\mathbb{P}(\\mathrm{Pred.~Class = 1|~True~Class = X,~Group = A}) \\neq \\mathbb{P}(\\mathrm{Pred.~Class = 1|~True~Class = X,~Group = B}) where, X \\in \\{0,1\\}"
  },
  {
    "objectID": "blogs/understanding-fairness-metrics.html#equal-accuracy",
    "href": "blogs/understanding-fairness-metrics.html#equal-accuracy",
    "title": "Common Outcome-Based Algorithmic Fairness Metrics",
    "section": "2.1 Equal Accuracy",
    "text": "2.1 Equal Accuracy\nPerhaps the most flat-footed algorithmic fairness metric is Equal Accuracy, which says that an algorithm is unfair if it has unequal accuracy across relevant groups. That is, suppose we had two relevant groups, denoted as A and B. Then, according to Equal Accuracy, an algorithm is unfair if:2\n\\mathbb{P}(\\mathrm{Pred.~Class = True~Class~|~Group = A}) \\neq \\mathbb{P}(\\mathrm{Pred.~Class = True~Class~|~Group = B}) In other words, an algorithm is unfair when the probability that the predicted class is the same as the true class is different between relevant groups under the Equal Accuracy metric."
  },
  {
    "objectID": "blogs/understanding-fairness-metrics.html#statistical-parity",
    "href": "blogs/understanding-fairness-metrics.html#statistical-parity",
    "title": "Understanding (and Distinguishing) Common Algorithmic Fairness Metrics",
    "section": "Statistical Parity",
    "text": "Statistical Parity"
  },
  {
    "objectID": "blogs/understanding-fairness-metrics.html#some-key-terms",
    "href": "blogs/understanding-fairness-metrics.html#some-key-terms",
    "title": "Understanding (and Distinguishing) Common Algorithmic Fairness Metrics",
    "section": "Some Key Terms",
    "text": "Some Key Terms\n\nTrue Positives, False Positives, True Negatives, and False Negatives\n\n\n\n\nDiagram from Analytics Vidhya.\n\n\n\nMost fairness metrics for discrete prediction algorithms use some combination of true positives, true negatives, false positives, and false negatives in their calculations. A helpful way of visualizing the difference between terms is through a confusion matrix (like the one depicted for a binary prediction algorithm on the left). Notice how true positives, true negatives, false positives, and false negatives all rely on comparing an observation’s predicted class to its true (or actual) class. Specifically, a true positive (TP) is when observation’s true class and predicted class are both positive, and a false positive (FP) is when the observation’s true class is positive but its predicted class is not. On the other hand, a true negative (TN) is when observation’s true class and predicted class are both negative, and a false negative (FN) is when the observation’s true class is positive but its predicted class is not.\nAn Real-World Example: The Correctional Offender Management Profiling for Alternative Sanctions (COMPAS) algorithm is used in US courtrooms to assess the likelihood of a defendant becoming a recidivist (i.e. being re-convicted) by assigning each defendant a score between 1-10 based on their predicted likelihood of recidivism. A defendant is considered to have a low risk of recidivism if their COMPAS score is between 1-4 and medium to high risk of recidivism if their COMPAS score is between 5-10. In ProPublica’s seminal analysis of COMPAS, true positives, false positives, true negatives, and false negatives are defined as follows:1\n\nA true positive is when a defendant receives a COMPAS score between 5-10 and is re-convicted in the two years after they were scored.\nA false positive is when a defendant receives a COMPAS score between 5-10 but is not re-convicted in the two years after they were scored.\nA true negative is when a defendant receives a COMPAS score between 1-4 and is not re-convicted in the two years after they were scored.\nA false negative is when a defendant receives a COMPAS score between 1-4 but is re-convicted in the two years after they were scored.\n\n\n\nConditional Probability\nAnother important term to understand when talking about statistical fairness metrics is conditional probability which is the probability of an event some other event occurs. Let A and B be two events, where B has a non-zero probability of occurring, then the conditional probability of A given B (i.e. \\(\\mathbb{P}(A|B)\\)) can be calculated as follows:\n\\[\\mathbb{P}(A|B) = \\frac{\\mathbb{P}(A\\cap B)}{\\mathbb{P}(B)}\\]\nIn real-life, we rarely know the true probability of events occurring. Instead, one can estimate this probability empirically using the sample estimate of the probability. That is, one can find the proportion of times that both event A and event B occurred out of the total number of times event B occurred within the sample."
  },
  {
    "objectID": "blogs/understanding-fairness-metrics.html#footnotes",
    "href": "blogs/understanding-fairness-metrics.html#footnotes",
    "title": "Common Outcome-Based Algorithmic Fairness Metrics",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFrom the COMPAS example, we can already see a point of contention in characterizing algorithmic fairness for discrete prediction problems, which is what threshold to use for decision-making? For example, why separate low from medium to high risk of recidivism at 4-5 rather than 3-4 or 5-6? How do we think about fairness when determining decision thresholds?↩︎\nIn this notebook, I interpret each fairness metric as a necessary but not necessarily sufficient condition of algorithm fairness, meaning that an algorithm would be considered unfair if it does not satisfy X metric necessary for algorithmic fairness but it would not necessarily be fair if it does satisfy the required metric. This, in my opinion, is the most charitable way of interpreting the common statistical fairness metrics that I discuss in this notebook.↩︎\nFor more information, see https://civilrights.osu.edu/training-and-education/protected-class-definitions.↩︎"
  },
  {
    "objectID": "blogs/understanding-fairness-metrics.html#demographicstatistical-parity",
    "href": "blogs/understanding-fairness-metrics.html#demographicstatistical-parity",
    "title": "Common Outcome-Based Algorithmic Fairness Metrics",
    "section": "2.6 Demographic/Statistical Parity",
    "text": "2.6 Demographic/Statistical Parity\n\\mathbb{P}(\\mathrm{Pred.~Class = 1~|~Group = A}) \\neq \\mathbb{P}(\\mathrm{Pred.~Class = 1~|~Group = B})"
  },
  {
    "objectID": "blogs/understanding-fairness-metrics.html#predictive-parity-sometimes-also-called-equal-opportunity",
    "href": "blogs/understanding-fairness-metrics.html#predictive-parity-sometimes-also-called-equal-opportunity",
    "title": "Understanding (and Distinguishing) Common Algorithmic Fairness Metrics",
    "section": "Predictive Parity (sometimes also called Equal Opportunity)",
    "text": "Predictive Parity (sometimes also called Equal Opportunity)\nHowever, depending on the context in which the algorithm is used, it might be crucial from the perspective fairness to ensure that there is equal accuracy between groups among those predicted as part of the positive class, rather than simply guaranteeing that there is equal accuracy between the relevant groups in general (i.e., unconditioned on the individual’s predicted class). For instance, it seems more important from the perspective of fairness that a hiring algorithm correctly identify the same proportion of candidates who are qualified for a job between relevant groups than that the same proportion of candidates are correctly identified as being unqualified between relevant groups.\nThe metric of Predictive Parity captures this concern. According to Predictive Parity, an algorithm is unfair between relevant groups A and B if:\n\\mathbb{P}(\\mathrm{True~Class = 1|~Pred.~Class =1,~Group = A}) \\neq \\mathbb{P}(\\mathrm{True~Class = 1|~Pred.~Class = 1,~Group = B}) Notice how Predictive Parity relates to the algorithm’s Positive Predictive Value (PPV) or Precision. Specifically, when an algorithm satisfies Predictive Parity, we expect for the PPV and Precision for the relevant groups to be equal. That is:\n\\left( \\frac{TP + FN}{TP + FP} \\right)_\\mathrm{Group~=~A} = \\left( \\frac{TP + FN}{TP + FP} \\right)_\\mathrm{Group~=~B} Sometimes, Predictive Parity is also referred to as the Equal Opportunity metric."
  },
  {
    "objectID": "blogs/understanding-fairness-metrics.html#equal-ratios-of-false-positive-rates-to-false-negative-rates",
    "href": "blogs/understanding-fairness-metrics.html#equal-ratios-of-false-positive-rates-to-false-negative-rates",
    "title": "Common Outcome-Based Algorithmic Fairness Metrics",
    "section": "2.5 Equal Ratios of False Positive Rates to False Negative Rates",
    "text": "2.5 Equal Ratios of False Positive Rates to False Negative Rates\nA looser requirement for algorithmic fairness is Equal Ratios of False Positive Rates to False Negative Rates, which does not require either Equal False Positive Rates or Equal False Negative Rates to hold between relevant groups. Putting together the formalizations of the two prior metrics, we get that an algorithm is unfair, according to Equal Ratios of False Positive Rates to False Negative Rates, if:\n\\frac{\\mathbb{P}(\\mathrm{Pred.~Class = 1|~True~Class = 0,~Group = A})}{\\mathbb{P}(\\mathrm{Pred.~Class = 0|~True~Class = 1,~Group = A})} \\neq \\frac{\\mathbb{P}(\\mathrm{Pred.~Class = 1|~True~Class = 0,~Group = B})}{\\mathbb{P}(\\mathrm{Pred.~Class = 0|~True~Class = 1,~Group = B})}\nAt first glance, it is hard to understand why would care about equal ratios of false positive rates to false negative rates; would it not make more sense to want both equal false positive rates and equal false negative rates to hold between relevant groups? One reason that we might care about Equal Ratios of False Positive Rates to False Negative Rates for algorithmic fairness is because we care about the algorithm ADD"
  },
  {
    "objectID": "blogs/understanding-fairness-metrics.html#balance-for-positive-class",
    "href": "blogs/understanding-fairness-metrics.html#balance-for-positive-class",
    "title": "Common Outcome-Based Algorithmic Fairness Metrics",
    "section": "2.9 Balance for Positive Class",
    "text": "2.9 Balance for Positive Class\nAlternatively, we might be most concerned with disparate performance of risk score algorithms on individuals who are members of the positive class.\nAccording to the Balance for Positive Class metric, an algorithm is unfair if the expected risk score assigned to individuals who are actually members of the positive differs across relevant groups:\n\n\\mathbb{E}[\\hat R \\mid \\mathrm{True~Class} = 1, \\mathrm{~Group = A}]= \\mathbb{E}[\\hat R \\mid \\mathrm{True~Class} = 1, \\mathrm{~Group = B}]"
  },
  {
    "objectID": "blogs/understanding-fairness-metrics.html#balance-for-negative-class",
    "href": "blogs/understanding-fairness-metrics.html#balance-for-negative-class",
    "title": "Common Outcome-Based Algorithmic Fairness Metrics",
    "section": "2.10 Balance for Negative Class",
    "text": "2.10 Balance for Negative Class\nThe (expected) average risk score assigned to those individuals who are actually members of the negative class is the same for each relevant group:\n\n\\mathbb{E}[\\hat R \\mid \\mathrm{True~Class} = 0, \\mathrm{~Group = A}]= \\mathbb{E}[\\hat R \\mid \\mathrm{True~Class} = 0, \\mathrm{~Group = B}]"
  },
  {
    "objectID": "cv.html#publications-and-papers",
    "href": "cv.html#publications-and-papers",
    "title": "Curriculum Vitae",
    "section": "Publications and Papers",
    "text": "Publications and Papers\n\nPeer-reviewed Articles\n\nColando, S., Schulz, D., Hardin, J. Selecting ChIP-Seq Normalization Methods from the Perspective of their Technical Conditions, Briefings in Bioinformatics, 2025. https://doi.org/10.1093/bib/bbaf431\nColando, S. & Hardin, J. Philosophy within Data Science Ethics Courses, Journal of Statistics and Data Science Education, 2024. https://doi.org/10.1080/26939169.2024.2394542\n\nIn Progress\n\nColando, S., Franke, E., Reinhart A., Analyzing Statistics Students’ Writing Before and After the Emergence of Large Language Models. In progress."
  },
  {
    "objectID": "cv.html#publications-and-manuscripts",
    "href": "cv.html#publications-and-manuscripts",
    "title": "Curriculum Vitae",
    "section": "Publications and Manuscripts",
    "text": "Publications and Manuscripts\n\nColando, S. & Hardin, J. Selecting ChIP-Seq Normalization Methods from the Perspective of their Technical Conditions. Revise and Resubmit, 2025.\nColando, S. & Hardin, J. (2024). Philosophy within Data Science Ethics Courses. Journal of Statistics and Data Science Education, 32(4), 361–373. https://doi.org/10.1080/26939169.2024.2394542"
  },
  {
    "objectID": "cv.html#grants",
    "href": "cv.html#grants",
    "title": "Curriculum Vitae",
    "section": "Grants",
    "text": "Grants\n\nColando, S. Graduate Student Fellowship 2025-2027\nAmount: $6,000 per year\nFunder: Institute of Complex Social Dynamics, Carnegie Mellon University\nColando, S. & Franke, E. Travel Grant 2025\nAmount: $500\nFunder: StatBytes, Carnegie Mellon Department of Statistics and Data Science\nColando, S. & Franke, E. data.table Ambassador Travel Grant 2025\nAmount: $2,700\nFunder: National Science Foundation (Award Abstract: #2303612)"
  },
  {
    "objectID": "cv.html#other-research-experience",
    "href": "cv.html#other-research-experience",
    "title": "Curriculum Vitae",
    "section": "Other Research Experience",
    "text": "Other Research Experience\nPomona College Department of Mathematics and StatisticsResearch Assistant to Dr. Johanna Hardin May 2022 – Aug 2024\n\nAnalyzed between-sample normalization techniques by their technical assumptions to improve differential binding analysis practices for ChIP-Seq and similarly structured types of high-throughput data\nCreated reproducible ChIP-Seq read count simulations using R, Bash, and high-performance computing to examine how violating normalization method assumptions impacts the false discovery rate and power of identifying genomic regions with true differential DNA occupancy.\n\nData Science Ethics Independent Project (Supervisor: Dr. Johanna Hardin) May – Oct 2023\n\nSynthesized resources and pedagogies of existing Data Science Ethics undergraduate courses to connect key ethical concepts to data science practices\nCreated a website to effectively communicate data science ethics concepts and relevant background information to a broad community of decision-makers\nAided in Pomona College’s development of an ethics course for the recently approved data science minor\n\nCarnegie Mellon University Department of Statistics and Data ScienceResearch Assistant to Dr. Ron Yurko May – Aug 2023\n\nPerforming mixture-model clustering analysis to discover trends in movement patterns for injured racehorses\nIdentifying horses who under-raced through residual analysis of a created expected race count model\nEngaged in 8 weeks of lectures about popular statistical analysis techniques, data engineering, and novel data science methodologies employed in sports analytics\n\nSelf-Directed ProjectData Analysis and Visualization CreatorJan 2021 – May 2024\n\nConducting exploratory data analysis in R to improve data visualization and communication skills every week using data provided by the “R for Data Science” Online Learning Community\nHelping students new to R become more familiar with the programming language and troubleshoot errors in 2-hour Pomona TidyTuesday workshop offered each week\n\nUniversity of California, San Diego Halıcıoğlu Data Science Institute Intelligence, Data, Ethics, and Society (IDEAS) Summer Institute Participant Aug 2023\n\nParticipated in workshops taught by experts in domains like data science, AI, philosophy, and law to improve critical and interdisciplinary thinking on data science and AI ethics topics\nCollaborated with three other students on a two-week research project, examining practical methods of implementing responsible data science and AI practices in a California wildfire resource allocation algorithm\n\nUniversity of Michigan School of Public HealthBig Data Summer Institute Research Assistant to Dr. Nikola Banovic June – Aug 2022\n\nCollaborated with three other students to implement a partially Bayesian neural network to quantify and communicate prediction uncertainty in a supervised brain tumor segmentation model via Python’s PyTorch and TensorFlow packages\nParticipated in 6 weeks of lectures about ethical data collection practice in healthcare, probability and statistical theory, and the ethical implications of using statistics and computer science methods in healthcare settings"
  },
  {
    "objectID": "blogs/understanding-fairness-metrics.html#predictive-parity-sometimes-called-equal-opportunity",
    "href": "blogs/understanding-fairness-metrics.html#predictive-parity-sometimes-called-equal-opportunity",
    "title": "Common Outcome-Based Algorithmic Fairness Metrics",
    "section": "2.2 Predictive Parity (sometimes called Equal Opportunity)",
    "text": "2.2 Predictive Parity (sometimes called Equal Opportunity)\nHowever, depending on the context in which the algorithm is used, it might be crucial from the perspective fairness to ensure that there is equal accuracy between groups among those predicted as part of the positive class, rather than simply guaranteeing that there is equal accuracy between the relevant groups in general (i.e., unconditioned on the individual’s predicted class). For instance, it seems more important from the perspective of fairness that a hiring algorithm correctly identify the same proportion of candidates who are qualified for a job between relevant groups than that the same proportion of candidates are correctly identified as being unqualified between relevant groups.\nThe metric of Predictive Parity captures this concern. According to Predictive Parity, an algorithm is unfair between relevant groups A and B if:\n\\mathbb{P}(\\mathrm{True~Class = 1|~Pred.~Class =1,~Group = A}) \\neq \\mathbb{P}(\\mathrm{True~Class = 1|~Pred.~Class = 1,~Group = B}) Notice how Predictive Parity relates to the algorithm’s Positive Predictive Value (PPV) or Precision. Specifically, when an algorithm satisfies Predictive Parity, we expect for the PPV and Precision for the relevant groups to be equal. That is:\n\\left( \\frac{TP + FN}{TP + FP} \\right)_\\mathrm{Group~=~A} = \\left( \\frac{TP + FN}{TP + FP} \\right)_\\mathrm{Group~=~B} Sometimes, Predictive Parity is called the Equal Opportunity metric."
  },
  {
    "objectID": "blogs/understanding-fairness-metrics.html#true-positives-false-positives-true-negatives-and-false-negatives",
    "href": "blogs/understanding-fairness-metrics.html#true-positives-false-positives-true-negatives-and-false-negatives",
    "title": "Common Outcome-Based Algorithmic Fairness Metrics",
    "section": "1.1 True Positives, False Positives, True Negatives, and False Negatives",
    "text": "1.1 True Positives, False Positives, True Negatives, and False Negatives\n\n\n\n\n\n\n\nFigure 2: Confusion Matrix Diagram from Analytics Vidhya.\n\n\n\n\nMost fairness metrics for discrete prediction algorithms use some combination of true positives, true negatives, false positives, and false negatives in their calculations. A helpful way of visualizing the difference between terms is through a confusion matrix (like the one depicted for a binary prediction algorithm in Figure 2). Notice how true positives, true negatives, false positives, and false negatives all rely on comparing an observation’s predicted class to its true (or actual) class. Specifically, a true positive (TP) is when observation’s true class and predicted class are both positive, and a false positive (FP) is when the observation’s true class is positive but its predicted class is not. On the other hand, a true negative (TN) is when observation’s true class and predicted class are both negative, and a false negative (FN) is when the observation’s true class is positive but its predicted class is not.\nAn Real-World Example: The Correctional Offender Management Profiling for Alternative Sanctions (COMPAS) algorithm is used in US courtrooms to assess the likelihood of a defendant becoming a recidivist (i.e. being re-convicted) by assigning each defendant a score between 1-10 based on their predicted likelihood of recidivism. A defendant is considered to have a low risk of recidivism if their COMPAS score is between 1-4 and medium to high risk of recidivism if their COMPAS score is between 5-10. In ProPublica’s seminal analysis of COMPAS, true positives, false positives, true negatives, and false negatives would be defined as follows:1\n\nA true positive is when a defendant receives a COMPAS score between 5-10 and is re-convicted in the two years after they were scored.\nA false positive is when a defendant receives a COMPAS score between 5-10 but is not re-convicted in the two years after they were scored.\nA true negative is when a defendant receives a COMPAS score between 1-4 and is not re-convicted in the two years after they were scored.\nA false negative is when a defendant receives a COMPAS score between 1-4 but is re-convicted in the two years after they were scored."
  },
  {
    "objectID": "blogs/understanding-fairness-metrics.html#conditional-probability",
    "href": "blogs/understanding-fairness-metrics.html#conditional-probability",
    "title": "Common Outcome-Based Algorithmic Fairness Metrics",
    "section": "1.2 Conditional Probability",
    "text": "1.2 Conditional Probability\nAnother important concept to understand when talking about statistical fairness metrics is conditional probability which is the probability of an event given that some other event occurs. For example, let A and B be two events, where B has a non-zero probability of occurring, then the conditional probability of A given B (i.e. \\mathbb{P}(A|B)) can be calculated as follows:\n\\mathbb{P}(A|B) = \\frac{\\mathbb{P}(A\\cap B)}{\\mathbb{P}(B)}\nIn real-life cases, we rarely know the true probability of events occurring. Instead, we must estimate this probability empirically from a sample. For instance, we can find the proportion of times that both event A and event B occurred out of the total number of times that event B occurred in our sample.\nOftentimes, in the context of outcome-based algorithmic fairness metrics, the relevant conditional probabilities are computed by comparing the true positives, false positives, true negatives, and/or false negatives between the relevant groups within a sample."
  },
  {
    "objectID": "blogs/understanding-fairness-metrics.html#add",
    "href": "blogs/understanding-fairness-metrics.html#add",
    "title": "Understanding (and Distinguishing) Common Algorithmic Fairness Metrics",
    "section": "3.1 ADD",
    "text": "3.1 ADD"
  },
  {
    "objectID": "blogs/understanding-fairness-metrics.html#defining-revelant-groups",
    "href": "blogs/understanding-fairness-metrics.html#defining-revelant-groups",
    "title": "Common Outcome-Based Algorithmic Fairness Metrics",
    "section": "3.2 Defining “Revelant Groups”",
    "text": "3.2 Defining “Revelant Groups”\nGroup-based fairness metrics (which includes all of the ones that I outline in this notebook) also require a condition to hold among “relevant groups”. In practice, “relevant groups” is treated as synonymous with groups that correspond to federally protected characteristics, which include race, color, religion, sex, sexual orientation, disability status, and age.3\nHowever, what identity group is “relevant” might change between contexts. Indeed, Lazar and Stone (2023) note that from the perspective of algorithmic justice, we care about comparing the model’s performance for “systematically disadvantaged groups” against that for “systematically advantaged groups” (see their Prioritarian Performance Principle), where a social ontology is needed to determine the relative social position of groups in a given context. This social ontology is often quite complicated in practice – reflecting that real-life social positions tend to be both complex and intersectional. For example, suppose we were trying to assess whether an hiring algorithm is unfair ADD.\nHence, group-based fairness metrics require us to define “socially salient” groups – often in a way that obfuscates the complexities of social positions for the sake of …ADD."
  },
  {
    "objectID": "blogs/understanding-fairness-metrics.html#additional-resources",
    "href": "blogs/understanding-fairness-metrics.html#additional-resources",
    "title": "Common Outcome-Based Algorithmic Fairness Metrics",
    "section": "3.3 Additional Resources",
    "text": "3.3 Additional Resources\n\n….\n…."
  },
  {
    "objectID": "blogs/understanding-fairness-metrics.html#impossibility-results",
    "href": "blogs/understanding-fairness-metrics.html#impossibility-results",
    "title": "Common Outcome-Based Algorithmic Fairness Metrics",
    "section": "3.1 Impossibility Results",
    "text": "3.1 Impossibility Results\nOne of the largest concerns in the Statistics and Machine Learning communities with outcome-based algorithmic fairness metrics is that it has been mathematically proven that three intuitively appealing outcome-based metrics for algorithmic fairness – (1) calibration within groups, (2) equal false positive rates, and (3) equal false negative rates – cannot be simultaneously satisfied when the base rates (i.e., the rate of positive instances) differ between the relevant groups (see Kleinberg, Mullainathan, and Raghavan (2016), Chouldechova (2017), and CITE). These results are often referred to as Impossibility Results for algorithmic fairness.\nA natural conclusion given the impossibility results is that algorithmic fairness is not possible when the base rates differ between relevant groups, which will be the case when there is pre-existing injustice, such as in bank lending, credit scores, hiring, education, and the criminal justice system. However, it is important to note that this conclusion relies on a hidden premise that all three of these outcome-based algorithmic fairness metrics – (1) calibration within groups, (2) equal false positive rates, and (3) equal false negative rates – all accurately capture the concept of fairness.\nHowever, I would argue that …ADD"
  },
  {
    "objectID": "about.html#ongoing-research-projects",
    "href": "about.html#ongoing-research-projects",
    "title": "About",
    "section": "Ongoing Research Projects",
    "text": "Ongoing Research Projects\nA shortlist of ongoing research projects (as of July 2025):\n\nPredicting Avoidance Ties in Avoidance Networks Leveraging only the Positive Network between the Same Individuals and Basic Node-Level Characteristics (Advanced Data Analysis project, in collaboration with Nynke Niezink and Eva Jaspers).\nSelecting Chip-Seq Normalization Methods from the Perspective of their Technical Conditions (in collaboration with Jo Hardin and Danae Schulz) – recently accepted to Briefings in Bioinformatics.\nAnalyzing Statistics Students’ Writing Before and After the Emergence of Large Language Models (in collaboration with Erin Franke) – presented progress during the Research Satellite at USCOTS 2025.\nTeaching Data Cleaning and Wrangling with R’s data.table Package (also in collaboration with Erin Franke) – presented at the Poster and Beyond Session at USCOTS 2025."
  },
  {
    "objectID": "about.html#ongoing-research-projects-as-of-may-2025",
    "href": "about.html#ongoing-research-projects-as-of-may-2025",
    "title": "About",
    "section": "Ongoing Research Projects (as of May 2025)",
    "text": "Ongoing Research Projects (as of May 2025)\n\nPredicting Avoidance Ties in Avoidance Networks Leveraging only the Positive Network between the Same Individuals and Basic Node-Level Characteristics (in collaboration with Nynke Niezink and Eva Jaspers)\nSelecting Chip-Seq Normalization Methods from the Perspective of their Technical Conditions (in collaboration with Jo Hardin and Danae Schulz)\n\nRevise and Resubmit, 2025\n\nAnalyzing Statistics Students’ Writing Before and After the Emergence of Large Language Models (in collaboration with my cohort-mate, Erin Franke)\n\nPresenting at the Research Satellite at USCOTS 2025\n\nTeaching Data Cleaning and Wrangling with R’s data.table Package (also in collaboration with my cohort-mate, Erin Franke)\n\nPresenting at the Poster and Beyond Session at USCOTS 2025"
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact",
    "section": "",
    "text": "skca2020@mymail.pomona.edu"
  },
  {
    "objectID": "contact.html#email",
    "href": "contact.html#email",
    "title": "Contact",
    "section": "",
    "text": "skca2020@mymail.pomona.edu"
  },
  {
    "objectID": "index.html#hello",
    "href": "index.html#hello",
    "title": "Sara Colando",
    "section": "Hello",
    "text": "Hello"
  },
  {
    "objectID": "blogs/projects.html",
    "href": "blogs/projects.html",
    "title": "TidyTuesday Creations",
    "section": "",
    "text": "TidyTuesday is a weekly community activity put on the the Data Science Learning Community. I try to spend a little time each week creating a data visualization or model with the data posted to the official TidyTuesday GitHub Repository (linked below)."
  },
  {
    "objectID": "blogs/projects.html#highlighted-creations",
    "href": "blogs/projects.html#highlighted-creations",
    "title": "TidyTuesday Creations",
    "section": "Highlighted Creations",
    "text": "Highlighted Creations\nHere are some of my favorite data visualizations that I have made from TidyTuesday over the past two years. Each title has a link to my code for creating the visualization.\n05/07/2024: Demographics of the Rolling Stone’s Top 500 Albums of All Time in 2003 vs. 2012 vs. 2020 \n\n\n\n\n\n03/05/2024: The Mr. Trash Wheel Fleet’s Collected Garbage over the Years \n\n\n\n\n\n11/28/2023: Dr. Who Distribution of Episode Rankings Based on the Episode Writer \n\n\n\n\n\n10/24/2023: Difference between the Taylor’s Version and the Old Version of Songs from Fearless and Red \n\n\n\n\n\n03/21/2023: Coding Language Creation over the Years \n\n\n\n\n\n12/20/2022: Seattle Weather in 2021 \n\n\n\n\n\n07/05/2022: Changes in Median Rent Prices and Percent of Apartments by Neighborhood in San Francisco"
  },
  {
    "objectID": "blogs/projects.html#helpful-resources",
    "href": "blogs/projects.html#helpful-resources",
    "title": "TidyTuesday Creations",
    "section": "Helpful Resources",
    "text": "Helpful Resources\nIf you are interested in participating in TidyTuesday yourself, here are some resources that I have found helpful for starting:\n\n1. Finding Inspiration\nWhen beginning TidyTuesdays, I find it super helpful to take inspiration from what others have done with the data either this week or in previous weeks. X (i.e., Twitter), Fossodon, and sometimes even Google searches are a great way of gaining inspiration from others!\n\n\n Search “#tidytuesday” on BlueSky\n\n\nhttps://bsky.app/hashtag/TidyTuesday\n\n\n\n\n Search “#tidytuesday” on Mastodon\n\n\nhttps://fosstodon.org/tags/tidytuesday\n\n\n\n\n2. Loading in the Data\nUsually, I read the data into an .Rmd file using the code block that looks like the following:\n```{r}\ndataset_1 &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2024/some_date/dataset_1.csv')\n```\n\n\n3. Analyzing and Visualizing the Data\nAfter that, the world (or, in this case, data) is your oyster! I primarily use Tidyverse to create my TidyTuesday data visualizations and models, so I appreciate the following cheat sheets:\n\n\n stringr Cheat Sheet PDF\n\n\nhttps://rstudio.github.io/cheatsheets/strings.pdf\n\n\n\n\n dplyr Cheat Sheet PDF\n\n\nhttps://rstudio.github.io/cheatsheets/data-transformation.pdf\n\n\n\n\n GGPlot2 Cheat Sheet PDF\n\n\nhttps://rstudio.github.io/cheatsheets/data-visualization.pdf"
  },
  {
    "objectID": "blogs/tidytuesday.html",
    "href": "blogs/tidytuesday.html",
    "title": "TidyTuesday Creations",
    "section": "",
    "text": "TidyTuesday is a weekly community activity put on the the Data Science Learning Community. I try to spend a little time each week creating a data visualization or model with the data posted to the official TidyTuesday GitHub Repository (linked below)."
  },
  {
    "objectID": "blogs/tidytuesday.html#highlighted-creations",
    "href": "blogs/tidytuesday.html#highlighted-creations",
    "title": "TidyTuesday Creations",
    "section": "Highlighted Creations",
    "text": "Highlighted Creations\nHere are some of my favorite data visualizations that I have made from TidyTuesday over the past two years. Each title has a link to my code for creating the visualization.\n05/07/2024: Demographics of the Rolling Stone’s Top 500 Albums of All Time in 2003 vs. 2012 vs. 2020 \n\n\n\n\n\n03/05/2024: The Mr. Trash Wheel Fleet’s Collected Garbage over the Years \n\n\n\n\n\n11/28/2023: Dr. Who Distribution of Episode Rankings Based on the Episode Writer \n\n\n\n\n\n10/24/2023: Difference between the Taylor’s Version and the Old Version of Songs from Fearless and Red \n\n\n\n\n\n03/21/2023: Coding Language Creation over the Years \n\n\n\n\n\n12/20/2022: Seattle Weather in 2021 \n\n\n\n\n\n07/05/2022: Changes in Median Rent Prices and Percent of Apartments by Neighborhood in San Francisco"
  },
  {
    "objectID": "blogs/tidytuesday.html#helpful-resources",
    "href": "blogs/tidytuesday.html#helpful-resources",
    "title": "TidyTuesday Creations",
    "section": "Helpful Resources",
    "text": "Helpful Resources\nIf you are interested in participating in TidyTuesday yourself, here are some resources that I have found helpful for starting:\n\n1. Finding Inspiration\nWhen beginning TidyTuesdays, I find it super helpful to take inspiration from what others have done with the data either this week or in previous weeks. X (i.e., Twitter), Fossodon, and sometimes even Google searches are a great way of gaining inspiration from others!\n\n\n Search “#tidytuesday” on BlueSky\n\n\nhttps://bsky.app/hashtag/TidyTuesday\n\n\n\n\n Search “#tidytuesday” on Mastodon\n\n\nhttps://fosstodon.org/tags/tidytuesday\n\n\n\n\n2. Loading in the Data\nUsually, I read the data into an .Rmd or .qmd file using a code block that looks like the following:\n```{r}\ndataset_1 &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2024/some_date/dataset_1.csv')\n```\n\n\n3. Analyzing and Visualizing the Data\nAfter that, the world (or, in this case, data) is your oyster! I primarily use Tidyverse to create my TidyTuesday data visualizations and models, so I appreciate the following cheat sheets:\n\n\n stringr Cheat Sheet PDF\n\n\nhttps://rstudio.github.io/cheatsheets/strings.pdf\n\n\n\n\n dplyr Cheat Sheet PDF\n\n\nhttps://rstudio.github.io/cheatsheets/data-transformation.pdf\n\n\n\n\n GGPlot2 Cheat Sheet PDF\n\n\nhttps://rstudio.github.io/cheatsheets/data-visualization.pdf"
  },
  {
    "objectID": "research/student-llms/students-llms.html",
    "href": "research/student-llms/students-llms.html",
    "title": "Analyzing Statistics Students’ Writing Before and After the Emergence of Large Language Models",
    "section": "",
    "text": "Overview: The use of Large Language Models (LLMs) has become ubiquitous in academic settings, particularly in written assignments (Baek et al., 2024). Reinhart et al. (2025) has identified systematic differences between human writing and LLMs by leveraging Biber feature and lemma usage rates.1 In this project, we investigate whether (and if so, how) students’ statistics writing has systematically shifted toward being more similar to LLM academic writing since LLMs became widely accessible in 2022. We compare student writing to LLM academic writing through two corpora. The HAP-E Corpus contains 1,227 documents for which ChatGPT-4o (August 2024) was asked to generate the next 500 words (in the same tone and style) when prompted with a piece of academic writing (Brown, 2024). Meanwhile, the Student Corpus contains 2,353 student reports from three undergraduate statistics courses at Carnegie Mellon University: 36-202, 36-401, and 36-402. 36-202: Methods for Statistics & Data Science is a lower division course, which is typically the second statistics course that students take. 36-401: Modern Regression and 36-402: Advanced Methods for Data Analysis are upper division courses that statistics majors take their junior or senior year. For the reports, students are given a dataset and asked to answer a domain question with a report in the IMRaD format.2 We find that there systematic shift in both the style and vocabulary of students’ statistics reports toward ChatGPT’s academic writing style in both lower and upper division courses at Carnegie Mellon since 2022. In particular, the writing style of students’ introductions and conclusions has become more similar to ChatGPT’s writing style, on average.\nWe are currently in the progress of figuring out next steps for this project. For more details on our current results and potential next steps, see Erin’s write-up."
  },
  {
    "objectID": "research/student-llms/students-llms.html#footnotes",
    "href": "research/student-llms/students-llms.html#footnotes",
    "title": "Analyzing Statistics Students’ Writing Before and After the Emergence of Large Language Models",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nBiber features are a set of 67 rhetorical features (e.g., frequency of past tense, participial clauses, mean word length) used to characterize texts (Biber, 1988). Lemmas are the “root” form of a word (e.g. ensure, ensured, and ensures all share the same lemma of ensure).↩︎\nIMRaD stands for Introduction, Methods, Results, and Discussion. Generally, students are also asked to write a one-page executive summary which is analogous to an abstract in academic writing.↩︎"
  },
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "As a Graduate Teaching Assistant at Carnegie Mellon University:\n\n36-402: Undergraduate Advanced Data Analysis (Spring 2026)\n36-707: Regression Analysis, guest lectured on penalized regression (Fall 2025)\nCarnegie Mellon Sports Analytics Camp, co-taught four lectures to REU students (Summer 2025)\n36-226: Introduction to Statistical Inference, Head TA (Spring 2025)\n36-600: Overview of Statistical Learning and Modeling (Fall 2024)\n\nAs a Undergraduate Teaching Assistant at Pomona College:\n\nMATH158: Statistical Linear Models (Spring 2024)\nMATH058B: Introduction to Biostatistics (Spring 2023)\nID001: First-Year Interdisciplinary Seminar: I Disagree (Fall 2023)\nBIOL040: Introductory Genetics with Lab (Fall 2022)\nMATH060: Linear Algebra (Fall 2021)\n1-2-1 Summer Bridge Program (Summer 2021)"
  },
  {
    "objectID": "teaching.html#teaching-experiences",
    "href": "teaching.html#teaching-experiences",
    "title": "Teaching",
    "section": "Teaching Experiences",
    "text": "Teaching Experiences\nAs a Graduate Student:\n\n\n\nAs an Undergraduate Student:"
  },
  {
    "objectID": "teaching.html#teaching-related-research",
    "href": "teaching.html#teaching-related-research",
    "title": "Teaching",
    "section": "Teaching-Related Research",
    "text": "Teaching-Related Research\n\nAnalyzing Statistics Students’ Writing Before and After the Emergence of Large Language Models; presented findings at the Research Satellite poster session during USCOTS 2025; manuscript in progress.\nTeaching Data Cleaning and Wrangling with R’s data.table Package; presented at Posters and Beyond during USCOTS 2025.\nPhilosophy within Data Science Ethics Courses; published in the Journal of Statistics and Data Science Education, 2024."
  },
  {
    "objectID": "teaching.html#teaching-experience",
    "href": "teaching.html#teaching-experience",
    "title": "Teaching",
    "section": "",
    "text": "As a Graduate Teaching Assistant at Carnegie Mellon University:\n\n36-402: Undergraduate Advanced Data Analysis (Spring 2026)\n36-707: Regression Analysis, guest lectured on penalized regression (Fall 2025)\nCarnegie Mellon Sports Analytics Camp, co-taught four lectures to REU students (Summer 2025)\n36-226: Introduction to Statistical Inference, Head TA (Spring 2025)\n36-600: Overview of Statistical Learning and Modeling (Fall 2024)\n\nAs a Undergraduate Teaching Assistant at Pomona College:\n\nMATH158: Statistical Linear Models (Spring 2024)\nMATH058B: Introduction to Biostatistics (Spring 2023)\nID001: First-Year Interdisciplinary Seminar: I Disagree (Fall 2023)\nBIOL040: Introductory Genetics with Lab (Fall 2022)\nMATH060: Linear Algebra (Fall 2021)\n1-2-1 Summer Bridge Program (Summer 2021)"
  },
  {
    "objectID": "blogs/DA-exam-materials.html",
    "href": "blogs/DA-exam-materials.html",
    "title": "Data Analysis Exam Thoughts and Materials",
    "section": "",
    "text": "This Spring, at the end of my first-year, I took the Data Analysis (DA) exam, which is the “qualifying” exam for CMU’s Statistics PhD program. The majority of the materials in this notebook are based on the Fall 2024 Regression Analysis (36-707) lecture notes, which can be found here.1\nA little background on CMU’s DA Exam from the Statistics Graduate Student Handbook: “At the conclusion of each Spring Semester the Department administers the ‘Data Analysis Exam,’ which is designed to test students’ ability to apply statistical methods to address a substantive, real problem. Students are given eight hours to complete the exam, during which time they analyze the data and write a [ten page] report to present their analysis and conclusions. The faculty are realistic as to what can be accomplished during the eight-hour period. In grading the exam, the faculty are looking for clear presentation of an appropriate analysis of the data. Emphasis is not placed on technical or mathematical sophistication,” (p. 13)."
  },
  {
    "objectID": "blogs/DA-exam-materials.html#footnotes",
    "href": "blogs/DA-exam-materials.html#footnotes",
    "title": "Data Analysis Exam Thoughts and Materials",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThanks to Alex Reinhart for compiling these!↩︎\nI did an Executive Summary + IMRaD (Introduction, Methods, Results, and Discussion) report for the exam.↩︎"
  },
  {
    "objectID": "blogs/DA-materials/model-types.html",
    "href": "blogs/DA-materials/model-types.html",
    "title": "Model Types and Diagnostics",
    "section": "",
    "text": "For the linear regression model \\mathbf{Y} = \\mathbf{X}\\beta + e, we have the following assumptions:\n\n\nErrors have mean 0: \\mathbb{E}[Y \\mid X] = 0\n\nThe error variance is constant: \\text{Var}[e \\mid \\mathbf{X}] = \\sigma^2\n\nThe errors are uncorrelated (the data points are iid): \\text{Var}[e \\mid \\mathbf{X}] = \\sigma^2\n\nThe errors are normally distributed\n\n\n\n\n\n\nLinearity: partial residual plots help us diagnose whether the true relationship between the outcome and some of the predictors is non-linear\n\n\nShow the code\nfit &lt;- lm(price2007 ~ distance + resid_walkscore + squarefeet + bedgroup + \n            zip, data = rail_trail_modeling)\n\nfit_new &lt;- lm(price2007 ~ distance + ns(resid_walkscore, knots = c(0.5)) + \n                squarefeet + bedgroup + zip, data = rail_trail_modeling)\n\np1 &lt;- partial_residuals(fit) |&gt;\n  mutate(.predictor_name = case_when(\n    .predictor_name == \"distance\" ~ \"Distance to Rail Trail\",\n    .predictor_name == \"walkscore\" ~ \"Walking Score\",\n    .predictor_name == \"bikescore\" ~ \"Biking Score\",\n    .predictor_name == \"squarefeet\" ~ \"Square Footage\",\n    .predictor_name == \"resid_walkscore\" ~ \"Residual Walk Score\",\n    TRUE ~ .predictor_name)) |&gt;\n  ggplot(aes(x = .predictor_value, y = .partial_resid)) +\n  geom_point(color = \"grey40\", size = 0.5) +\n  geom_smooth(color = \"blue\", fill = 'skyblue2',\n              linewidth = 0.7, linetype = \"twodash\") +\n  geom_line(aes(y = .predictor_effect), color = 'red') +\n  facet_wrap(vars(.predictor_name), scales = \"free\", ncol = 2) +\n  labs(x = \"Predictor value\", y = \"Partial residual\")+\n  ggtitle(\"(a) Partial Residual Plots,\\nPreliminary Linear Model\")+\n  theme_bw()+\n  theme(strip.background = element_rect(fill = \"white\", color = \"white\"),\n        strip.text = element_text(color = \"black\", face = \"bold\"),\n        axis.text = element_text(color = \"black\"),\n        plot.title = element_text(face = \"bold\", size = 11))\n\np2 &lt;- partial_residuals(fit_new) |&gt;\n  mutate(.predictor_name = case_when(\n    .predictor_name == \"distance\" ~ \"Distance to Rail Trail\",\n    .predictor_name == \"walkscore\" ~ \"Walking Score\",\n    .predictor_name == \"bikescore\" ~ \"Biking Score\",\n    .predictor_name == \"squarefeet\" ~ \"Square Footage\",\n    .predictor_name == \"resid_walkscore\" ~ \"Residual Walk Score\",\n    TRUE ~ .predictor_name)) |&gt;\n  ggplot(aes(x = .predictor_value, y = .partial_resid)) +\n  geom_point(color = \"grey40\", size = 0.5) +\n  geom_smooth(color = \"blue\", fill = 'skyblue2',\n              linewidth = 0.7, linetype = \"twodash\") +\n  geom_line(aes(y = .predictor_effect), color = 'red') +\n  facet_wrap(vars(.predictor_name), scales = \"free\", ncol = 2) +\n  labs(x = \"Predictor value\", y = \"Partial residual\")+\n  ggtitle(\"(b) Partial Residual Plots, Adding\\nNatural Splines (Knot at Median)\\nto Residual Walk Score\")+\n  theme_bw()+\n  theme(strip.background = element_rect(fill = \"white\", color = \"white\"),\n        strip.text = element_text(color = \"black\", face = \"bold\"),\n        axis.text = element_text(color = \"black\"),\n        plot.title = element_text(face = \"bold\", size = 11))\n\np1 + p2\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrom Figure 1(a), we see that residual walk score has a nonlinear relationship with estimated house price in 2007 – as indicated by the smoothed residual curve (in blue) diverging from the predicted effect line (in red). We would address this nonlinearity by fitting natural splines on the residual walk score, with one knot at the median. Figure 1(b) provides the updated partial residual plots when we add natural splines to the residual walk score. We see now that the predicted effect line (in red) tracks closely to the smoothed residual curve (in blue) without over-fitting to model to the turns in the smoothed residual curve.\nLineups: If we aren’t sure whether the true relationship between the outcome and a given predictor is non-linear, we can conduct a partial residual visual lineup.\nFor instance, it is not entirely clear whether distance has a non-linear relationship to estimated house price in 2007 or if this is just due to the high-leverage home with the largest distance to the rail trail. Figure 2 shows the partial residual line-up. If we cannot pick out the partial residual plot corresponding to the real data from the simulated data, we can conclude that there is insufficient evidence of a non-linear relationship between distance to rail trail and estimated house price in 2007 at a significance level of 0.05.\n\n\nShow the code\nset.seed(123)\nmodel_lineup(fit_new, fn = partial_residuals, nsim = 20) |&gt;\n  filter(.predictor_name == \"distance\") |&gt;\n  mutate(.sample_new = paste0(\"Position \", .sample),\n         .sample_new = fct_reorder(.sample_new, .sample)) |&gt;\n  ggplot(aes(x = .predictor_value, y = .partial_resid))+\n  geom_point(color = \"grey40\", size = 0.5) +\n  geom_line(aes(y = .predictor_effect), color = 'red') +\n  geom_smooth(color = \"blue\", fill = 'skyblue2',\n              linewidth = 0.7, linetype = \"twodash\") +\n  facet_wrap(~.sample_new)+\n  labs(x = \"Distance to Rail Trail (in Miles)\", y = \"Partial residual\")+\n  theme_bw()+\n  theme(strip.background = element_rect(fill = \"white\", color = \"white\"),\n        strip.text = element_text(color = \"black\", face = \"bold\"),\n        axis.text = element_text(color = \"black\"),\n        plot.title = element_text(face = \"bold\", size = 11))\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can decrypt (show the the position of our actual data) by running the command, which is given at the end of the line-up call (dycrypt()):\n\ndecrypt(\"QUg2 qFyF Rx 8tLRyRtx ZP\")\n\n[1] \"True data in position  16\"\n\n\n\n\n\nAdd polynomial terms to the model\n\nSimplest way to add polynomial terms is with I():\n\nfit_new &lt;- lm(price2007 ~ distance + resid_walkscore + I(resid_walkscore^2) +\n                squarefeet + bedgroup + zip, data = rail_trail_modeling)\n\nIf there is numerical issues (e.g., from the predictor having large values), we can use the poly()function which constructs the design matric using an orthogonal polynomial basis.\nA downside is that the coefficients are no longer is terms of the interpretable regressor terms. The fitted values though wil be the same between the two approaches for adding polynomial terms.\n\nfit_new &lt;- lm(price2007 ~ distance + poly(resid_walkscore, degree = 2) + squarefeet + \n                bedgroup + zip, data = rail_trail_modeling)\n\n\nAdd regression splines to the model\n\nRegression splines model relationships as being piecewise polynomial. They require us to choose knots, which are the fixed points between which the function is polynomial. Two common approaches are natural splines and B-spline basis. Statisticians almost universally use cubic splines. Cubic splines are continuous in their first and second derivatives, making the knots almost visually imperceptible in plots of the spline.\nFor knot selection, a typical approach is to use quantiles of the data as knots – erring on the side of less knots (especially if there is few observations).\n\nfit_new &lt;- lm(price2007 ~ distance + ns(resid_walkscore, knots = c(0.5)) + \n                squarefeet + bedgroup + zip, data = rail_trail_modeling)\n\n\n\n\n\nErrors with non-constant variance are called heteroskedastic. To detect heteroskedasticity, we can examine the residuals plotted against the fitted values.\n\n\nShow the code\nfit_new |&gt;\n  augment() |&gt;\n  ggplot(aes(x = .fitted, y = .resid))+\n  geom_point(size = 0.5)+\n  geom_hline(yintercept = 0)+\n  geom_smooth(fill = 'skyblue2')+\n  labs(x = \"Fitted Values\", y = \"Residuals\")+\n  theme_bw()+\n  theme(axis.text = element_text(color = \"black\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn Figure 3, we have non-constant error variance. Indeed, there appears to be an approximately quadratic relationship between our fitted values and our residual terms.\nWe can remedy non-constant error variance by using the sandwich estimator for the variance of our estimators in all inference.\n\n# uses sandwich estimator with HC3 estimator for Omega\nConfint(fit_new, vcov = vcovHC(fit_new))\n\nStandard errors computed by vcovHC(fit_new) \n\n\n                                       Estimate      2.5 %     97.5 %\n(Intercept)                           48.607106 -15.216646 112.430857\ndistance                             -14.779103 -23.768308  -5.789898\nns(resid_walkscore, knots = c(0.5))1  93.909271 -17.234103 205.052646\nns(resid_walkscore, knots = c(0.5))2 -18.347598 -85.985661  49.290465\nsquarefeet                           145.196077 122.379951 168.012202\nbedgroup3 beds                         7.595663  -9.282322  24.473649\nbedgroup4+ beds                      -28.637850 -55.631857  -1.643844\nzip1062                              -30.040888 -50.154050  -9.927726\n\n\n\n\n\nWe can check for non-normality of errors using residual Q-Q plots. Using the standardized residuals, we can detect gross deviations from normality while ignoring small deviations in non-normality. An example of a Q-Q plot for linear regression is in Figure 4. Since we are doing linear regression, the theoretical quantiles correspond to a normal distribution with mean 0 and variance 1.\n\n\nShow the code\naugment(fit_new) |&gt;\n  ggplot(aes(sample = .std.resid)) +\n  geom_qq_line(linewidth = 0.4) +\n  geom_qq(size = 0.75)+\n  labs(x = 'Theoretical Quantiles',\n       y = 'Observed Quantiles')+\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\n\n\n\nLike with partial residuals, we can use a lineup to detect whether the non-normality in errors is statistically significant (at a significance level of 0.05).\n\n\nShow the code\nset.seed(123)\nmodel_lineup(fit_new) |&gt; \n  mutate(.sample_new = paste0(\"Position \", .sample),\n         .sample_new = fct_reorder(.sample_new, .sample)) |&gt; \n  ggplot(aes(sample = .std.resid)) +\n  geom_qq_line(linewidth = 0.4) +\n  geom_qq(size = 0.5) +\n  facet_wrap(vars(.sample_new)) +\n  labs(x = \"Theoretical Quantiles\",\n       y = \"Observed Quantiles\")+\n  theme_bw()+\n  theme(strip.text = element_text(face = \"bold\"),\n        strip.background = element_rect(fill = \"white\", color = \"white\"),\n        text = element_text(color = \"black\"),\n        axis.text = element_text(color = \"black\"))\n\n\n\n\n\n\n\n\n\n\n\n[1] \"True data in position  16\"\n\n\nNote: if the sample size is sufficiently large, then violating non-normality of errors is not a serious concern. Otherwise, if the assumptions linearity and constant error variance hold, then interpret coefficient estimates with caution (since coefficient estimates would not be approximately normal).\n\n\n\n\nOutliers: Not all outliers have a major effect on the regression line. So, their presence may not be a problem. We hence need a way to detect outliers and characterize their influence on the regression line.\n\nIf we can determine the outlier is due to some kind of measurement or recording error, we can correct the error.\nIf we cannot, we must make the difficult decision of whether to keep the outlier, and acknowledge that it may significantly influence our estimates, or to remove it and risk throwing away good information\n\nOne way to measure the influence of an observation on the regression is to quantify how its inclusion changes our coefficient estimates via Cook’s Distance.\n\nLook for Cook’s Distance (D_i), where D_i \\geq 1, though this is again a matter of judgment, to indicate that a particular observation substantially changes the regression fit.\n\nFigure 5 shows that all observations have Cook’s Distances well below the standard cut-off of 1, so there does not appear that any of the homes in the data have a large influence on our regression model.\n\n\nShow the code\nmax_price &lt;- augment(fit_new) |&gt;\n  arrange(desc(price2007)) |&gt;\n  slice_head(n = 1)\n\naugment(fit_new) |&gt;\n  ggplot(aes(y = .cooksd, x = price2007))+\n  geom_point(size = 0.7, alpha = 0.7)+\n  geom_point(data = max_price, aes(y = .cooksd, x = price2007),\n             size = 2, color = \"red\")+\n  labs(y = \"Cook's Distance\", x = \"Estimated House Price (2007)\")+\n  #geom_hline(yintercept = 1, linetype = \"dashed\")+\n  theme_bw()+\n  theme(plot.title = element_blank(),\n        text = element_text(color = \"black\"),\n        axis.text = element_text(color = \"black\"),\n        axis.title = element_text(color = \"black\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhen the model matrix X is not full column rank, its columns span a lower-dimensional space. There is still a unique perpendicular projection of Y onto this space, and hence a unique \\hat Y. But the coordinates of that fit are not unique, meaning that there are infinite values of \\hat \\beta that correspond to the same squared error/prediction. If we care about inference on \\hat \\beta this is a serious problem.\n\n\nShow the code\nloess_wrapper &lt;- function(data, mapping, ...){\n      p &lt;- ggplot(data = data, mapping = mapping) + \n      geom_point(size = 0.35, color = \"skyblue2\") + \n      geom_smooth(linetype = \"dashed\", color = \"black\", linewidth = 0.6)\n      return(p)\n}\n\nrail_trail_modeling |&gt;\n  select(price2007, distance, walkscore, squarefeet) |&gt;\n  rename(`2007 Est.\\nHouse Price\\n(thousands of $)` = price2007,\n         `Distance to\\nRail Trail (miles)` = distance,\n         `Walk\\nScore` = walkscore,\n         `Square\\nFootage` = squarefeet) |&gt;\n  ggpairs(upper = list(continuous = wrap(\"cor\", color = \"black\")),\n          lower = list(continuous = loess_wrapper,\n                       combo = wrap(\"barDiag\", size=0.2,\n                                     alpha = 0.9))) +\n  scale_x_continuous(breaks = scales::pretty_breaks(n = 4))+\n  scale_y_continuous(breaks = scales::pretty_breaks(n = 4))+\n  theme_bw()+\n  theme(strip.background = element_rect(fill = \"white\", color = \"white\"),\n        strip.text = element_text(color = \"black\", face = \"bold\", size = 6.5),\n        axis.text = element_text(color = \"black\", size = 6.5))\n\n\n\n\n\n\n\n\n\n\n\nCondition Number:\nHowever, multiple variables can be collinear even when individual pairs of variables have low correlation. One way to detect non-pairwise collinearity is to use the condition number, where we compare the largest and smallest eigenvalues of X^\\intercal X:\n\nX &lt;- model.matrix(price2007 ~ distance + resid_walkscore + \n                squarefeet + bedgroup + zip - 1, data = rail_trail_modeling)\n# first step is to scale the model matrix X\nscaled_x &lt;- scale(X, center = FALSE, scale = TRUE)\n  \n# Then, we can use the kappa() function on X'X\nkappa(t(scaled_x) %*% scaled_x)\n\n[1] 123.3692\n\n\nIdeally, we want the condition number to be small. The general cut-off is condition numbers larger than 50 or 100. So, from the results above, we see that there is potentially concerning levels of collinearity among our predictors, which could influence the accuracy of our inference.\nVariance Inflation Factors:\nAnother way to look at collinearity is with variance inflation factors (VIF), which can be interpreted as showing how much the variance of \\hat \\beta_j is inflated relative to a model where there is no collinearity (and all columns are orthogonal).\n\n# consider using type = 'predictor' for interactions (or ignoring VIFs since compared to main effecs)\ncar::vif(lm(price2007 ~ distance + ns(resid_walkscore, knots = c(0.5)) + \n                squarefeet + bedgroup + zip, data = rail_trail_modeling))\n\n                                        GVIF Df GVIF^(1/(2*Df))\ndistance                            1.379898  1        1.174691\nns(resid_walkscore, knots = c(0.5)) 1.409755  2        1.089647\nsquarefeet                          1.951119  1        1.396825\nbedgroup                            1.725889  2        1.146181\nzip                                 1.342293  1        1.158574\n\n\nA general rule of thumb is that VIF values exceeding 5 warrant further investigation, and VIFs exceeding 10 are signs of serious multicollinearity requiring correction.\nGeneral Notes: If we are confronted with collinearity, then, the question to ask is:\n\n“Have we chosen the correct predictors for the research question?”\n\nIf we have, there is little to be done; if we have not, we can reconsider our choice of predictors and perhaps eliminate the collinear ones.\nIf we are interested in prediction rather than in the coefficients, collinearity is a problem insofar as it creates high prediction variance, and we might reconsider our model and use a penalization model to reduce the prediction variance.s\n\n\n\n\nAn interaction allows one predictor’s association with the outcome to depend on values of another predictor.\nWhen an interaction is present, the normal interpretation of coefficients as slopes no longer holds for the predictors involved in the interaction.\nSee Example 7.4\n\n\n\n\n\nMany useful null hypotheses can be written in terms of linear combinations of the coefficients. For example, consider a linear model with a continuous predictor X_1 and dummy-coded regressor X_2 \\in \\{0,1 \\}:\nY = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_1 X_2 We could think of several null hypotheses to test:\n\n\\beta_2 = 0: the two factor levels have identical intercepts\n\\beta_3 = 0: the two factor levels have identical slopes\n\\beta_1 = c: the slope when X_2 = 0 is some value c predicted by a theory we are testing (usually c = 0 in linear regression, which corresponds to no association)\n\\beta_1 + \\beta_3 = 0: when X_2 = 1, there is no association between X_1 and Y\n\\beta_2 = \\beta_3 = 0: the two factor levels have identical relationships between X_1 and Y\n\nFor all but the last listed hypothesis, we can use a t test. R conducts a t test for every coefficient by default, with c = 0.\nThe degrees of freedom for a t test is n - p, where p is the number of parameters in our model. We can get this via fit$df.residual. E.g., for our rail trail linear model example we have df = 96.\nThe function tidy() does the test for c = 0 with every other predictor held constant in the model.\n\n\nShow the code\ngt(tidy(fit_new)) |&gt;\n  fmt_number(decimals = 3)\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n48.607\n27.295\n1.781\n0.078\n\n\ndistance\n−14.779\n5.039\n−2.933\n0.004\n\n\nns(resid_walkscore, knots = c(0.5))1\n93.909\n43.190\n2.174\n0.032\n\n\nns(resid_walkscore, knots = c(0.5))2\n−18.348\n22.890\n−0.802\n0.425\n\n\nsquarefeet\n145.196\n10.082\n14.401\n0.000\n\n\nbedgroup3 beds\n7.596\n12.269\n0.619\n0.537\n\n\nbedgroup4+ beds\n−28.638\n15.037\n−1.905\n0.060\n\n\nzip1062\n−30.041\n9.438\n−3.183\n0.002\n\n\n\n\n\n\n\nWe can also use tbl_regression() which gives us the confidence intervals and p-values:\n\n\nShow the code\ntbl_regression(fit_new)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nBeta\n95% CI\np-value\n\n\n\n\ndistance\n-15\n-25, -4.8\n0.004\n\n\nns(resid_walkscore, knots = c(0.5))\n\n\n\n\n\n\n\n\n    ns(resid_walkscore, knots = c(0.5))1\n94\n8.2, 180\n0.032\n\n\n    ns(resid_walkscore, knots = c(0.5))2\n-18\n-64, 27\n0.4\n\n\nsquarefeet\n145\n125, 165\n&lt;0.001\n\n\nbedgroup\n\n\n\n\n\n\n\n\n    1-2 beds\n—\n—\n\n\n\n\n    3 beds\n7.6\n-17, 32\n0.5\n\n\n    4+ beds\n-29\n-58, 1.2\n0.060\n\n\nzip\n\n\n\n\n\n\n\n\n    1060\n—\n—\n\n\n\n\n    1062\n-30\n-49, -11\n0.002\n\n\n\nAbbreviation: CI = Confidence Interval\n\n\n\n\n\n\n\n\nIf we have non-constant error variance and thus use the sandwich estimator, we can use the following code to conduct t tests and present our results:\n\np.values &lt;- coeftest(fit_new, vcov = vcovHC(fit_new)) |&gt;\n  tidy() |&gt; _$p.value\n\nt.stats &lt;- coeftest(fit_new, vcov = vcovHC(fit_new)) |&gt;\n  tidy() |&gt; _$statistic\n\ndata_table &lt;- as.data.frame(Confint(fit_new, vcov = vcovHC(fit_new))) |&gt;\n  cbind(t.stats) |&gt;\n  cbind(p.values) |&gt;\n  rownames_to_column(var = \"Characteristic\") |&gt;\n  rename(`p-value` = p.values,\n         `t-stat` = t.stats)\n\nStandard errors computed by vcovHC(fit_new) \n\ngt(data_table) |&gt;\n  fmt_number(decimals = 3)\n\n\n\n\n\n\n\nCharacteristic\nEstimate\n2.5 %\n97.5 %\nt-stat\np-value\n\n\n\n\n(Intercept)\n48.607\n−15.217\n112.431\n1.512\n0.134\n\n\ndistance\n−14.779\n−23.768\n−5.790\n−3.264\n0.002\n\n\nns(resid_walkscore, knots = c(0.5))1\n93.909\n−17.234\n205.053\n1.677\n0.097\n\n\nns(resid_walkscore, knots = c(0.5))2\n−18.348\n−85.986\n49.290\n−0.538\n0.592\n\n\nsquarefeet\n145.196\n122.380\n168.012\n12.632\n0.000\n\n\nbedgroup3 beds\n7.596\n−9.282\n24.474\n0.893\n0.374\n\n\nbedgroup4+ beds\n−28.638\n−55.632\n−1.644\n−2.106\n0.038\n\n\nzip1062\n−30.041\n−50.154\n−9.928\n−2.965\n0.004\n\n\n\n\n\n\n\nOf course, a fourth option is to manually perform the t test, which might make sense if we have a complex null or would like to do a one-sided test. Below is the general code for this:\n\nbeta_hats &lt;- coef(gentoo_fit)\na &lt;- c(0, 0, 0, 1) # beta_3 = 0\n\nt_stat &lt;- sum(a * beta_hats) /\n  sqrt(a %*% vcov(gentoo_fit) %*% a)[1,1]\n\np_value &lt;- pt(t_stat, gentoo_fit$df.residual,\n              lower.tail = FALSE)\n\np_value * 2 # if two-sided\n\n\n\n\nThe anova() function can be used to conduct F tests for nested models. For instance, we can test whether there was a significant association between residual walk score and estimated home price in 2007 (in practice, probably not a good idea since residual walk score is random):\n\n\nShow the code\n# create a model without the predictor\nfit_no_resid_ws &lt;- lm(price2007 ~ distance + \n                squarefeet + bedgroup + zip, data = rail_trail_modeling)\n\nanova(fit_no_resid_ws, fit_new)\n\n\nAnalysis of Variance Table\n\nModel 1: price2007 ~ distance + squarefeet + bedgroup + zip\nModel 2: price2007 ~ distance + ns(resid_walkscore, knots = c(0.5)) + \n    squarefeet + bedgroup + zip\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)  \n1     98 170673                             \n2     96 160689  2    9983.3 2.9821 0.0554 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe parameters of an F-statistic are q and n - p. Df is the outputted table corresponds to q and the second row of Res.Df corresponds to n - p. E.g, we have F(2, 96) as our null distribution in this example.\n\n\n\n\nIn a report, we just include full interpretations for the hypothesis tests and/or coefficient estimates that relevant to the practical problem at hand!\n\n\nShow the code\nlibrary(palmerpenguins)\n\npenguin_fit &lt;- lm(\n  bill_length_mm ~ flipper_length_mm + species +\n    flipper_length_mm:species,\n  data = penguins\n)\ntbl_regression(penguin_fit)\n\n\n\n\nTable 1: ADD\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nBeta\n95% CI\np-value\n\n\n\n\nflipper_length_mm\n0.13\n0.07, 0.20\n&lt;0.001\n\n\nspecies\n\n\n\n\n\n\n\n\n    Adelie\n—\n—\n\n\n\n\n    Chinstrap\n-8.0\n-29, 13\n0.4\n\n\n    Gentoo\n-34\n-54, -15\n&lt;0.001\n\n\nflipper_length_mm * species\n\n\n\n\n\n\n\n\n    flipper_length_mm * Chinstrap\n0.09\n-0.02, 0.19\n0.10\n\n\n    flipper_length_mm * Gentoo\n0.18\n0.09, 0.28\n&lt;0.001\n\n\n\nAbbreviation: CI = Confidence Interval\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe intercept \\beta_0 is the mean value of when all other regressors are 0:\n\\mathbb{E}[Y \\mid X_1 = 0, \\dots, X_q = 0] Interpreting the intercept, quoting confidence intervals, and conducting hypothesis tests for its value may only be useful when the intercept has a substantive meaning and we have observed X values nearby.\n\n\n\nNote these are the general parts of an interpretation for a continuous predictor in a linear regression:\n\nDifference in the mean value of Y\nAssociated with a one-unit increase in X_1\nHolding all other regressors constant\n\nExact interpretation depends on if the predictor is continuous or a factor level:\n\nTest for a factor coefficient: For a given flipper length, gentoo penguins have a smaller mean bill length (M = 47.5 mm, SD = 3.1) than Adelie penguins (M = 38.8 mm, SD = 2.7), t(336) = -3.5, p = 0.001.\n\nIn part calculated using group_by():\n\n\nShow the code\npenguins |&gt;\n  group_by(species)|&gt;\n  summarize(mean_bill_length =\n              mean(bill_length_mm,\n                 na.rm = TRUE)) |&gt;\n  gt() |&gt;\n  fmt_number(decimals = 1)\n\n\n\n\n\n\n\n\nspecies\nmean_bill_length\n\n\n\n\nAdelie\n38.8\n\n\nChinstrap\n48.8\n\n\nGentoo\n47.5\n\n\n\n\n\n\n\n\nConfidence interval for a factor coefficient: For a given flipper length, gentoo penguin bills are shorter than Adelie penguin bills by an average of 34.3 mm (95% CI [15.01, 15.64]).\nTest for slope: In Adelie penguins, there was a statistically significant association between flipper length and bill length, \\hat \\beta = 0.13, t(336) = 4.17, p &lt; 0.001.\nDescribe a slope: Among Adelie penguins, each additional millimeter of flipper length is associated with 0.13 mm of additional bill length, on average (95% CI [0.07, 0.2]).\n\n\n\n\nDescribing an interaction coefficient: For the interaction term between flipper length and species from Table 1: For Adelie penguins (the baseline level), the association between bill length and flipper length is 0.13 mm of bill length per millimeter of flipper length, on average (95% CI [0.07, 0.20]). But for Chinstrap penguins, the association is 0.13 + 0.09 = 0.22 mm of bill length per millimeter of flipper length, on average (95% CI [0.11, 0.31]). [LOOK AT THEOREM 5.5 AND HW 3 TO CONFIRM]\nA helpful way to visualize interaction terms, polynomial terms, (or other complex relationships) is to use an effects plot:\n\npredict_response(penguin_fit,\n                 terms = c(\"flipper_length_mm\", \"species\")) |&gt;\n  plot() +\n  labs(x = \"Flipper length (mm)\", y = \"Bill length (mm)\",\n       color = \"Species\",\n       title = \"Flipper length effect plot\")\n\n\n\n\n\n\n\n\nIn an effects plot, non-focal predictors are set to their mean (numeric variables), reference level (factors), or “most common” value (mode) in case of character vectors.\n\n\n\nIf we fit a spline or a polynomial for our predictor-of-interest, we cannot determine the statistical significance or effect size from the outputted regression tables. Instead, we must use the following framework:\n\nDo a LRT/F-test for nested models to obtain the overall significance of the predictor-of-interest.\n\n\nanova(fit_new, fit_no_resid_ws)\n\nAnalysis of Variance Table\n\nModel 1: price2007 ~ distance + ns(resid_walkscore, knots = c(0.5)) + \n    squarefeet + bedgroup + zip\nModel 2: price2007 ~ distance + squarefeet + bedgroup + zip\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)  \n1     96 160689                             \n2     98 170673 -2   -9983.3 2.9821 0.0554 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nCreate an effects plot fort the variable of interest\n\n\npredict_response(fit_new,\n                 terms = c(\"resid_walkscore\")) |&gt;\n  plot()\n\n\n\n\n\n\n\n\n\nGive an example of an interpretation using the effects plot with specific values\n\n\npreds &lt;- predict_response(fit_new, terms = c(\"resid_walkscore\"))\nhead(preds)\n\n# Predicted values of price2007\n\nresid_walkscore | Predicted |         95% CI\n--------------------------------------------\n         -43.01 |    259.57 | 213.44, 305.71\n         -36.08 |    271.78 | 233.60, 309.96\n         -31.03 |    280.37 | 247.16, 313.58\n         -29.04 |    283.65 | 252.14, 315.16\n         -25.22 |    289.68 | 260.94, 318.43\n         -25.21 |    289.70 | 260.96, 318.44\n\nAdjusted for:\n*   distance =     1.11\n* squarefeet =     1.57\n*   bedgroup = 1-2 beds\n*        zip =     1060\n\n\nFor example, when the residual walk score is -43.01, the predicted house price (in 2007) is $259,570 in Northampton, MA (95% CI [\\$213,440, \\$305,710]) versus when the residual walk score is -36.08, the predicted house price (in 2007) is $271,780 in Northampton, MA (95% CI [\\$233,600, \\$309,960]) – holding all other covariates at their mean or reference level."
  },
  {
    "objectID": "blogs/DA-materials/model-types.html#assumptions",
    "href": "blogs/DA-materials/model-types.html#assumptions",
    "title": "Model Types and Diagnostics",
    "section": "",
    "text": "For the linear regression model \\mathbf{Y} = \\mathbf{X}\\beta + e, we have the following assumptions:\n\n\nErrors have mean 0: \\mathbb{E}[Y \\mid X] = 0\n\nThe error variance is constant: \\text{Var}[e \\mid \\mathbf{X}] = \\sigma^2\n\nThe errors are uncorrelated (the data points are iid): \\text{Var}[e \\mid \\mathbf{X}] = \\sigma^2\n\nThe errors are normally distributed"
  },
  {
    "objectID": "blogs/DA-materials/model-types.html#diagnostics-and-solutions",
    "href": "blogs/DA-materials/model-types.html#diagnostics-and-solutions",
    "title": "Model Types and Diagnostics",
    "section": "",
    "text": "Linearity: partial residual plots help us diagnose whether the true relationship between the outcome and some of the predictors is non-linear\n\n\nShow the code\nfit &lt;- lm(price2007 ~ distance + resid_walkscore + squarefeet + bedgroup + \n            zip, data = rail_trail_modeling)\n\nfit_new &lt;- lm(price2007 ~ distance + ns(resid_walkscore, knots = c(0.5)) + \n                squarefeet + bedgroup + zip, data = rail_trail_modeling)\n\np1 &lt;- partial_residuals(fit) |&gt;\n  mutate(.predictor_name = case_when(\n    .predictor_name == \"distance\" ~ \"Distance to Rail Trail\",\n    .predictor_name == \"walkscore\" ~ \"Walking Score\",\n    .predictor_name == \"bikescore\" ~ \"Biking Score\",\n    .predictor_name == \"squarefeet\" ~ \"Square Footage\",\n    .predictor_name == \"resid_walkscore\" ~ \"Residual Walk Score\",\n    TRUE ~ .predictor_name)) |&gt;\n  ggplot(aes(x = .predictor_value, y = .partial_resid)) +\n  geom_point(color = \"grey40\", size = 0.5) +\n  geom_smooth(color = \"blue\", fill = 'skyblue2',\n              linewidth = 0.7, linetype = \"twodash\") +\n  geom_line(aes(y = .predictor_effect), color = 'red') +\n  facet_wrap(vars(.predictor_name), scales = \"free\", ncol = 2) +\n  labs(x = \"Predictor value\", y = \"Partial residual\")+\n  ggtitle(\"(a) Partial Residual Plots,\\nPreliminary Linear Model\")+\n  theme_bw()+\n  theme(strip.background = element_rect(fill = \"white\", color = \"white\"),\n        strip.text = element_text(color = \"black\", face = \"bold\"),\n        axis.text = element_text(color = \"black\"),\n        plot.title = element_text(face = \"bold\", size = 11))\n\np2 &lt;- partial_residuals(fit_new) |&gt;\n  mutate(.predictor_name = case_when(\n    .predictor_name == \"distance\" ~ \"Distance to Rail Trail\",\n    .predictor_name == \"walkscore\" ~ \"Walking Score\",\n    .predictor_name == \"bikescore\" ~ \"Biking Score\",\n    .predictor_name == \"squarefeet\" ~ \"Square Footage\",\n    .predictor_name == \"resid_walkscore\" ~ \"Residual Walk Score\",\n    TRUE ~ .predictor_name)) |&gt;\n  ggplot(aes(x = .predictor_value, y = .partial_resid)) +\n  geom_point(color = \"grey40\", size = 0.5) +\n  geom_smooth(color = \"blue\", fill = 'skyblue2',\n              linewidth = 0.7, linetype = \"twodash\") +\n  geom_line(aes(y = .predictor_effect), color = 'red') +\n  facet_wrap(vars(.predictor_name), scales = \"free\", ncol = 2) +\n  labs(x = \"Predictor value\", y = \"Partial residual\")+\n  ggtitle(\"(b) Partial Residual Plots, Adding\\nNatural Splines (Knot at Median)\\nto Residual Walk Score\")+\n  theme_bw()+\n  theme(strip.background = element_rect(fill = \"white\", color = \"white\"),\n        strip.text = element_text(color = \"black\", face = \"bold\"),\n        axis.text = element_text(color = \"black\"),\n        plot.title = element_text(face = \"bold\", size = 11))\n\np1 + p2\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrom Figure 1(a), we see that residual walk score has a nonlinear relationship with estimated house price in 2007 – as indicated by the smoothed residual curve (in blue) diverging from the predicted effect line (in red). We would address this nonlinearity by fitting natural splines on the residual walk score, with one knot at the median. Figure 1(b) provides the updated partial residual plots when we add natural splines to the residual walk score. We see now that the predicted effect line (in red) tracks closely to the smoothed residual curve (in blue) without over-fitting to model to the turns in the smoothed residual curve.\nLineups: If we aren’t sure whether the true relationship between the outcome and a given predictor is non-linear, we can conduct a partial residual visual lineup.\nFor instance, it is not entirely clear whether distance has a non-linear relationship to estimated house price in 2007 or if this is just due to the high-leverage home with the largest distance to the rail trail. Figure 2 shows the partial residual line-up. If we cannot pick out the partial residual plot corresponding to the real data from the simulated data, we can conclude that there is insufficient evidence of a non-linear relationship between distance to rail trail and estimated house price in 2007 at a significance level of 0.05.\n\n\nShow the code\nset.seed(123)\nmodel_lineup(fit_new, fn = partial_residuals, nsim = 20) |&gt;\n  filter(.predictor_name == \"distance\") |&gt;\n  mutate(.sample_new = paste0(\"Position \", .sample),\n         .sample_new = fct_reorder(.sample_new, .sample)) |&gt;\n  ggplot(aes(x = .predictor_value, y = .partial_resid))+\n  geom_point(color = \"grey40\", size = 0.5) +\n  geom_line(aes(y = .predictor_effect), color = 'red') +\n  geom_smooth(color = \"blue\", fill = 'skyblue2',\n              linewidth = 0.7, linetype = \"twodash\") +\n  facet_wrap(~.sample_new)+\n  labs(x = \"Distance to Rail Trail (in Miles)\", y = \"Partial residual\")+\n  theme_bw()+\n  theme(strip.background = element_rect(fill = \"white\", color = \"white\"),\n        strip.text = element_text(color = \"black\", face = \"bold\"),\n        axis.text = element_text(color = \"black\"),\n        plot.title = element_text(face = \"bold\", size = 11))\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can decrypt (show the the position of our actual data) by running the command, which is given at the end of the line-up call (dycrypt()):\n\ndecrypt(\"QUg2 qFyF Rx 8tLRyRtx ZP\")\n\n[1] \"True data in position  16\"\n\n\n\n\n\nAdd polynomial terms to the model\n\nSimplest way to add polynomial terms is with I():\n\nfit_new &lt;- lm(price2007 ~ distance + resid_walkscore + I(resid_walkscore^2) +\n                squarefeet + bedgroup + zip, data = rail_trail_modeling)\n\nIf there is numerical issues (e.g., from the predictor having large values), we can use the poly()function which constructs the design matric using an orthogonal polynomial basis.\nA downside is that the coefficients are no longer is terms of the interpretable regressor terms. The fitted values though wil be the same between the two approaches for adding polynomial terms.\n\nfit_new &lt;- lm(price2007 ~ distance + poly(resid_walkscore, degree = 2) + squarefeet + \n                bedgroup + zip, data = rail_trail_modeling)\n\n\nAdd regression splines to the model\n\nRegression splines model relationships as being piecewise polynomial. They require us to choose knots, which are the fixed points between which the function is polynomial. Two common approaches are natural splines and B-spline basis. Statisticians almost universally use cubic splines. Cubic splines are continuous in their first and second derivatives, making the knots almost visually imperceptible in plots of the spline.\nFor knot selection, a typical approach is to use quantiles of the data as knots – erring on the side of less knots (especially if there is few observations).\n\nfit_new &lt;- lm(price2007 ~ distance + ns(resid_walkscore, knots = c(0.5)) + \n                squarefeet + bedgroup + zip, data = rail_trail_modeling)\n\n\n\n\n\nErrors with non-constant variance are called heteroskedastic. To detect heteroskedasticity, we can examine the residuals plotted against the fitted values.\n\n\nShow the code\nfit_new |&gt;\n  augment() |&gt;\n  ggplot(aes(x = .fitted, y = .resid))+\n  geom_point(size = 0.5)+\n  geom_hline(yintercept = 0)+\n  geom_smooth(fill = 'skyblue2')+\n  labs(x = \"Fitted Values\", y = \"Residuals\")+\n  theme_bw()+\n  theme(axis.text = element_text(color = \"black\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn Figure 3, we have non-constant error variance. Indeed, there appears to be an approximately quadratic relationship between our fitted values and our residual terms.\nWe can remedy non-constant error variance by using the sandwich estimator for the variance of our estimators in all inference.\n\n# uses sandwich estimator with HC3 estimator for Omega\nConfint(fit_new, vcov = vcovHC(fit_new))\n\nStandard errors computed by vcovHC(fit_new) \n\n\n                                       Estimate      2.5 %     97.5 %\n(Intercept)                           48.607106 -15.216646 112.430857\ndistance                             -14.779103 -23.768308  -5.789898\nns(resid_walkscore, knots = c(0.5))1  93.909271 -17.234103 205.052646\nns(resid_walkscore, knots = c(0.5))2 -18.347598 -85.985661  49.290465\nsquarefeet                           145.196077 122.379951 168.012202\nbedgroup3 beds                         7.595663  -9.282322  24.473649\nbedgroup4+ beds                      -28.637850 -55.631857  -1.643844\nzip1062                              -30.040888 -50.154050  -9.927726\n\n\n\n\n\nWe can check for non-normality of errors using residual Q-Q plots. Using the standardized residuals, we can detect gross deviations from normality while ignoring small deviations in non-normality. An example of a Q-Q plot for linear regression is in Figure 4. Since we are doing linear regression, the theoretical quantiles correspond to a normal distribution with mean 0 and variance 1.\n\n\nShow the code\naugment(fit_new) |&gt;\n  ggplot(aes(sample = .std.resid)) +\n  geom_qq_line(linewidth = 0.4) +\n  geom_qq(size = 0.75)+\n  labs(x = 'Theoretical Quantiles',\n       y = 'Observed Quantiles')+\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\n\n\n\nLike with partial residuals, we can use a lineup to detect whether the non-normality in errors is statistically significant (at a significance level of 0.05).\n\n\nShow the code\nset.seed(123)\nmodel_lineup(fit_new) |&gt; \n  mutate(.sample_new = paste0(\"Position \", .sample),\n         .sample_new = fct_reorder(.sample_new, .sample)) |&gt; \n  ggplot(aes(sample = .std.resid)) +\n  geom_qq_line(linewidth = 0.4) +\n  geom_qq(size = 0.5) +\n  facet_wrap(vars(.sample_new)) +\n  labs(x = \"Theoretical Quantiles\",\n       y = \"Observed Quantiles\")+\n  theme_bw()+\n  theme(strip.text = element_text(face = \"bold\"),\n        strip.background = element_rect(fill = \"white\", color = \"white\"),\n        text = element_text(color = \"black\"),\n        axis.text = element_text(color = \"black\"))\n\n\n\n\n\n\n\n\n\n\n\n[1] \"True data in position  16\"\n\n\nNote: if the sample size is sufficiently large, then violating non-normality of errors is not a serious concern. Otherwise, if the assumptions linearity and constant error variance hold, then interpret coefficient estimates with caution (since coefficient estimates would not be approximately normal)."
  },
  {
    "objectID": "blogs/DA-materials/model-types.html#contaminated-errors-and-outliers",
    "href": "blogs/DA-materials/model-types.html#contaminated-errors-and-outliers",
    "title": "Model Types and Diagnostics",
    "section": "",
    "text": "Outliers: Not all outliers have a major effect on the regression line. So, their presence may not be a problem. We hence need a way to detect outliers and characterize their influence on the regression line.\n\nIf we can determine the outlier is due to some kind of measurement or recording error, we can correct the error.\nIf we cannot, we must make the difficult decision of whether to keep the outlier, and acknowledge that it may significantly influence our estimates, or to remove it and risk throwing away good information\n\nOne way to measure the influence of an observation on the regression is to quantify how its inclusion changes our coefficient estimates via Cook’s Distance.\n\nLook for Cook’s Distance (D_i), where D_i \\geq 1, though this is again a matter of judgment, to indicate that a particular observation substantially changes the regression fit.\n\nFigure 5 shows that all observations have Cook’s Distances well below the standard cut-off of 1, so there does not appear that any of the homes in the data have a large influence on our regression model.\n\n\nShow the code\nmax_price &lt;- augment(fit_new) |&gt;\n  arrange(desc(price2007)) |&gt;\n  slice_head(n = 1)\n\naugment(fit_new) |&gt;\n  ggplot(aes(y = .cooksd, x = price2007))+\n  geom_point(size = 0.7, alpha = 0.7)+\n  geom_point(data = max_price, aes(y = .cooksd, x = price2007),\n             size = 2, color = \"red\")+\n  labs(y = \"Cook's Distance\", x = \"Estimated House Price (2007)\")+\n  #geom_hline(yintercept = 1, linetype = \"dashed\")+\n  theme_bw()+\n  theme(plot.title = element_blank(),\n        text = element_text(color = \"black\"),\n        axis.text = element_text(color = \"black\"),\n        axis.title = element_text(color = \"black\"))"
  },
  {
    "objectID": "blogs/DA-materials/model-types.html#collinearity",
    "href": "blogs/DA-materials/model-types.html#collinearity",
    "title": "Model Types and Diagnostics",
    "section": "",
    "text": "When the model matrix X is not full column rank, its columns span a lower-dimensional space. There is still a unique perpendicular projection of Y onto this space, and hence a unique \\hat Y. But the coordinates of that fit are not unique, meaning that there are infinite values of \\hat \\beta that correspond to the same squared error/prediction. If we care about inference on \\hat \\beta this is a serious problem.\n\n\nShow the code\nloess_wrapper &lt;- function(data, mapping, ...){\n      p &lt;- ggplot(data = data, mapping = mapping) + \n      geom_point(size = 0.35, color = \"skyblue2\") + \n      geom_smooth(linetype = \"dashed\", color = \"black\", linewidth = 0.6)\n      return(p)\n}\n\nrail_trail_modeling |&gt;\n  select(price2007, distance, walkscore, squarefeet) |&gt;\n  rename(`2007 Est.\\nHouse Price\\n(thousands of $)` = price2007,\n         `Distance to\\nRail Trail (miles)` = distance,\n         `Walk\\nScore` = walkscore,\n         `Square\\nFootage` = squarefeet) |&gt;\n  ggpairs(upper = list(continuous = wrap(\"cor\", color = \"black\")),\n          lower = list(continuous = loess_wrapper,\n                       combo = wrap(\"barDiag\", size=0.2,\n                                     alpha = 0.9))) +\n  scale_x_continuous(breaks = scales::pretty_breaks(n = 4))+\n  scale_y_continuous(breaks = scales::pretty_breaks(n = 4))+\n  theme_bw()+\n  theme(strip.background = element_rect(fill = \"white\", color = \"white\"),\n        strip.text = element_text(color = \"black\", face = \"bold\", size = 6.5),\n        axis.text = element_text(color = \"black\", size = 6.5))\n\n\n\n\n\n\n\n\n\n\n\nCondition Number:\nHowever, multiple variables can be collinear even when individual pairs of variables have low correlation. One way to detect non-pairwise collinearity is to use the condition number, where we compare the largest and smallest eigenvalues of X^\\intercal X:\n\nX &lt;- model.matrix(price2007 ~ distance + resid_walkscore + \n                squarefeet + bedgroup + zip - 1, data = rail_trail_modeling)\n# first step is to scale the model matrix X\nscaled_x &lt;- scale(X, center = FALSE, scale = TRUE)\n  \n# Then, we can use the kappa() function on X'X\nkappa(t(scaled_x) %*% scaled_x)\n\n[1] 123.3692\n\n\nIdeally, we want the condition number to be small. The general cut-off is condition numbers larger than 50 or 100. So, from the results above, we see that there is potentially concerning levels of collinearity among our predictors, which could influence the accuracy of our inference.\nVariance Inflation Factors:\nAnother way to look at collinearity is with variance inflation factors (VIF), which can be interpreted as showing how much the variance of \\hat \\beta_j is inflated relative to a model where there is no collinearity (and all columns are orthogonal).\n\n# consider using type = 'predictor' for interactions (or ignoring VIFs since compared to main effecs)\ncar::vif(lm(price2007 ~ distance + ns(resid_walkscore, knots = c(0.5)) + \n                squarefeet + bedgroup + zip, data = rail_trail_modeling))\n\n                                        GVIF Df GVIF^(1/(2*Df))\ndistance                            1.379898  1        1.174691\nns(resid_walkscore, knots = c(0.5)) 1.409755  2        1.089647\nsquarefeet                          1.951119  1        1.396825\nbedgroup                            1.725889  2        1.146181\nzip                                 1.342293  1        1.158574\n\n\nA general rule of thumb is that VIF values exceeding 5 warrant further investigation, and VIFs exceeding 10 are signs of serious multicollinearity requiring correction.\nGeneral Notes: If we are confronted with collinearity, then, the question to ask is:\n\n“Have we chosen the correct predictors for the research question?”\n\nIf we have, there is little to be done; if we have not, we can reconsider our choice of predictors and perhaps eliminate the collinear ones.\nIf we are interested in prediction rather than in the coefficients, collinearity is a problem insofar as it creates high prediction variance, and we might reconsider our model and use a penalization model to reduce the prediction variance.s"
  },
  {
    "objectID": "blogs/DA-materials/model-types.html#interactions",
    "href": "blogs/DA-materials/model-types.html#interactions",
    "title": "Model Types and Diagnostics",
    "section": "",
    "text": "An interaction allows one predictor’s association with the outcome to depend on values of another predictor.\nWhen an interaction is present, the normal interpretation of coefficients as slopes no longer holds for the predictors involved in the interaction.\nSee Example 7.4"
  },
  {
    "objectID": "blogs/DA-materials/model-types.html#hypothesis-tests",
    "href": "blogs/DA-materials/model-types.html#hypothesis-tests",
    "title": "Model Types and Diagnostics",
    "section": "",
    "text": "Many useful null hypotheses can be written in terms of linear combinations of the coefficients. For example, consider a linear model with a continuous predictor X_1 and dummy-coded regressor X_2 \\in \\{0,1 \\}:\nY = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_1 X_2 We could think of several null hypotheses to test:\n\n\\beta_2 = 0: the two factor levels have identical intercepts\n\\beta_3 = 0: the two factor levels have identical slopes\n\\beta_1 = c: the slope when X_2 = 0 is some value c predicted by a theory we are testing (usually c = 0 in linear regression, which corresponds to no association)\n\\beta_1 + \\beta_3 = 0: when X_2 = 1, there is no association between X_1 and Y\n\\beta_2 = \\beta_3 = 0: the two factor levels have identical relationships between X_1 and Y\n\nFor all but the last listed hypothesis, we can use a t test. R conducts a t test for every coefficient by default, with c = 0.\nThe degrees of freedom for a t test is n - p, where p is the number of parameters in our model. We can get this via fit$df.residual. E.g., for our rail trail linear model example we have df = 96.\nThe function tidy() does the test for c = 0 with every other predictor held constant in the model.\n\n\nShow the code\ngt(tidy(fit_new)) |&gt;\n  fmt_number(decimals = 3)\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n48.607\n27.295\n1.781\n0.078\n\n\ndistance\n−14.779\n5.039\n−2.933\n0.004\n\n\nns(resid_walkscore, knots = c(0.5))1\n93.909\n43.190\n2.174\n0.032\n\n\nns(resid_walkscore, knots = c(0.5))2\n−18.348\n22.890\n−0.802\n0.425\n\n\nsquarefeet\n145.196\n10.082\n14.401\n0.000\n\n\nbedgroup3 beds\n7.596\n12.269\n0.619\n0.537\n\n\nbedgroup4+ beds\n−28.638\n15.037\n−1.905\n0.060\n\n\nzip1062\n−30.041\n9.438\n−3.183\n0.002\n\n\n\n\n\n\n\nWe can also use tbl_regression() which gives us the confidence intervals and p-values:\n\n\nShow the code\ntbl_regression(fit_new)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nBeta\n95% CI\np-value\n\n\n\n\ndistance\n-15\n-25, -4.8\n0.004\n\n\nns(resid_walkscore, knots = c(0.5))\n\n\n\n\n\n\n\n\n    ns(resid_walkscore, knots = c(0.5))1\n94\n8.2, 180\n0.032\n\n\n    ns(resid_walkscore, knots = c(0.5))2\n-18\n-64, 27\n0.4\n\n\nsquarefeet\n145\n125, 165\n&lt;0.001\n\n\nbedgroup\n\n\n\n\n\n\n\n\n    1-2 beds\n—\n—\n\n\n\n\n    3 beds\n7.6\n-17, 32\n0.5\n\n\n    4+ beds\n-29\n-58, 1.2\n0.060\n\n\nzip\n\n\n\n\n\n\n\n\n    1060\n—\n—\n\n\n\n\n    1062\n-30\n-49, -11\n0.002\n\n\n\nAbbreviation: CI = Confidence Interval\n\n\n\n\n\n\n\n\nIf we have non-constant error variance and thus use the sandwich estimator, we can use the following code to conduct t tests and present our results:\n\np.values &lt;- coeftest(fit_new, vcov = vcovHC(fit_new)) |&gt;\n  tidy() |&gt; _$p.value\n\nt.stats &lt;- coeftest(fit_new, vcov = vcovHC(fit_new)) |&gt;\n  tidy() |&gt; _$statistic\n\ndata_table &lt;- as.data.frame(Confint(fit_new, vcov = vcovHC(fit_new))) |&gt;\n  cbind(t.stats) |&gt;\n  cbind(p.values) |&gt;\n  rownames_to_column(var = \"Characteristic\") |&gt;\n  rename(`p-value` = p.values,\n         `t-stat` = t.stats)\n\nStandard errors computed by vcovHC(fit_new) \n\ngt(data_table) |&gt;\n  fmt_number(decimals = 3)\n\n\n\n\n\n\n\nCharacteristic\nEstimate\n2.5 %\n97.5 %\nt-stat\np-value\n\n\n\n\n(Intercept)\n48.607\n−15.217\n112.431\n1.512\n0.134\n\n\ndistance\n−14.779\n−23.768\n−5.790\n−3.264\n0.002\n\n\nns(resid_walkscore, knots = c(0.5))1\n93.909\n−17.234\n205.053\n1.677\n0.097\n\n\nns(resid_walkscore, knots = c(0.5))2\n−18.348\n−85.986\n49.290\n−0.538\n0.592\n\n\nsquarefeet\n145.196\n122.380\n168.012\n12.632\n0.000\n\n\nbedgroup3 beds\n7.596\n−9.282\n24.474\n0.893\n0.374\n\n\nbedgroup4+ beds\n−28.638\n−55.632\n−1.644\n−2.106\n0.038\n\n\nzip1062\n−30.041\n−50.154\n−9.928\n−2.965\n0.004\n\n\n\n\n\n\n\nOf course, a fourth option is to manually perform the t test, which might make sense if we have a complex null or would like to do a one-sided test. Below is the general code for this:\n\nbeta_hats &lt;- coef(gentoo_fit)\na &lt;- c(0, 0, 0, 1) # beta_3 = 0\n\nt_stat &lt;- sum(a * beta_hats) /\n  sqrt(a %*% vcov(gentoo_fit) %*% a)[1,1]\n\np_value &lt;- pt(t_stat, gentoo_fit$df.residual,\n              lower.tail = FALSE)\n\np_value * 2 # if two-sided\n\n\n\n\nThe anova() function can be used to conduct F tests for nested models. For instance, we can test whether there was a significant association between residual walk score and estimated home price in 2007 (in practice, probably not a good idea since residual walk score is random):\n\n\nShow the code\n# create a model without the predictor\nfit_no_resid_ws &lt;- lm(price2007 ~ distance + \n                squarefeet + bedgroup + zip, data = rail_trail_modeling)\n\nanova(fit_no_resid_ws, fit_new)\n\n\nAnalysis of Variance Table\n\nModel 1: price2007 ~ distance + squarefeet + bedgroup + zip\nModel 2: price2007 ~ distance + ns(resid_walkscore, knots = c(0.5)) + \n    squarefeet + bedgroup + zip\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)  \n1     98 170673                             \n2     96 160689  2    9983.3 2.9821 0.0554 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe parameters of an F-statistic are q and n - p. Df is the outputted table corresponds to q and the second row of Res.Df corresponds to n - p. E.g, we have F(2, 96) as our null distribution in this example."
  },
  {
    "objectID": "blogs/DA-materials/model-types.html#coefficient-interpretation",
    "href": "blogs/DA-materials/model-types.html#coefficient-interpretation",
    "title": "Model Types and Diagnostics",
    "section": "",
    "text": "In a report, we just include full interpretations for the hypothesis tests and/or coefficient estimates that relevant to the practical problem at hand!\n\n\nShow the code\nlibrary(palmerpenguins)\n\npenguin_fit &lt;- lm(\n  bill_length_mm ~ flipper_length_mm + species +\n    flipper_length_mm:species,\n  data = penguins\n)\ntbl_regression(penguin_fit)\n\n\n\n\nTable 1: ADD\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nBeta\n95% CI\np-value\n\n\n\n\nflipper_length_mm\n0.13\n0.07, 0.20\n&lt;0.001\n\n\nspecies\n\n\n\n\n\n\n\n\n    Adelie\n—\n—\n\n\n\n\n    Chinstrap\n-8.0\n-29, 13\n0.4\n\n\n    Gentoo\n-34\n-54, -15\n&lt;0.001\n\n\nflipper_length_mm * species\n\n\n\n\n\n\n\n\n    flipper_length_mm * Chinstrap\n0.09\n-0.02, 0.19\n0.10\n\n\n    flipper_length_mm * Gentoo\n0.18\n0.09, 0.28\n&lt;0.001\n\n\n\nAbbreviation: CI = Confidence Interval\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe intercept \\beta_0 is the mean value of when all other regressors are 0:\n\\mathbb{E}[Y \\mid X_1 = 0, \\dots, X_q = 0] Interpreting the intercept, quoting confidence intervals, and conducting hypothesis tests for its value may only be useful when the intercept has a substantive meaning and we have observed X values nearby.\n\n\n\nNote these are the general parts of an interpretation for a continuous predictor in a linear regression:\n\nDifference in the mean value of Y\nAssociated with a one-unit increase in X_1\nHolding all other regressors constant\n\nExact interpretation depends on if the predictor is continuous or a factor level:\n\nTest for a factor coefficient: For a given flipper length, gentoo penguins have a smaller mean bill length (M = 47.5 mm, SD = 3.1) than Adelie penguins (M = 38.8 mm, SD = 2.7), t(336) = -3.5, p = 0.001.\n\nIn part calculated using group_by():\n\n\nShow the code\npenguins |&gt;\n  group_by(species)|&gt;\n  summarize(mean_bill_length =\n              mean(bill_length_mm,\n                 na.rm = TRUE)) |&gt;\n  gt() |&gt;\n  fmt_number(decimals = 1)\n\n\n\n\n\n\n\n\nspecies\nmean_bill_length\n\n\n\n\nAdelie\n38.8\n\n\nChinstrap\n48.8\n\n\nGentoo\n47.5\n\n\n\n\n\n\n\n\nConfidence interval for a factor coefficient: For a given flipper length, gentoo penguin bills are shorter than Adelie penguin bills by an average of 34.3 mm (95% CI [15.01, 15.64]).\nTest for slope: In Adelie penguins, there was a statistically significant association between flipper length and bill length, \\hat \\beta = 0.13, t(336) = 4.17, p &lt; 0.001.\nDescribe a slope: Among Adelie penguins, each additional millimeter of flipper length is associated with 0.13 mm of additional bill length, on average (95% CI [0.07, 0.2]).\n\n\n\n\nDescribing an interaction coefficient: For the interaction term between flipper length and species from Table 1: For Adelie penguins (the baseline level), the association between bill length and flipper length is 0.13 mm of bill length per millimeter of flipper length, on average (95% CI [0.07, 0.20]). But for Chinstrap penguins, the association is 0.13 + 0.09 = 0.22 mm of bill length per millimeter of flipper length, on average (95% CI [0.11, 0.31]). [LOOK AT THEOREM 5.5 AND HW 3 TO CONFIRM]\nA helpful way to visualize interaction terms, polynomial terms, (or other complex relationships) is to use an effects plot:\n\npredict_response(penguin_fit,\n                 terms = c(\"flipper_length_mm\", \"species\")) |&gt;\n  plot() +\n  labs(x = \"Flipper length (mm)\", y = \"Bill length (mm)\",\n       color = \"Species\",\n       title = \"Flipper length effect plot\")\n\n\n\n\n\n\n\n\nIn an effects plot, non-focal predictors are set to their mean (numeric variables), reference level (factors), or “most common” value (mode) in case of character vectors.\n\n\n\nIf we fit a spline or a polynomial for our predictor-of-interest, we cannot determine the statistical significance or effect size from the outputted regression tables. Instead, we must use the following framework:\n\nDo a LRT/F-test for nested models to obtain the overall significance of the predictor-of-interest.\n\n\nanova(fit_new, fit_no_resid_ws)\n\nAnalysis of Variance Table\n\nModel 1: price2007 ~ distance + ns(resid_walkscore, knots = c(0.5)) + \n    squarefeet + bedgroup + zip\nModel 2: price2007 ~ distance + squarefeet + bedgroup + zip\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)  \n1     96 160689                             \n2     98 170673 -2   -9983.3 2.9821 0.0554 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nCreate an effects plot fort the variable of interest\n\n\npredict_response(fit_new,\n                 terms = c(\"resid_walkscore\")) |&gt;\n  plot()\n\n\n\n\n\n\n\n\n\nGive an example of an interpretation using the effects plot with specific values\n\n\npreds &lt;- predict_response(fit_new, terms = c(\"resid_walkscore\"))\nhead(preds)\n\n# Predicted values of price2007\n\nresid_walkscore | Predicted |         95% CI\n--------------------------------------------\n         -43.01 |    259.57 | 213.44, 305.71\n         -36.08 |    271.78 | 233.60, 309.96\n         -31.03 |    280.37 | 247.16, 313.58\n         -29.04 |    283.65 | 252.14, 315.16\n         -25.22 |    289.68 | 260.94, 318.43\n         -25.21 |    289.70 | 260.96, 318.44\n\nAdjusted for:\n*   distance =     1.11\n* squarefeet =     1.57\n*   bedgroup = 1-2 beds\n*        zip =     1060\n\n\nFor example, when the residual walk score is -43.01, the predicted house price (in 2007) is $259,570 in Northampton, MA (95% CI [\\$213,440, \\$305,710]) versus when the residual walk score is -36.08, the predicted house price (in 2007) is $271,780 in Northampton, MA (95% CI [\\$233,600, \\$309,960]) – holding all other covariates at their mean or reference level."
  },
  {
    "objectID": "blogs/DA-materials/model-types.html#assumptions-1",
    "href": "blogs/DA-materials/model-types.html#assumptions-1",
    "title": "Model Types and Diagnostics",
    "section": "2.1 Assumptions",
    "text": "2.1 Assumptions\nLogistic regression makes two basic assumptions about the population relationship:\n\nThe log-odds is linearly related to the regressors: \\log(\\text{odds}(Y = 1 \\mid X = x)) = \\beta^\\intercal x\nThe observation Y_i are conditionally independent given the covariates X_i"
  },
  {
    "objectID": "blogs/DA-materials/model-types.html#linearity-diagnostics",
    "href": "blogs/DA-materials/model-types.html#linearity-diagnostics",
    "title": "Model Types and Diagnostics",
    "section": "2.2 Linearity Diagnostics",
    "text": "2.2 Linearity Diagnostics\nEmpirical Link Plot (EDA):\nExample: Figure 6 shows that TWA core temperature during surgery appears to have an approximately linear relationship with the log-odds of superficial post-op infection but possibly a non-linear relationship with log-odds of a serious post-op infection.\n\n\nShow the code\ncore_temperature &lt;- read_csv(\"data/core-temperature.csv\") |&gt;\n  select(-SurgeryType) |&gt;\n  filter(DEAD == 0 | DurationHosp &gt;= 30)\n\np1 &lt;- core_temperature |&gt;\n  bin_by_quantile(TWATemp, breaks = 10) |&gt;\n  summarize(\n    mean_temp = mean(TWATemp),\n    prob = mean(SeriousInfection),\n    log_odds = empirical_link(\n      SeriousInfection,\n      family = binomial(link = \"logit\")),\n    `Patient Count` = n()\n  ) |&gt;\n  ggplot(aes(x = mean_temp, y = log_odds)) +\n  geom_point(aes(size = `Patient Count`))+\n  geom_smooth(fill = \"skyblue3\")+\n  scale_size_continuous(range = c(1,3))+\n  labs(title = \"(A) Serious Infection\",\n       y = \"Log-odds of Infection\",\n       x = \"TWA Core Temperature during\\nSurgery (in Celsius)\")+\n  theme_minimal()+\n  theme(legend.position = \"None\",\n        plot.title = element_text(face = \"bold\", size = 11, hjust = 0),\n        axis.text = element_text(color = \"black\"))\n\np2 &lt;- core_temperature |&gt;\n  bin_by_quantile(TWATemp, breaks = 10) |&gt;\n  summarize(\n    mean_temp = mean(TWATemp),\n    prob = mean(SuperficialInfection),\n    log_odds = empirical_link(\n      SuperficialInfection,\n      family = binomial(link = \"logit\")),\n    `Patient Count` = n()\n  ) |&gt;\n  ggplot(aes(x = mean_temp, y = log_odds)) +\n  geom_point(aes(size = `Patient Count`))+\n  geom_smooth(fill = \"skyblue3\")+\n  scale_size_continuous(range = c(1,3))+\n  labs(title = \"(B) Superficial Infection\",\n       y = \"Log-odds of Infection\",\n      x = \"TWA Core Temperature during\\nSurgery (in Celsius)\")+\n  theme_minimal()+\n  theme(plot.title = element_text(face = \"bold\", size = 11, hjust = 0),\n        axis.text = element_text(color = \"black\"))\n\np1 + p2\n\n\n\n\n\n\n\n\n\n\n\n\n\nPartial Residuals:\nAnother method is to use partial residuals, just like we did for linear regression.\nExample: Figure 7 shows that surgery duration and TWA core temperature both appear to have non-linear relationships with the log-odds of serious infection, which confirms what we see in the empirical link plot.\n\n\nShow the code\ncore_temperature_model &lt;- core_temperature |&gt;\n  filter(DEAD == 0 | DurationHosp &gt;= 30) |&gt;\n  mutate(WeightLoss = ifelse(WGHTLOSS == 1, \"Yes\", \"No\"),\n         SteroidUsage = ifelse(SteroidHx == 1, \"Yes\", \"No\"),\n         SurgeryDuration = SurgDuration)\n\nprelim_fit &lt;- glm(SeriousInfection ~ TWATemp + WeightLoss +\n                    SurgeryDuration + SteroidUsage,\n    data = core_temperature_model,\n    family = binomial()) \n\npartial_residuals(prelim_fit) |&gt;\n    mutate(.predictor_name = case_when(\n    .predictor_name == \"Age\" ~ \"Age (in Years)\",\n    .predictor_name == \"BMI\" ~ \"Body Mass Index (kg/m^2)\",\n    .predictor_name == \"SurgeryDuration\" ~ \"Surgery Duration (in Minutes)\",\n    .predictor_name == \"TWATemp\" ~ \"TWA Core Temperature (in Celsius)\",\n    TRUE ~ .predictor_name\n  )) |&gt;\n  ggplot((aes(x = .predictor_value, y = .partial_resid)))+\n  geom_point(size = 0.3, color = 'grey50')+\n  geom_line(aes(y = .predictor_effect), linewidth = 0.3)+\n  geom_smooth(method = \"loess\", linetype = \"twodash\", fill = \"skyblue3\")+\n  facet_wrap(~.predictor_name, scales = \"free\")+\n  labs(y = \"Partial Residual\",\n       x = \"Predictor Value\")+\n  theme_minimal()+\n  theme(strip.text = element_text(face = \"bold\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe address the non-linearity in Figure 7 by fitting natural splines to both surgical duration and TWA core temperature.\n\n\nShow the code\nupdated_fit &lt;- glm(SeriousInfection ~ splines::ns(TWATemp, knots = c(35.7, 36, 36.4)) + WeightLoss + ns(SurgeryDuration, knots = c(550)) + SteroidUsage,\n    data = core_temperature_model,\n    family = binomial()) \n\npartial_residuals(updated_fit) |&gt;\n  mutate(.predictor_name = case_when(\n    .predictor_name == \"Age\" ~ \"Age (in Years)\",\n    .predictor_name == \"BMI\" ~ \"Body Mass Index (kg/m^2)\",\n    .predictor_name == \"SurgeryDuration\" ~ \"Surgery Duration (in Minutes)\",\n    .predictor_name == \"TWATemp\" ~ \"TWA Core Temperature (in Celsius)\",\n    TRUE ~ .predictor_name\n  )) |&gt;\n  ggplot((aes(x = .predictor_value, y = .partial_resid)))+\n  geom_point(size = 0.3, color = 'grey50')+\n  geom_line(aes(y = .predictor_effect), linewidth = 0.3)+\n  geom_smooth(method = \"loess\", linetype = \"twodash\", fill = \"skyblue3\")+\n  facet_wrap(~.predictor_name, scales = \"free\")+\n  labs(y = \"Partial Residual\",\n       x = \"Predictor Value\")+\n  theme_minimal()+\n  theme(strip.text = element_text(face = \"bold\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\nRandomized Quantile Residuals:\nA third method is to use randomized quantile residuals. We can use plots of randomized quantile residuals against predictors and/or fitted values, as with any other residuals, to check the overall fit of our model.\n\nIf the model is correctly specified, we would want the conditional mean of the randomized quantile residuals to be approximately 0.5.\n\nIt is also useful to check that their distribution is indeed uniform (e.g. with a Q-Q plot). When the model is incorrectly specified, the distribution will not be uniform, producing patterns on the residual plots that can be interpreted.\nExample: Below is an example with Pima women data from the MASS package.\n\n\nShow the code\nlibrary(MASS)\nPima.tr$pregnancy &lt;- factor(\n  ifelse(Pima.tr$npreg &gt; 0, \"Yes\", \"No\")\n)\n\npima_fit &lt;- glm(type ~ pregnancy + bp, data = Pima.tr,\n                family = binomial())\n\npima_aug &lt;- augment_quantile(pima_fit)\n\np1 &lt;- pima_aug |&gt;\n  ggplot(aes(x = .fitted, y = .quantile.resid))+\n  geom_point()+\n  geom_hline(yintercept = 0.5)+\n  geom_smooth(fill = 'skyblue2')+\n  labs(x = 'Fitted Value', y = 'Randomized Quantile Residual')+\n  theme_bw()\n\np2 &lt;- pima_aug |&gt;\n  ggplot(aes(sample = .quantile.resid))+\n  geom_qq_line(linewidth = 0.4, distribution = stats::qunif) +\n  geom_qq(size = 0.75, distribution = stats::qunif)+\n  labs(x = 'Theoretical Quantiles',\n       y = 'Observed Quantiles')+\n  theme_bw()\n\np1 + p2"
  },
  {
    "objectID": "blogs/DA-materials/model-types.html#calibration-plots",
    "href": "blogs/DA-materials/model-types.html#calibration-plots",
    "title": "Model Types and Diagnostics",
    "section": "2.3 Calibration Plots",
    "text": "2.3 Calibration Plots\nRoughly speaking, a calibrated model is one whose predicted probabilities are accurate. For example, if the model predicts \\text{Pr}(Y = 1 \\mid X = x) = 0.8 for a particular x, and we observe many responses with that x, about 80\\% of those responses should be 1 and 20\\% should be 0.\n\n\nShow the code\ncalibration_data &lt;- data.frame(\n  x = predict(pima_fit, type = \"response\"),\n  y = ifelse(Pima.tr$type == \"Yes\", 1, 0)\n)\n\nggplot(calibration_data, aes(x = x, y = y)) +\n  geom_point() +\n  geom_smooth(se = FALSE) +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\") +\n  labs(x = \"Predicted probability\", y = \"Observed fraction\") +\n  ylim(0, 1)+\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe start by converting the response from a factor to 0 or 1, so the smoother can take the average. Figure 9 shows that the calibration looks good across most of the range in our data, but as the predicted probability gets over 0.6, we see odd behavior; examining the plotted points, it appears this is due to a few observations with high predicted probability but Y = 0. We should not get too worried over two or three observations, so this is not particularly concerning.\nBut perfect calibration does not make a good model. Calibration hence should be used together with other measures of the adequacy of the model fit, not on its own!"
  },
  {
    "objectID": "blogs/DA-materials/model-types.html#hypothesis-tests-1",
    "href": "blogs/DA-materials/model-types.html#hypothesis-tests-1",
    "title": "Model Types and Diagnostics",
    "section": "2.4 Hypothesis Tests",
    "text": "2.4 Hypothesis Tests\nWald tests are what tidy() defaults to for GLMs. Wald tests use a z test statistic and, by default tidy() tests \\beta = 0.\n\n2.4.1 Wald Tests\nFirst, we can find a confidence interval for a probability for a specific set of regressors:\n\n\nShow the code\nilogit &lt;- function(x) 1 / (1 + exp(-x))\n\nx &lt;- data.frame(bp = 80,\n                pregnancy = \"Yes\")\n\npred_lp &lt;- predict(pima_fit, newdata = x,\n                   type = \"link\", se.fit = TRUE)\n\nilogit(pred_lp$fit + (\n  qnorm(p = c(0.025, 0.975)) *\n    pred_lp$se.fit\n))\n\n\n[1] 0.3119114 0.4923628\n\n\n\n\n2.4.2 Confidence Intervals\nWe can also derive the confidence intervals for the coefficients in our model using Wald tests just using confint()\n\n\nShow the code\nexp(confint(pima_fit)) # exponentiate puts it of the odds scale\n\n\n                   2.5 %    97.5 %\n(Intercept)  0.004724633 0.3286171\npregnancyYes 0.271584007 1.4697789\nbp           1.013722770 1.0710464\n\n\nExample interpretation: A 95% confidence interval for the odds ratio associated with having a pregnancy is [0.272, 1.47]. Since the confidence interval overlaps with 1, we cannot conclusively say whether prior pregnancy is associated with an increase or decrease in odds.\n\n\n2.4.3 Deviance Tests\nDeviance tests are used for nested models and thus equivalent to an F test in linear regression. Specifically:\n\\text{Dev}_{\\text{reduced}} - \\text{Dev}_{\\text{full}} \\overset{\\text{d}}{\\rightarrow} \\chi^2_q, \\quad \\text{where } q \\text{ is the diff in the degrees of freedom}\nExample code:\n\n\nShow the code\npima_larger_fit &lt;- glm(type ~ pregnancy + bp + age + glu,\n                       data = Pima.tr, family = binomial())\n\nanova(pima_fit, pima_larger_fit, test = \"Chisq\") #chisq is same as LRT\n\n\nAnalysis of Deviance Table\n\nModel 1: type ~ pregnancy + bp\nModel 2: type ~ pregnancy + bp + age + glu\n  Resid. Df Resid. Dev Df Deviance  Pr(&gt;Chi)    \n1       197     246.37                          \n2       195     194.34  2   52.031 5.032e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nHere, q = 2. Since the p-value is significant we reject the null hypothesis that both age and glucose status have coefficients equal to one (meaning at least one is associated with a change in the odds of diabetes status), at a significance level of 0.05."
  },
  {
    "objectID": "blogs/DA-materials/model-types.html#coefficient-interpretation-1",
    "href": "blogs/DA-materials/model-types.html#coefficient-interpretation-1",
    "title": "Model Types and Diagnostics",
    "section": "2.5 Coefficient Interpretation",
    "text": "2.5 Coefficient Interpretation\n\n\nShow the code\ngt(tidy(pima_fit)) |&gt; \n  fmt_number(decimals = 3) |&gt;\n  cols_align_decimal(c(estimate, p.value))\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n−3.165\n1.077\n−2.939\n0.003\n\n\npregnancyYes\n−0.468\n0.427\n−1.097\n0.273\n\n\nbp\n 0.040\n0.014\n2.886\n0.004\n\n\n\n\n\n\n\nNote the above is on the log-odds scale\n\n\nShow the code\ntbl_regression(pima_fit, exponentiate = TRUE) |&gt;\n  as_gt() |&gt;\n  cols_align_decimal(c(estimate, p.value))\n\n\n\n\nTable 2: ADD\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nOR\n95% CI\np-value\n\n\n\n\npregnancy\n\n\n\n\n\n\n\n\n    No\n—\n—\n\n\n\n\n    Yes\n0.63\n0.27, 1.47\n0.3  \n\n\nbp\n1.04\n1.01, 1.07\n0.004\n\n\n\nAbbreviations: CI = Confidence Interval, OR = Odds Ratio\n\n\n\n\n\n\n\n\n\n\n\nExample interpretation: Table 2 gives the results of the logistic regression fit. Women with prior pregnancies were less likely to have diabetes (OR = 0.63, 95% CI [0.27, 1.5]), but this result was not statistically significant (z = -1.1, p = 0.27). A larger sample may be necessary to determine if a relationship exists in the population.\nAnother one: Each unit of increase in blood pressure (measured in mm Hg) is associated with an increase in the log-odds of diabetes of 0.04, or a multiplication in the odds of diabetes by 1.04 (95% CI [1.01, 1.07])."
  },
  {
    "objectID": "blogs/DA-materials/model-types.html#outliers",
    "href": "blogs/DA-materials/model-types.html#outliers",
    "title": "Model Types and Diagnostics",
    "section": "2.6 Outliers",
    "text": "2.6 Outliers\nWe can approximate Cook’s distance fairly well to estimate the influence of each observation on our coefficient estimates, using augment just like before.\nWe could also likely remove the outliers and see if it qualitatively changes our findings if we aren’t sure."
  },
  {
    "objectID": "blogs/DA-materials/model-types.html#assumptions-2",
    "href": "blogs/DA-materials/model-types.html#assumptions-2",
    "title": "Model Types and Diagnostics",
    "section": "3.1 Assumptions",
    "text": "3.1 Assumptions\nWhen we fit a generalized linear model (such as Binomial and Poisson models), we make three key assumptions:\n\nThe observations are conditionally independent given X\nThe response variable follows the chosen distribution\nThe mean of the response is related to the predictors through the chosen link function and functional form\n\nResidual diagnostics can be used to check the latter two assumptions."
  },
  {
    "objectID": "blogs/DA-materials/model-types.html#diagnostics",
    "href": "blogs/DA-materials/model-types.html#diagnostics",
    "title": "Model Types and Diagnostics",
    "section": "3.2 Diagnostics",
    "text": "3.2 Diagnostics\nWe can use similar diagnostics to what we used for logistic regression. A good precursor to these is an empirical link plot, which would be done in the EDA section.\nPartial Residuals:\n\n\nShow the code\nants &lt;- read.csv(\"data/ants.csv\")\n\nants_fit &lt;- glm(Srich ~ Latitude + Elevation + Habitat, data = ants,\n                family = poisson())\n\npartial_residuals(ants_fit) |&gt;\n  ggplot(aes(x = .predictor_value, y = .partial_resid)) +\n  geom_point(color = \"grey40\", size = 0.5) +\n  geom_smooth(color = \"blue\", fill = 'skyblue2',\n              linewidth = 0.7, linetype = \"twodash\") +\n  geom_line(aes(y = .predictor_effect), color = 'red') +\n  facet_wrap(vars(.predictor_name), scales = \"free\", ncol = 2) +\n  labs(x = \"Predictor value\", y = \"Partial residual\")+\n  theme_bw()+\n  theme(strip.background = element_rect(fill = \"white\", color = \"white\"),\n        strip.text = element_text(color = \"black\", face = \"bold\"),\n        axis.text = element_text(color = \"black\"),\n        plot.title = element_text(face = \"bold\", size = 11))\n\n\n\n\n\n\n\n\n\nRandomized Quantile Residuals:\n\n\nShow the code\nlibrary(agridat)\n\nseed_fit_1 &lt;- glm(cbind(germ, n - germ) ~\n                    extract + gen,\n                  data = crowder.seeds, family = binomial())\n\nseed_aug &lt;- augment_quantile(seed_fit_1)\n\np1 &lt;- seed_aug |&gt;\n  ggplot(aes(x = .fitted, y = .quantile.resid))+\n  geom_point()+\n  geom_hline(yintercept = 0.5)+\n  geom_smooth(fill = 'skyblue2')+\n  labs(x = 'Fitted Value', y = 'Randomized Quantile Residual')+\n  theme_bw()\n\np2 &lt;- seed_aug |&gt;\n  ggplot(aes(sample = .quantile.resid))+\n  geom_qq_line(linewidth = 0.4, distribution = stats::qunif) +\n  geom_qq(size = 0.75, distribution = stats::qunif)+\n  labs(x = 'Theoretical Quantiles',\n       y = 'Observed Quantiles')+\n  theme_bw()\n\np1 + p2"
  },
  {
    "objectID": "blogs/DA-materials/model-types.html#binomial-regresson",
    "href": "blogs/DA-materials/model-types.html#binomial-regresson",
    "title": "Model Types and Diagnostics",
    "section": "3.3 Binomial Regresson",
    "text": "3.3 Binomial Regresson\nThe binomial distribution is a common response distribution whenever outcomes are binary, or whenever we count a certain binary outcome out of a total number of trials.\nThe binomial distribution is suitable when there is a fixed and known total number of trials. Each observation hence consists of a number of successes and a total number of trials; the number of trials may differ between observations.\nThe model is (with a logit-link function, the default):\nn_iY_i \\mid X_i = x_i \\sim \\text{Binomial}(n_i, \\text{logit}(\\beta^\\intercal x_i)) Hence, the sample proportion Y_i should be proportional to \\text{logit}(\\beta^\\intercal x_i).\n\nY_i is the rate/probability of success!\n\nIn R, a binomial response variable for n &gt; 1 can be provided as a a two-column matrix with the number of successes and the number of failures.\n\nlibrary(agridat)\n\nseed_fit_1 &lt;- glm(cbind(germ, n - germ) ~\n                    extract + gen,\n                  data = crowder.seeds, family = binomial())\nseed_fit_2 &lt;- glm(cbind(germ, n - germ) ~\n                    extract * gen,\n                  data = crowder.seeds, family = binomial())\n\n\n3.3.1 Inference and Predictions\nCrowder Seed Example:\n\n\nShow the code\ncrowder.seeds |&gt;\n  mutate(germ_rate = germ/n) |&gt;\n  ggplot(aes(x = gen, y = germ_rate, color = extract))+\n  geom_boxplot()+\n  labs(y = 'Germination Rate', x = 'Seed Variety')+\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nseed_test &lt;- anova(seed_fit_1, seed_fit_2, test = \"Chisq\")\nseed_test\n\n\nAnalysis of Deviance Table\n\nModel 1: cbind(germ, n - germ) ~ extract + gen\nModel 2: cbind(germ, n - germ) ~ extract * gen\n  Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi)  \n1        18     39.686                       \n2        17     33.278  1   6.4081  0.01136 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nBased on the analysis of deviance table, we conclude that that extract differs by side type, \\chi^2(1) = 6.41, p = 0.011.\n\nLike with logistic regression, we can use tbl_regression() to compute the coefficients in each model. E.g., for the interaction model:\n\n\ntbl_regression(seed_fit_2, exponentiate = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nOR\n95% CI\np-value\n\n\n\n\nextract\n\n\n\n\n\n\n\n\n    bean\n—\n—\n\n\n\n\n    cucumber\n1.72\n1.05, 2.81\n0.031\n\n\ngen\n\n\n\n\n\n\n\n\n    O73\n—\n—\n\n\n\n\n    O75\n0.86\n0.56, 1.34\n0.5\n\n\nextract * gen\n\n\n\n\n\n\n\n\n    cucumber * O75\n2.18\n1.19, 3.97\n0.011\n\n\n\nAbbreviations: CI = Confidence Interval, OR = Odds Ratio\n\n\n\n\n\n\n\n\n\nWe can also compute the predictions and the standard errors. From the table, we see that the ’75 seeds with cucumber extract have the highest predicted probability of germination.\n\n\n# first we create a grid of all possible versions of the relevant regressors we want to get predictions for:\nxs &lt;- expand.grid(gen = c(\"O75\", \"O73\"),\n                  extract = c(\"bean\", \"cucumber\"))\n\npredictions &lt;- predict(seed_fit_2, newdata = xs,\n                       type = \"response\", se.fit = TRUE) # response = rate scale\n\ngt(data.frame(gen = xs$gen, extract = xs$extract,\n           pred_germ_rate = predictions$fit, se = predictions$se.fit)) |&gt;\n  fmt_number(decimals = 3)\n\n\n\n\n\n\n\ngen\nextract\npred_germ_rate\nse\n\n\n\n\nO75\nbean\n0.364\n0.029\n\n\nO73\nbean\n0.398\n0.044\n\n\nO75\ncucumber\n0.681\n0.027\n\n\nO73\ncucumber\n0.532\n0.042\n\n\n\n\n\n\n\n\n3.3.1.1 Interpretation\n\n\nShow the code\nlibrary(Sleuth3)\n\nisland_fit &lt;- glm(cbind(Extinct, AtRisk - Extinct) ~ log10(Area), data = case2101,\n    family = binomial())\n\nexp(coef(island_fit))\n\n\n(Intercept) log10(Area) \n  0.3023423   0.5045408 \n\n\nShow the code\nci &lt;- exp(confint(island_fit)['log10(Area)',])\nci\n\n\n    2.5 %    97.5 % \n0.3910622 0.6422837 \n\n\nInterpretation: a ten-fold increase in island size (i.e., an island that is 10 times larger) is associated with the odds of extinct being multiplied by 0.5 (95% CI [0.39, 0.64]) (if not on log scale, would be a one-unit increase in X)."
  },
  {
    "objectID": "blogs/DA-materials/model-types.html#poisson-regression",
    "href": "blogs/DA-materials/model-types.html#poisson-regression",
    "title": "Model Types and Diagnostics",
    "section": "3.4 Poisson Regression",
    "text": "3.4 Poisson Regression\nIf a certain event occurs with a fixed rate, and the events are independent (so that the occurrence of one event does not make another more or less likely), then the count of events over a fixed period of time will be Poisson-distributed. This makes Poisson GLMs well-suited for response variables that are .\nThe canonical link for a Poisson distribution is the log-link, meaning that under the Poisson model, we have:\n\\log(\\mathbb{E}[Y \\mid X = x]) = \\beta^\\intercal x which is equivalent to:\n\\mathbb{E}[Y \\mid X = x] = \\exp(\\beta^\\intercal x)\nThus, the variance of Y depends on X in Poisson GLMs!\n\n3.4.1 Inference and Predictions\nAnt Example:\n\n\nShow the code\n# omitted EDA but idea is to see which covariates are associated with the ant species counts in the data\n\nants_fit &lt;- glm(Srich ~ Latitude + Elevation + Habitat, data = ants,\n                family = poisson())\n\ntbl_regression(ants_fit, exponentiate = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nIRR\n95% CI\np-value\n\n\n\n\nLatitude\n0.79\n0.70, 0.89\n&lt;0.001\n\n\nElevation\n1.00\n1.00, 1.00\n0.002\n\n\nHabitat\n\n\n\n\n\n\n\n\n    Bog\n—\n—\n\n\n\n\n    Forest\n1.89\n1.50, 2.39\n&lt;0.001\n\n\n\nAbbreviations: CI = Confidence Interval, IRR = Incidence Rate Ratio\n\n\n\n\n\n\n\n\nExample Interpretation: One additional degree in latitude is associated with the mean number of ant species being multiplied by 0.79, holding all other covariates in the model constant.\nWe can also find confidence intervals by exponentiating the confidence interval based on the profile likelihood:\n\n\n                    2.5 %       97.5 %\n(Intercept)   999.8169873 2.941911e+07\nLatitude        0.6979144 8.890260e-01\nElevation       0.9981167 9.995855e-01\nHabitatForest   1.4972591 2.393810e+00\n\n\nAnother example (Howard the Duck): Increasing the fraction of time a duck spends at the park by 0.1 is associated with the mean number of hospitalizations being multiplied by 1.09 (95% CI [1.06, 1.13]), holding all other covariates in the model constant.\n\n\n3.4.2 Offsets\nOffsets are useful when the observed counts recorded for different population sizes or time periods.\nSmoking Example:\n\n\nShow the code\nsmokers &lt;- read.csv(\"data/smokers.csv\")\n\nsmokers |&gt;\n  group_by(age, smoke) |&gt;\n  summarize(Deaths = sum(deaths),\n            `Person-years` = sum(py)) |&gt;\n  ungroup() |&gt;\n  gt()\n\n\n\n\n\n\n\n\nage\nsmoke\nDeaths\nPerson-years\n\n\n\n\n40\nno\n2\n18790\n\n\n40\nyes\n32\n52407\n\n\n50\nno\n12\n10673\n\n\n50\nyes\n104\n43248\n\n\n60\nno\n28\n5710\n\n\n60\nyes\n206\n28612\n\n\n70\nno\n28\n2585\n\n\n70\nyes\n186\n12663\n\n\n80\nno\n31\n1462\n\n\n80\nyes\n102\n5317\n\n\n\n\n\n\n\nThe person-years column indicates that people in the smoking study were observed for different number of years and/or there were different numbers of people in the study who were in each age range.\nIf we are interested in death rate, we could model the rate directly as some function of our covariates.\n\n\nShow the code\nsmokers |&gt;\n  mutate(death_rate = deaths/py) |&gt;\n  ggplot(aes(x = age, y = death_rate))+\n  geom_point(aes(color = smoke, shape = smoke))+\n  theme_bw()\n\n\n\n\n\n\n\n\n\nIn turn, we could model the outcomes (deaths in our case) as approximately Poisson, given the death rate f(\\beta^\\intercal X):\nY \\sim \\text{Poisson}(\\exp(\\beta^\\intercal X) + \\log(P)) where P is the offset, i.e., a term in our model that is fixed to have coefficient one rather than a slope that is estimated.\nNote for deaths, we are technically approximately Poisson as the upper-bound on deaths is the number of people in our study. That said, so long as the death rate is low, the Poisson distribution will assign very little probability on impossible death numbers\nIf the model with the offset is true, then we would expect a linear relationship between \\log(Y/P) and X. We can use the offset argument in the glm() function to specify our offset:\n\nsmoke_fit &lt;- glm(deaths ~ (age + I(age^2)) * smoke,\n                 offset = log(py), #offset!\n                 data = smokers, family = poisson(link = \"log\"))\n\nFurther, we can use deviance tests with offsets\n\n\nAnalysis of Deviance Table\n\nModel 1: deaths ~ age + smoke\nModel 2: deaths ~ (age + I(age^2)) * smoke\n  Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi)    \n1         7     69.182                         \n2         4      1.246  3   67.936 1.18e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nRisk Ratio Calculation:\nNow suppose we want to compare smokers and non-smokers at age 40, calculating a risk ratio: the ratio of risk of death for smokers versus non-smokers. A simple way would be to make a prediction and take the ratio. To predict the rate of death, we can predict the mean number of deaths for one person-year:\n\n\nShow the code\nnew_smokers &lt;- data.frame(smoke = c(\"no\", \"yes\"),\n                          age = c(40, 40),\n                          py = c(1, 1))\npreds &lt;- predict(smoke_fit, newdata = new_smokers, type = \"response\")\npreds[2] / preds[1]\n\n\n       2 \n3.927567 \n\n\nShow the code\n# or with:\ncoef_vec &lt;- c(0, 0, 0, 1, 40, 1600)\n\nexp(sum(coef_vec * coef(smoke_fit)))\n\n\n[1] 3.927567\n\n\nThe standard errors are not independent. We can create the Wald confidence interval by leveraging the fact that the variance and mean are equal in Poisson models.\n\n\nShow the code\nse &lt;- sqrt(t(coef_vec) %*% vcov(smoke_fit) %*% coef_vec)[1,1]\n\nbounds &lt;- sum(coef_vec * coef(smoke_fit)) + c(-2, 2) * se\nexp(bounds)\n\n\n[1]  1.49177 10.34059\n\n\nThe quite a large confidence interval is indicative of the difficulty in estimating risk ratios when the underlying rate of events is so low."
  },
  {
    "objectID": "blogs/DA-materials/model-types.html#overdispersion",
    "href": "blogs/DA-materials/model-types.html#overdispersion",
    "title": "Model Types and Diagnostics",
    "section": "3.5 Overdispersion",
    "text": "3.5 Overdispersion\nOverdispersion is when there is more variance in Y than the response distribution would predict. This could be due to:\n\nInsufficient predictors. That is, there might be other factors associated with the expected value of Y that we do not observe\nThere might be correlations we did not account for (e.g., a binomial distribution assumes the n trials are independent but what if success in one is correlated with increased success in the others?)\n\nA remedy for overdispersion is to use quasi-likleihood. These can be fit using quasibinomial() and quasipoisson() families in the glm() function. Compared to non-quasi models, the estimates of the coefficients will be the same, only the confidence intervals differ, witht heir width expanding by a constant factor\nWe can check for overdispersion using Q-Q plot of randomized quantile residuals\nExample: returning to the seed data:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nOR\n95% CI\np-value\n\n\n\n\nextract\n\n\n\n\n\n\n\n\n    bean\n—\n—\n\n\n\n\n    cucumber\n1.72\n0.88, 3.37\n0.13\n\n\ngen\n\n\n\n\n\n\n\n\n    O73\n—\n—\n\n\n\n\n    O75\n0.86\n0.48, 1.58\n0.6\n\n\nextract * gen\n\n\n\n\n\n\n\n\n    cucumber * O75\n2.18\n0.96, 4.94\n0.080\n\n\n\nAbbreviations: CI = Confidence Interval, OR = Odds Ratio\n\n\n\n\n\n\n\n\nImportant Note: since overdispersed models no longer specify a distribution of the outcome conditional on the covariates, tests based on the log-likelihood are no longer valid and we can no longer use randomized quantile residuals or do line-ups.\nWe can still still do Wald Tests and interpret coefficients using the new Confidence Interval\nEstimating the dispersion parameter…\n\nsum(residuals(ants_fit, type = 'deviance')^2)/ants_fit$df.residual\n\n[1] 1.017261"
  },
  {
    "objectID": "blogs/DA-materials/model-types.html#outliers-1",
    "href": "blogs/DA-materials/model-types.html#outliers-1",
    "title": "Model Types and Diagnostics",
    "section": "3.6 Outliers",
    "text": "3.6 Outliers\nWe can approximate Cook’s distance fairly well to estimate the influence of each observation on our coefficient estimates, using augment just like before.\nWe could also likely remove the outliers and see if it qualitatively changes our findings if we aren’t sure.\n\n\nShow the code\naugment(ants_fit) |&gt;\n  ggplot(aes(x = .fitted, y = .cooksd))+\n  geom_point()+\n  theme_bw()"
  },
  {
    "objectID": "blogs/DA-materials/model-types.html#ridge-regression",
    "href": "blogs/DA-materials/model-types.html#ridge-regression",
    "title": "Model Types and Diagnostics",
    "section": "5.1 Ridge Regression",
    "text": "5.1 Ridge Regression\nRidge Regression penalizes the L-2 norm of the covariates. We can implement Ridge Regression using the glmnet package, which requires us to provide X as a matrix and Y as a vector of responses. Note that glmnet() standardizes each column to have variance 1 automatically.\n[For Ridge Regression, \\alpha = 0]\n\n\nShow the code\nlibrary(mvtnorm)\n\nsparse_pop &lt;- population(\n  x = predictor(rmvnorm,\n                mean = rep(0, 100),\n                sigma = diag(100)),\n  y = response(4 + 0.2 * x1 - 5 * x2 + x3 + 0.4 * x4 + 8 * x5,\n               error_scale = 1)\n)\n\nsparse_samp &lt;- sparse_pop |&gt;\n  sample_x(n = 90) |&gt;\n  sample_y()\n\nx &lt;- model.matrix(~ . - 1 - y, data = sparse_samp)\n\nridge_fit &lt;- glmnet(x, sparse_samp$y, alpha = 0)\n\nplot(ridge_fit)\n\n\n\n\n\n\n\n\n\nTo determine which penalty parameter to select, we can use cross-validation via cv_glmnet() (which by default does 10-fold CV).\n\n\nShow the code\ncv_results &lt;- cv.glmnet(x, sparse_samp$y, alpha = 0)\ncv_results$lambda.min\n\n\n[1] 82.11175\n\n\nShow the code\n# if we want to plot to see what is happening:\nplot(cv_results)\n\n\n\n\n\n\n\n\n\nWhile the coefficients of Ridge Regression are not fully interpretable, we can look at which predictors have the largest magnitudes to get a sense of the relative effect size between different covariates and our outcome variable.\n\n\nShow the code\ncoefs &lt;- coef(ridge_fit, s = cv_results$lambda.min)\ncoefs[order(abs(coefs[, 1]), decreasing = TRUE)[1:5], ]\n\n\n(Intercept)          x5          x2         x82         x29 \n  3.8773665   0.8550939  -0.5844830  -0.2314598  -0.2218641 \n\n\nRidge Regression is particularly helpful when there are collinear predictors – which result in high prediction variance (encourages the effects to be ``shared” between collinear predictors). So, Ridge Regression reduces the variance of predictions."
  },
  {
    "objectID": "blogs/DA-materials/model-types.html#lasso",
    "href": "blogs/DA-materials/model-types.html#lasso",
    "title": "Model Types and Diagnostics",
    "section": "5.2 Lasso",
    "text": "5.2 Lasso\nLasso is another penalized regression model. Its most useful propoerty is that (depending on the value of the penalization parameter), it forces many of the coefficient estimates to be exactly zero. That is, Lasso promotes sparsity, which matches real-world populations where we expect the true coefficients to be sparse (think: genetics)\nWe can implement Lasso with glmnet (the only difference is now we set \\alpha = 1).\n\n\nShow the code\nlasso_fit &lt;- glmnet(x, sparse_samp$y, alpha = 1)\n\nplot(lasso_fit)\n\n\n\n\n\n\n\n\n\nLike before, we can also cross-validate:\n\n\nShow the code\ncv_results &lt;- cv.glmnet(x, sparse_samp$y, alpha = 1)\ncv_results$lambda.min\n\n\n[1] 0.1434926\n\n\nShow the code\n# and plot if we want a bitter idea of what is happening\nplot(cv_results)\n\n\n\n\n\n\n\n\n\nand, we also can look at the coefficients with the largest magnitude like before:\n\n\nShow the code\ncoefs &lt;- coef(lasso_fit, s = cv_results$lambda.min)\ncoefs[order(abs(coefs[, 1]), decreasing = TRUE)[1:15], ]\n\n\n         x5          x2 (Intercept)          x3         x63         x94 \n 7.96584001 -5.13112348  3.97693917  0.77328053 -0.12880254 -0.12133145 \n        x98          x4         x20         x91         x78         x44 \n 0.10029135  0.08173381  0.06660953 -0.06330502 -0.06226934 -0.05644002 \n        x99         x15          x9 \n 0.04719759  0.04191703 -0.03643751 \n\n\nImportantly, it can be shown that if we know the right penalization value, the model selection by Lasso is model selection consistent (if chooses all and only the true non-zero coefficient with probability 1 as the sample size tends to infinity).\nWhile we choose our penalization value based on cross-validation, so model selection consistency might not hold."
  },
  {
    "objectID": "blogs/DA-materials/model-types.html#the-elastic-net",
    "href": "blogs/DA-materials/model-types.html#the-elastic-net",
    "title": "Model Types and Diagnostics",
    "section": "5.3 The Elastic Net",
    "text": "5.3 The Elastic Net\nElastic net generalizes the case of Ridge Regression and Lasso by allowing both \\alpha and \\lambda to be hyperparameters, the combination of which trades off the benefits of Ridge Regression and Lasso. That is, with a \\alpha closer to one, more sparsity is induced, but with a \\alpha closer to 0, variance from collinearity is reduced more.\nTo cross-validate over both lambda and alpha, you have to manually loop over alpha values yourself, and run cv.glmnet() inside that loop."
  },
  {
    "objectID": "blogs/DA-materials/model-types.html#lasso-model-example-on-genetic-data",
    "href": "blogs/DA-materials/model-types.html#lasso-model-example-on-genetic-data",
    "title": "Model Types and Diagnostics",
    "section": "5.4 Lasso Model Example (on Genetic Data)",
    "text": "5.4 Lasso Model Example (on Genetic Data)\n\nset.seed(47)\ngenedat &lt;- read_csv('data/genedat-exam.csv')\n\nx &lt;- model.matrix(Disease ~ . -1 -CaseId -X, data = genedat) \nY &lt;- if_else(genedat$Disease == 'control', 0, 1)\n\ncv_genedat &lt;- cv.glmnet(x, Y, alpha = 1, family = \"binomial\") \n#ideally would have done group cv here\n\nfit_genedat &lt;- glmnet(x, Y, family = \"binomial\", alpha = 1)\n\ncoefs &lt;- coef(fit_genedat, s = cv_genedat$lambda.min)\n\nresults_df &lt;- data.frame(coeff_est = coefs[,1]) |&gt;\n  rownames_to_column('covariate') |&gt;\n  filter(coeff_est != 0, !grepl('Intercept', covariate)) |&gt;\n  arrange(desc(abs(coeff_est))) |&gt;\n  slice_head(n = 10) |&gt; \n  gt() |&gt;\n  fmt_number(decimals = 2) |&gt;\n  cols_align_decimal(coeff_est)\n  \n# ignoring within person correlation for now, but would likely want to  subset data to just one cortex area especially since there isn't two observations (could also do mixed effects approach as a future step). Also, if goal is prediction, we would want to have test and train split sets -- probably by group to avoid data leakage\n\n\nset.seed(123)\nfolds &lt;- group_vfold_cv(genedat, group = CaseId, v = 5)\nlasso_errors &lt;- sapply(folds$splits, function(split) {\n  train &lt;- analysis(split)\n  train_x &lt;- model.matrix(~ . -1 -X -Disease -CaseId, data = train)\n  \n  test &lt;- assessment(split)\n  test_x &lt;- model.matrix( ~ . -1 -X -Disease -CaseId, data = test)\n  \n  fit_genedat &lt;- glmnet(train_x, train$Disease, family = \"binomial\", alpha = 1)\n  \n  pred &lt;- predict(fit_genedat, newx = test_x, s = cv_genedat$lambda.min,\n                   type = 'response')\n  \n  pred_class &lt;- if_else(pred &gt;= 0.5, 'autism', 'control')\n  \n  mean(pred_class == test$Disease)\n})\n\nmean(lasso_errors)\n\n[1] 0.7375425"
  },
  {
    "objectID": "blogs/DA-materials/model-types.html#splitting-the-data",
    "href": "blogs/DA-materials/model-types.html#splitting-the-data",
    "title": "Model Types and Diagnostics",
    "section": "6.1 Splitting the Data",
    "text": "6.1 Splitting the Data\n\n6.1.1 General Data Splitting\n\nset.seed(47)\ncar_split &lt;- initial_split(mtcars) #default is 3:1 training-test split\ntrain_data &lt;- training(car_split)\ntest_data &lt;- testing(car_split)\n\n\n\n6.1.2 With Stratification\nIncluding a strata is helpful, particularly if there is class imbalance in our covariates or outcome to the point where it is possible that training and/or test won’t have the same levels of the variables\n\ncar_split &lt;- initial_split(mtcars, strata = 'cyl')\ntrain_data &lt;- training(car_split)\ntest_data &lt;- testing(car_split)\n\n\n\n6.1.3 By Time\n\ncovid_hew &lt;- read_csv('data/covidcast-hew.csv')\n\ntraining &lt;- covid_hew |&gt;\n  mutate(year_value = year(time_value),\n         month_value = month(time_value)) |&gt;\n  filter(month_value == 1)\n\ntesting &lt;- covid_hew |&gt;\n  mutate(year_value = year(time_value),\n         month_value = month(time_value)) |&gt;\n  filter(month_value == 3)\n\n\n\n6.1.4 By Group\nHelpful if there is correlation between observations which would lead to data leakage concerns if we do not split the data by group.\n\ncar_split &lt;- group_initial_split(mtcars, group = cyl)\ntrain_data &lt;- training(car_split)\ntest_data &lt;- testing(car_split)"
  },
  {
    "objectID": "blogs/DA-materials/model-types.html#nested-cross-validation",
    "href": "blogs/DA-materials/model-types.html#nested-cross-validation",
    "title": "Model Types and Diagnostics",
    "section": "6.2 Nested Cross Validation",
    "text": "6.2 Nested Cross Validation\nOften, we might need to do cross-validation and want to keep a hold out set that we can test our model on later. Here would be a a framework for nested cross validation:\n\nSplit the full dataset into testing and training (we might do this randomly, by time, by group, etc.)\nOn the training data do k-fold cross validation to select any needed parameters (e.g., penalization parameters)\nFit the model on the full training data with the selected tuning parameter values\nAssess predictive performance on the test data"
  },
  {
    "objectID": "blogs/DA-materials/model-types.html#useful-metrics",
    "href": "blogs/DA-materials/model-types.html#useful-metrics",
    "title": "Model Types and Diagnostics",
    "section": "6.3 Useful Metrics",
    "text": "6.3 Useful Metrics\nReport on test data but could be interesting to compare test performance to how it performs on training data\n\nIf test data performance is same or better than training data, this indicates that the model is underfit/biased\nIf the test data performance is much lower than the training data, this might raise concerns about overfitting\n\nFor regression:\n\nMean-Squared Error (MSE)\nRoot Mean-Squared Error (RMSE): a measure of the average magnitude of the errors between predicted and actual values in a regression model\nActual versus Predicted Outcome Plot (ideal would be along the main diagonal)\n\nFor classification:\n\nCalibration plot\nConfusion matrix\n\nFalse Negative Rate, False Positive Rate, Specificity, Sensitivity\n\nROC Curve (AUC)\nAccuracy\n\nNote: when looking at accuracy, always report what the base incidence rate is (this would be the accuracy of a null model)."
  },
  {
    "objectID": "blogs/DA-materials/recipes.html",
    "href": "blogs/DA-materials/recipes.html",
    "title": "Data Analysis Recipes",
    "section": "",
    "text": "For the linear regression model \\(\\mathbf{Y} = \\mathbf{X}\\beta + e\\), we have the following assumptions:\n\n\nErrors have mean 0: \\(\\mathbb{E}[Y \\mid X] = 0\\)\n\nThe error variance is constant: \\(\\text{Var}[e \\mid \\mathbf{X}] = \\sigma^2\\)\n\nThe errors are uncorrelated (the data points are iid): \\(\\text{Var}[e \\mid \\mathbf{X}] = \\sigma^2\\)\n\nThe errors are normally distributed\n\nSteps:\n\nFit model (consider interactions, non-linear patterns seen in EDA, confounding variables, transformations)\n\nPartial residual analysis to account for any remaining non-linearity, adjusting the model as needed [diagnostic for non-linearity]\n\nPlot residuals against fitted values; look for heteroskedasticity and use sandwich estimator for all inference if present [diagnostic for constant error variance and errors have mean zero]\n\nPlot Q-Q plot to check if there are any gross deviations from normality [diagnostic for normally distributed errors; not a big deal if the sample size is large]\nCheck for how outliers identified during EDA impact on coefficients via Cook’s Distance\n\nCheck for collinearity issues using EDA and VIF scores\n\nIf we included spline or polynomial terms, conduct an F test and create effect plots, with example interpretations that include confidence intervals\n\nOtherwise, for linear terms, conduct a t test and provide test statistic, p-value, and confidence interval\n\nIf sample size is small, make a statement about power\n\nLimitations:\n\nUnaccounted for correlation/repeated measures (lack of independence)\nUnmeasured confounders\nDichotomized confounders\nNon-random missingness\nSample demographics versus population of interest\nStatistical versus practical significance"
  },
  {
    "objectID": "blogs/DA-materials/recipes.html#linear-regression",
    "href": "blogs/DA-materials/recipes.html#linear-regression",
    "title": "Data Analysis Recipes",
    "section": "",
    "text": "For the linear regression model \\(\\mathbf{Y} = \\mathbf{X}\\beta + e\\), we have the following assumptions:\n\n\nErrors have mean 0: \\(\\mathbb{E}[Y \\mid X] = 0\\)\n\nThe error variance is constant: \\(\\text{Var}[e \\mid \\mathbf{X}] = \\sigma^2\\)\n\nThe errors are uncorrelated (the data points are iid): \\(\\text{Var}[e \\mid \\mathbf{X}] = \\sigma^2\\)\n\nThe errors are normally distributed\n\nSteps:\n\nFit model (consider interactions, non-linear patterns seen in EDA, confounding variables, transformations)\n\nPartial residual analysis to account for any remaining non-linearity, adjusting the model as needed [diagnostic for non-linearity]\n\nPlot residuals against fitted values; look for heteroskedasticity and use sandwich estimator for all inference if present [diagnostic for constant error variance and errors have mean zero]\n\nPlot Q-Q plot to check if there are any gross deviations from normality [diagnostic for normally distributed errors; not a big deal if the sample size is large]\nCheck for how outliers identified during EDA impact on coefficients via Cook’s Distance\n\nCheck for collinearity issues using EDA and VIF scores\n\nIf we included spline or polynomial terms, conduct an F test and create effect plots, with example interpretations that include confidence intervals\n\nOtherwise, for linear terms, conduct a t test and provide test statistic, p-value, and confidence interval\n\nIf sample size is small, make a statement about power\n\nLimitations:\n\nUnaccounted for correlation/repeated measures (lack of independence)\nUnmeasured confounders\nDichotomized confounders\nNon-random missingness\nSample demographics versus population of interest\nStatistical versus practical significance"
  },
  {
    "objectID": "blogs/DA-materials/recipes.html#logistic-regression",
    "href": "blogs/DA-materials/recipes.html#logistic-regression",
    "title": "Data Analysis Recipes",
    "section": "2 Logistic Regression",
    "text": "2 Logistic Regression\nFor the logistic regression model \\(\\log(\\text{odds}(\\mathbf{Y})) = \\mathbf{X}\\beta\\), we have the following assumptions:\n\n\nLog-odds of \\(Y\\) are linearly related to the regressors \\(X\\)\n\nThe observations \\(Y_i\\) are conditionally independent of each other given the covariates \\(X_i\\)\n\nSteps\n\nFit model (consider interactions, non-linear patterns seen in EDA, confounding variables, transformations)\n\nPartial residual analysis to account for any remaining non-linearity with log-odds, adjusting the model as needed [diagnostic for non-linearity between log-odds and regressors]\n\nPlot randomized quantile residuals and corresponding Q-Q plot (with uniform distribution) for each predictor [second diagnostic for non-linearity between log-odds and regressors]\n\nOptionally, create a calibration plot and randomized quantile residuals against fitted values\n\nCheck for how outliers identified during EDA impact on coefficients via Cook’s Distance\n\nCheck for collinearity issues using EDA and VIF scores\n\nIf we included spline or polynomial terms, conduct a deviance test and create effect plots, with example interpretations that include confidence intervals (ensure it is on response scale, i.e., not log-odds)\n\nOtherwise, for linear terms, conduct a Wald test and provide test statistic (\\(z\\)), p-value, and confidence interval (exponentiate!)\n\nIf sample size is small, make a statement about power\n\nLimitations:\n\nUnaccounted for correlation/repeated measures (lack of independence)\nUnmeasured confounders\nDichotomized confounders\nNon-random missingness\nSample demographics versus population of interest\nStatistical versus practical significance\nIf cohort study think about which type: prospective, retrospective, case-control"
  },
  {
    "objectID": "blogs/DA-materials/recipes.html#binomial-regression",
    "href": "blogs/DA-materials/recipes.html#binomial-regression",
    "title": "Data Analysis Recipes",
    "section": "3 Binomial Regression",
    "text": "3 Binomial Regression\nFor the binomial regression model \\(n_iY_i \\mid X_i = x_i \\sim \\text{Binomial}(n_i, \\log(\\text{odds}(x_i\\beta)))\\). We use binomial regression when we have a fixed number of trials, with each observation consisting of a number of successes and total number of trials (so we can infer failures). In binomial regression, we have the following assumptions:\n(1)The observations are conditionally independent given \\(X\\)\n(2) The response variable follows a binomial distribution\n(3) The mean of the response variable is related to the predictors through the logit link and functional form [We want a linear relationship between the predictor and the log-odds of the rate, \\(Y_i\\)]:\n\\[\\log \\left( \\frac{rate}{1 - rate} \\right)\\]\nSteps:\n\nFit model (consider interactions, non-linear patterns seen in EDA, confounding variables, transformations)\n\nPartial residual analysis to account for any remaining non-linearity with log-odds, adjusting the model as needed [diagnostic for non-linearity between log-odds and regressors]\n\nPlot randomized quantile residuals and corresponding Q-Q plot (with uniform distribution) for each predictor [second diagnostic for non-linearity between log-odds and regressors]\n\nCheck for overdispersion using Q-Q plot from fitted. If there is evidence of overdispersion, move to quasi-GLM section\nCheck for how outliers identified during EDA impact on coefficients via Cook’s Distance\n\nCheck for collinearity issues using EDA and VIF scores\n\nIf we included spline or polynomial terms, conduct a deviance test and create effect plots, with example interpretations that include confidence intervals (ensure it is on response scale, i.e., not log-odds)\nOtherwise, for linear terms, conduct a Wald test and provide test statistic (\\(z\\)), p-value, and confidence interval (exponentiate!)\n\nIf sample size is small, make a statement about power\n\nLimitations:\n\nUnaccounted for correlation (spatiotemporal), dependence of successive trials (e.g., success in one, increases probability of success in another)\nUnmeasured confounders\nDichotomized confounders\nNon-random missingness\nSample demographics versus population of interest\nStatistical versus practical significance\nIf cohort study think about which type: prospective, retrospective, case-control"
  },
  {
    "objectID": "blogs/DA-materials/recipes.html#poisson-regression",
    "href": "blogs/DA-materials/recipes.html#poisson-regression",
    "title": "Data Analysis Recipes",
    "section": "4 Poisson Regression",
    "text": "4 Poisson Regression\nWhen certain event that occurs with a fixed rate, and the events are independent (so that the occurrence of one event does not make another more or less likely), then the count of events over a fixed period of time will be Poisson-distributed. For the Poisson regression model \\(\\mathbb{E}[Y \\mid X = x] = \\exp(\\beta^\\intercal x)\\), we have the following assumptions:\n(1)The observations are conditionally independent given \\(X\\)\n(2) The response variable follows a Poisson distribution\n(3) The mean of the response variable is related to the predictors through the log link and functional form [We want a linear relationship between the predictor and the log mean of the outcome, either rate or count]\nSteps:\n\nFit model (consider interactions, non-linear patterns seen in EDA, confounding variables, transformations)\n(1b) Consider whether an offset is needed. Used in cases where the counts are from different time period lengths or popualtion sizes\n\nPartial residual analysis to account for any remaining non-linearity with log mean outcome, adjusting the model as needed [diagnostic for non-linearity between log mean outcome and regressors]\n\nPlot randomized quantile residuals and corresponding Q-Q plot (with uniform distribution) for each predictor [second diagnostic for non-linearity between log mean outcome and regressors]\n\nCheck for overdispersion using Q-Q plot from fitted. If there is evidence of overdispersion, move to quasi-GLM section\n\nCheck for how outliers identified during EDA impact on coefficients via Cook’s Distance\n\nCheck for collinearity issues using EDA and VIF scores\n\nIf we included spline or polynomial terms, conduct a deviance test and create effect plots, with example interpretations that include confidence intervals (ensure it is on response scale, i.e., not log scale)\nOtherwise, for linear terms, conduct a Wald test and provide test statistic (\\(z\\)), p-value, and confidence interval (exponentiate!)\n\nIf sample size is small, make a statement about power\n\nLimitations:\n\nUnaccounted for correlation (spatiotemporal), dependence between events, and non-fixed rate of occurence\nInsufficient data to include an offset term\nUnreasonable approximation of count data as Poisson – could assign non-trivial probability to impossible counts\nUnmeasured confounders\nDichotomized confounders\nNon-random missingness\nSample demographics versus population of interest\nStatistical versus practical significance\nIf cohort study think about which type: prospective, retrospective, case-control"
  },
  {
    "objectID": "blogs/DA-materials/recipes.html#quasi-glm-regression",
    "href": "blogs/DA-materials/recipes.html#quasi-glm-regression",
    "title": "Data Analysis Recipes",
    "section": "5 Quasi-GLM Regression",
    "text": "5 Quasi-GLM Regression\nQuasi-GLM models are when there is overdispersion (or underdispersion) in the standard Binomial or Poisson regression model.\nNotably, overdispersion is when there is more variance in \\(Y\\) than the response distribution would predict. This could be due to:\n\nInsufficient predictors. That is, there might be other factors associated with the expected value of \\(Y\\) that we do not observe\nThere might be correlations we did not account for (e.g., a binomial distribution assumes the \\(n\\) trials are independent but what if success in one is correlated with increased success in the others?)\n\nSteps:\n\nFit the quasi-GLM model (using the regressors determined from the standard Binomial or Poisson regression model)\n\nConfirm linear relationships/goodness-of-fit with partial residual plot (should be same to above with perhaps wider variance bands)\n\nBased we no have a proper likelihood function, we cannot check our model with randomized quantile residuals\nCheck for how outliers identified during EDA impact on coefficients via Cook’s Distance\n\nCheck for collinearity issues using EDA and VIF scores\n\nIf we included spline or polynomial terms, we can no longer conduct deviance tests. We can still use effect plots with example interpretations that include confidence intervals (ensure it is on response scale, i.e., not log-odds or log scale)\nOtherwise, for linear terms, conduct a Wald test and provide test statistic (\\(z\\)), p-value, and confidence interval (exponentiate!); these coefficient estimates should be the same but the confidence interval should have a different width\n\nIf sample size is small, make a statement about power\n\nLimitations:\n\nCannot test for significance of spline or polynomial terms\nUnaccounted for correlation (spatiotemporal), dependence of successive trials (e.g., success in one, increases probability of success in another), dependence between events, and non-fixed rate of occurence\nInsufficient data to include an offset term\nUnreasonable approximation of count data as Poisson – could assign non-trivial probability to impossible counts\nUnmeasured confounders\nDichotomized confounders\nNon-random missingness\nSample demographics versus population of interest\nStatistical versus practical significance\nIf cohort study think about which type: prospective, retrospective, case-control"
  },
  {
    "objectID": "blogs/DA-materials/recipes.html#prediction",
    "href": "blogs/DA-materials/recipes.html#prediction",
    "title": "Data Analysis Recipes",
    "section": "6 Prediction",
    "text": "6 Prediction\nWe want to use ridge regression for high-dimensional data, where we have highly collinear predictors (since it promites sharing effects) and lasso for high-dimensional data, where we want sparsity.\nSteps:\n\nSplit the data into test and training sets trying as much as possible to avoid data leakage (e.g., keep repeated measures together if possible)\n\nCreate model matrix of covariates for testing and training data\n\nCross validate to select penalization parameter \\(\\lambda\\) using the training data\n\nFit model on full training data using cross-validated penalization parameter – make sure to specify the correct distribution family based on outcome. Note it is gaussian by default.\n(4b) If doing classification, pick the threshold for positive versus negative class using ROC curve where it is closest to top left\n\nPredict on the test data and calculate (R)MSE to assess overall predictive performance (if binary compare accuracy to incidence rate)\n\n\nRMSE: a measure of the average magnitude of the errors between predicted and actual values in a regression model\n\n\nIf classification, also compute sensitivity and specificity and/or AUC to show relative performance on positive and negative classes\n\nIf continuous outcome, plot actual versus predicted values (ideal predictive model would like along slope = 1, intercept = 0 line)"
  },
  {
    "objectID": "model-types-2.html",
    "href": "model-types-2.html",
    "title": "Model Types and Diagnostics",
    "section": "",
    "text": "For the linear regression model \\mathbf{Y} = \\mathbf{X}\\beta + e, we have the following assumptions:\n\n\nErrors have mean 0: \\mathbb{E}[Y \\mid X] = 0\n\nThe error variance is constant: \\text{Var}[e \\mid \\mathbf{X}] = \\sigma^2\n\nThe errors are uncorrelated (the data points are iid): \\text{Var}[e \\mid \\mathbf{X}] = \\sigma^2\n\nThe errors are normally distributed\n\n\n\n\n\n\nLinearity: partial residual plots help us diagnose whether the true relationship between the outcome and some of the predictors is non-linear\n\n\nShow the code\nfit &lt;- lm(price2007 ~ distance + resid_walkscore + squarefeet + bedgroup + \n            zip, data = rail_trail_modeling)\n\nfit_new &lt;- lm(price2007 ~ distance + ns(resid_walkscore, knots = c(0.5)) + \n                squarefeet + bedgroup + zip, data = rail_trail_modeling)\n\np1 &lt;- partial_residuals(fit) |&gt;\n  mutate(.predictor_name = case_when(\n    .predictor_name == \"distance\" ~ \"Distance to Rail Trail\",\n    .predictor_name == \"walkscore\" ~ \"Walking Score\",\n    .predictor_name == \"bikescore\" ~ \"Biking Score\",\n    .predictor_name == \"squarefeet\" ~ \"Square Footage\",\n    .predictor_name == \"resid_walkscore\" ~ \"Residual Walk Score\",\n    TRUE ~ .predictor_name)) |&gt;\n  ggplot(aes(x = .predictor_value, y = .partial_resid)) +\n  geom_point(color = \"grey40\", size = 0.5) +\n  geom_smooth(color = \"blue\", fill = 'skyblue2',\n              linewidth = 0.7, linetype = \"twodash\") +\n  geom_line(aes(y = .predictor_effect), color = 'red') +\n  facet_wrap(vars(.predictor_name), scales = \"free\", ncol = 2) +\n  labs(x = \"Predictor value\", y = \"Partial residual\")+\n  ggtitle(\"(a) Partial Residual Plots,\\nPreliminary Linear Model\")+\n  theme_bw()+\n  theme(strip.background = element_rect(fill = \"white\", color = \"white\"),\n        strip.text = element_text(color = \"black\", face = \"bold\"),\n        axis.text = element_text(color = \"black\"),\n        plot.title = element_text(face = \"bold\", size = 11))\n\np2 &lt;- partial_residuals(fit_new) |&gt;\n  mutate(.predictor_name = case_when(\n    .predictor_name == \"distance\" ~ \"Distance to Rail Trail\",\n    .predictor_name == \"walkscore\" ~ \"Walking Score\",\n    .predictor_name == \"bikescore\" ~ \"Biking Score\",\n    .predictor_name == \"squarefeet\" ~ \"Square Footage\",\n    .predictor_name == \"resid_walkscore\" ~ \"Residual Walk Score\",\n    TRUE ~ .predictor_name)) |&gt;\n  ggplot(aes(x = .predictor_value, y = .partial_resid)) +\n  geom_point(color = \"grey40\", size = 0.5) +\n  geom_smooth(color = \"blue\", fill = 'skyblue2',\n              linewidth = 0.7, linetype = \"twodash\") +\n  geom_line(aes(y = .predictor_effect), color = 'red') +\n  facet_wrap(vars(.predictor_name), scales = \"free\", ncol = 2) +\n  labs(x = \"Predictor value\", y = \"Partial residual\")+\n  ggtitle(\"(b) Partial Residual Plots, Adding\\nNatural Splines (Knot at Median)\\nto Residual Walk Score\")+\n  theme_bw()+\n  theme(strip.background = element_rect(fill = \"white\", color = \"white\"),\n        strip.text = element_text(color = \"black\", face = \"bold\"),\n        axis.text = element_text(color = \"black\"),\n        plot.title = element_text(face = \"bold\", size = 11))\n\np1 + p2\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrom Figure 1(a), we see that residual walk score has a nonlinear relationship with estimated house price in 2007 – as indicated by the smoothed residual curve (in blue) diverging from the predicted effect line (in red). We would address this nonlinearity by fitting natural splines on the residual walk score, with one knot at the median. Figure 1(b) provides the updated partial residual plots when we add natural splines to the residual walk score. We see now that the predicted effect line (in red) tracks closely to the smoothed residual curve (in blue) without over-fitting to model to the turns in the smoothed residual curve.\nLineups: If we aren’t sure whether the true relationship between the outcome and a given predictor is non-linear, we can conduct a partial residual visual lineup.\nFor instance, it is not entirely clear whether distance has a non-linear relationship to estimated house price in 2007 or if this is just due to the high-leverage home with the largest distance to the rail trail. Figure 2 shows the partial residual line-up. If we cannot pick out the partial residual plot corresponding to the real data from the simulated data, we can conclude that there is insufficient evidence of a non-linear relationship between distance to rail trail and estimated house price in 2007 at a significance level of 0.05.\n\n\nShow the code\nset.seed(123)\nmodel_lineup(fit_new, fn = partial_residuals, nsim = 20) |&gt;\n  filter(.predictor_name == \"distance\") |&gt;\n  mutate(.sample_new = paste0(\"Position \", .sample),\n         .sample_new = fct_reorder(.sample_new, .sample)) |&gt;\n  ggplot(aes(x = .predictor_value, y = .partial_resid))+\n  geom_point(color = \"grey40\", size = 0.5) +\n  geom_line(aes(y = .predictor_effect), color = 'red') +\n  geom_smooth(color = \"blue\", fill = 'skyblue2',\n              linewidth = 0.7, linetype = \"twodash\") +\n  facet_wrap(~.sample_new)+\n  labs(x = \"Distance to Rail Trail (in Miles)\", y = \"Partial residual\")+\n  theme_bw()+\n  theme(strip.background = element_rect(fill = \"white\", color = \"white\"),\n        strip.text = element_text(color = \"black\", face = \"bold\"),\n        axis.text = element_text(color = \"black\"),\n        plot.title = element_text(face = \"bold\", size = 11))\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can decrypt (show the the position of our actual data) by running the command, which is given at the end of the line-up call (dycrypt()):\n\ndecrypt(\"QUg2 qFyF Rx 8tLRyRtx ZP\")\n\n[1] \"True data in position  16\"\n\n\n\n\n\nAdd polynomial terms to the model\n\nSimplest way to add polynomial terms is with I():\n\nfit_new &lt;- lm(price2007 ~ distance + resid_walkscore + I(resid_walkscore^2) +\n                squarefeet + bedgroup + zip, data = rail_trail_modeling)\n\nIf there is numerical issues (e.g., from the predictor having large values), we can use the poly()function which constructs the design matric using an orthogonal polynomial basis.\nA downside is that the coefficients are no longer is terms of the interpretable regressor terms. The fitted values though wil be the same between the two approaches for adding polynomial terms.\n\nfit_new &lt;- lm(price2007 ~ distance + poly(resid_walkscore, degree = 2) + squarefeet + \n                bedgroup + zip, data = rail_trail_modeling)\n\n\nAdd regression splines to the model\n\nRegression splines model relationships as being piecewise polynomial. They require us to choose knots, which are the fixed points between which the function is polynomial. Two common approaches are natural splines and B-spline basis. Statisticians almost universally use cubic splines. Cubic splines are continuous in their first and second derivatives, making the knots almost visually imperceptible in plots of the spline.\nFor knot selection, a typical approach is to use quantiles of the data as knots – erring on the side of less knots (especially if there is few observations).\n\nfit_new &lt;- lm(price2007 ~ distance + ns(resid_walkscore, knots = c(0.5)) + \n                squarefeet + bedgroup + zip, data = rail_trail_modeling)\n\n\n\n\n\nErrors with non-constant variance are called heteroskedastic. To detect heteroskedasticity, we can examine the residuals plotted against the fitted values.\n\n\nShow the code\nfit_new |&gt;\n  augment() |&gt;\n  ggplot(aes(x = .fitted, y = .resid))+\n  geom_point(size = 0.5)+\n  geom_hline(yintercept = 0)+\n  geom_smooth(fill = 'skyblue2')+\n  labs(x = \"Fitted Values\", y = \"Residuals\")+\n  theme_bw()+\n  theme(axis.text = element_text(color = \"black\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn Figure 3, we have non-constant error variance. Indeed, there appears to be an approximately quadratic relationship between our fitted values and our residual terms.\nWe can remedy non-constant error variance by using the sandwich estimator for the variance of our estimators in all inference.\n\n# uses sandwich estimator with HC3 estimator for Omega\nConfint(fit_new, vcov = vcovHC(fit_new))\n\nStandard errors computed by vcovHC(fit_new) \n\n\n                                       Estimate      2.5 %     97.5 %\n(Intercept)                           48.607106 -15.216646 112.430857\ndistance                             -14.779103 -23.768308  -5.789898\nns(resid_walkscore, knots = c(0.5))1  93.909271 -17.234103 205.052646\nns(resid_walkscore, knots = c(0.5))2 -18.347598 -85.985661  49.290465\nsquarefeet                           145.196077 122.379951 168.012202\nbedgroup3 beds                         7.595663  -9.282322  24.473649\nbedgroup4+ beds                      -28.637850 -55.631857  -1.643844\nzip1062                              -30.040888 -50.154050  -9.927726\n\n\n\n\n\nWe can check for non-normality of errors using residual Q-Q plots. Using the standardized residuals, we can detect gross deviations from normality while ignoring small deviations in non-normality. An example of a Q-Q plot for linear regression is in Figure 4. Since we are doing linear regression, the theoretical quantiles correspond to a normal distribution with mean 0 and variance 1.\n\n\nShow the code\naugment(fit_new) |&gt;\n  ggplot(aes(sample = .std.resid)) +\n  geom_qq_line(linewidth = 0.4) +\n  geom_qq(size = 0.75)+\n  labs(x = 'Theoretical Quantiles',\n       y = 'Observed Quantiles')+\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\n\n\n\nLike with partial residuals, we can use a lineup to detect whether the non-normality in errors is statistically significant (at a significance level of 0.05).\n\n\nShow the code\nset.seed(123)\nmodel_lineup(fit_new) |&gt; \n  mutate(.sample_new = paste0(\"Position \", .sample),\n         .sample_new = fct_reorder(.sample_new, .sample)) |&gt; \n  ggplot(aes(sample = .std.resid)) +\n  geom_qq_line(linewidth = 0.4) +\n  geom_qq(size = 0.5) +\n  facet_wrap(vars(.sample_new)) +\n  labs(x = \"Theoretical Quantiles\",\n       y = \"Observed Quantiles\")+\n  theme_bw()+\n  theme(strip.text = element_text(face = \"bold\"),\n        strip.background = element_rect(fill = \"white\", color = \"white\"),\n        text = element_text(color = \"black\"),\n        axis.text = element_text(color = \"black\"))\n\n\n\n\n\n\n\n\n\n\n\n[1] \"True data in position  16\"\n\n\nNote: if the sample size is sufficiently large, then violating non-normality of errors is not a serious concern. Otherwise, if the assumptions linearity and constant error variance hold, then interpret coefficient estimates with caution (since coefficient estimates would not be approximately normal).\n\n\n\n\nOutliers: Not all outliers have a major effect on the regression line. So, their presence may not be a problem. We hence need a way to detect outliers and characterize their influence on the regression line.\n\nIf we can determine the outlier is due to some kind of measurement or recording error, we can correct the error.\nIf we cannot, we must make the difficult decision of whether to keep the outlier, and acknowledge that it may significantly influence our estimates, or to remove it and risk throwing away good information\n\nOne way to measure the influence of an observation on the regression is to quantify how its inclusion changes our coefficient estimates via Cook’s Distance.\n\nLook for Cook’s Distance (D_i), where D_i \\geq 1, though this is again a matter of judgment, to indicate that a particular observation substantially changes the regression fit.\n\nFigure 5 shows that all observations have Cook’s Distances well below the standard cut-off of 1, so there does not appear that any of the homes in the data have a large influence on our regression model.\n\n\nShow the code\nmax_price &lt;- augment(fit_new) |&gt;\n  arrange(desc(price2007)) |&gt;\n  slice_head(n = 1)\n\naugment(fit_new) |&gt;\n  ggplot(aes(y = .cooksd, x = price2007))+\n  geom_point(size = 0.7, alpha = 0.7)+\n  geom_point(data = max_price, aes(y = .cooksd, x = price2007),\n             size = 2, color = \"red\")+\n  labs(y = \"Cook's Distance\", x = \"Estimated House Price (2007)\")+\n  #geom_hline(yintercept = 1, linetype = \"dashed\")+\n  theme_bw()+\n  theme(plot.title = element_blank(),\n        text = element_text(color = \"black\"),\n        axis.text = element_text(color = \"black\"),\n        axis.title = element_text(color = \"black\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhen the model matrix X is not full column rank, its columns span a lower-dimensional space. There is still a unique perpendicular projection of Y onto this space, and hence a unique \\hat Y. But the coordinates of that fit are not unique, meaning that there are infinite values of \\hat \\beta that correspond to the same squared error/prediction. If we care about inference on \\hat \\beta this is a serious problem.\n\n\nShow the code\nloess_wrapper &lt;- function(data, mapping, ...){\n      p &lt;- ggplot(data = data, mapping = mapping) + \n      geom_point(size = 0.35, color = \"skyblue2\") + \n      geom_smooth(linetype = \"dashed\", color = \"black\", linewidth = 0.6)\n      return(p)\n}\n\nrail_trail_modeling |&gt;\n  select(price2007, distance, walkscore, squarefeet) |&gt;\n  rename(`2007 Est.\\nHouse Price\\n(thousands of $)` = price2007,\n         `Distance to\\nRail Trail (miles)` = distance,\n         `Walk\\nScore` = walkscore,\n         `Square\\nFootage` = squarefeet) |&gt;\n  ggpairs(upper = list(continuous = wrap(\"cor\", color = \"black\")),\n          lower = list(continuous = loess_wrapper,\n                       combo = wrap(\"barDiag\", size=0.2,\n                                     alpha = 0.9))) +\n  scale_x_continuous(breaks = scales::pretty_breaks(n = 4))+\n  scale_y_continuous(breaks = scales::pretty_breaks(n = 4))+\n  theme_bw()+\n  theme(strip.background = element_rect(fill = \"white\", color = \"white\"),\n        strip.text = element_text(color = \"black\", face = \"bold\", size = 6.5),\n        axis.text = element_text(color = \"black\", size = 6.5))\n\n\n\n\n\n\n\n\n\n\n\nCondition Number:\nHowever, multiple variables can be collinear even when individual pairs of variables have low correlation. One way to detect non-pairwise collinearity is to use the condition number, where we compare the largest and smallest eigenvalues of X^\\intercal X:\n\nX &lt;- model.matrix(price2007 ~ distance + resid_walkscore + \n                squarefeet + bedgroup + zip - 1, data = rail_trail_modeling)\n# first step is to scale the model matrix X\nscaled_x &lt;- scale(X, center = FALSE, scale = TRUE)\n  \n# Then, we can use the kappa() function on X'X\nkappa(t(scaled_x) %*% scaled_x)\n\n[1] 123.3692\n\n\nIdeally, we want the condition number to be small. The general cut-off is condition numbers larger than 50 or 100. So, from the results above, we see that there is potentially concerning levels of collinearity among our predictors, which could influence the accuracy of our inference.\nVariance Inflation Factors:\nAnother way to look at collinearity is with variance inflation factors (VIF), which can be interpreted as showing how much the variance of \\hat \\beta_j is inflated relative to a model where there is no collinearity (and all columns are orthogonal).\n\n# consider using type = 'predictor' for interactions (or ignoring VIFs since compared to main effecs)\ncar::vif(lm(price2007 ~ distance + ns(resid_walkscore, knots = c(0.5)) + \n                squarefeet + bedgroup + zip, data = rail_trail_modeling))\n\n                                        GVIF Df GVIF^(1/(2*Df))\ndistance                            1.379898  1        1.174691\nns(resid_walkscore, knots = c(0.5)) 1.409755  2        1.089647\nsquarefeet                          1.951119  1        1.396825\nbedgroup                            1.725889  2        1.146181\nzip                                 1.342293  1        1.158574\n\n\nA general rule of thumb is that VIF values exceeding 5 warrant further investigation, and VIFs exceeding 10 are signs of serious multicollinearity requiring correction.\nGeneral Notes: If we are confronted with collinearity, then, the question to ask is:\n\n“Have we chosen the correct predictors for the research question?”\n\nIf we have, there is little to be done; if we have not, we can reconsider our choice of predictors and perhaps eliminate the collinear ones.\nIf we are interested in prediction rather than in the coefficients, collinearity is a problem insofar as it creates high prediction variance, and we might reconsider our model and use a penalization model to reduce the prediction variance.s\n\n\n\n\nAn interaction allows one predictor’s association with the outcome to depend on values of another predictor.\nWhen an interaction is present, the normal interpretation of coefficients as slopes no longer holds for the predictors involved in the interaction.\nSee Example 7.4\n\n\n\n\n\nMany useful null hypotheses can be written in terms of linear combinations of the coefficients. For example, consider a linear model with a continuous predictor X_1 and dummy-coded regressor X_2 \\in \\{0,1 \\}:\nY = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_1 X_2 We could think of several null hypotheses to test:\n\n\\beta_2 = 0: the two factor levels have identical intercepts\n\\beta_3 = 0: the two factor levels have identical slopes\n\\beta_1 = c: the slope when X_2 = 0 is some value c predicted by a theory we are testing (usually c = 0 in linear regression, which corresponds to no association)\n\\beta_1 + \\beta_3 = 0: when X_2 = 1, there is no association between X_1 and Y\n\\beta_2 = \\beta_3 = 0: the two factor levels have identical relationships between X_1 and Y\n\nFor all but the last listed hypothesis, we can use a t test. R conducts a t test for every coefficient by default, with c = 0.\nThe degrees of freedom for a t test is n - p, where p is the number of parameters in our model. We can get this via fit$df.residual. E.g., for our rail trail linear model example we have df = 96.\nThe function tidy() does the test for c = 0 with every other predictor held constant in the model.\n\n\nShow the code\ngt(tidy(fit_new)) |&gt;\n  fmt_number(decimals = 3)\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n48.607\n27.295\n1.781\n0.078\n\n\ndistance\n−14.779\n5.039\n−2.933\n0.004\n\n\nns(resid_walkscore, knots = c(0.5))1\n93.909\n43.190\n2.174\n0.032\n\n\nns(resid_walkscore, knots = c(0.5))2\n−18.348\n22.890\n−0.802\n0.425\n\n\nsquarefeet\n145.196\n10.082\n14.401\n0.000\n\n\nbedgroup3 beds\n7.596\n12.269\n0.619\n0.537\n\n\nbedgroup4+ beds\n−28.638\n15.037\n−1.905\n0.060\n\n\nzip1062\n−30.041\n9.438\n−3.183\n0.002\n\n\n\n\n\n\n\nWe can also use tbl_regression() which gives us the confidence intervals and p-values:\n\n\nShow the code\ntbl_regression(fit_new)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nBeta\n95% CI\np-value\n\n\n\n\ndistance\n-15\n-25, -4.8\n0.004\n\n\nns(resid_walkscore, knots = c(0.5))\n\n\n\n\n\n\n\n\n    ns(resid_walkscore, knots = c(0.5))1\n94\n8.2, 180\n0.032\n\n\n    ns(resid_walkscore, knots = c(0.5))2\n-18\n-64, 27\n0.4\n\n\nsquarefeet\n145\n125, 165\n&lt;0.001\n\n\nbedgroup\n\n\n\n\n\n\n\n\n    1-2 beds\n—\n—\n\n\n\n\n    3 beds\n7.6\n-17, 32\n0.5\n\n\n    4+ beds\n-29\n-58, 1.2\n0.060\n\n\nzip\n\n\n\n\n\n\n\n\n    1060\n—\n—\n\n\n\n\n    1062\n-30\n-49, -11\n0.002\n\n\n\nAbbreviation: CI = Confidence Interval\n\n\n\n\n\n\n\n\nIf we have non-constant error variance and thus use the sandwich estimator, we can use the following code to conduct t tests and present our results:\n\np.values &lt;- coeftest(fit_new, vcov = vcovHC(fit_new)) |&gt;\n  tidy() |&gt; _$p.value\n\nt.stats &lt;- coeftest(fit_new, vcov = vcovHC(fit_new)) |&gt;\n  tidy() |&gt; _$statistic\n\ndata_table &lt;- as.data.frame(Confint(fit_new, vcov = vcovHC(fit_new))) |&gt;\n  cbind(t.stats) |&gt;\n  cbind(p.values) |&gt;\n  rownames_to_column(var = \"Characteristic\") |&gt;\n  rename(`p-value` = p.values,\n         `t-stat` = t.stats)\n\nStandard errors computed by vcovHC(fit_new) \n\ngt(data_table) |&gt;\n  fmt_number(decimals = 3)\n\n\n\n\n\n\n\nCharacteristic\nEstimate\n2.5 %\n97.5 %\nt-stat\np-value\n\n\n\n\n(Intercept)\n48.607\n−15.217\n112.431\n1.512\n0.134\n\n\ndistance\n−14.779\n−23.768\n−5.790\n−3.264\n0.002\n\n\nns(resid_walkscore, knots = c(0.5))1\n93.909\n−17.234\n205.053\n1.677\n0.097\n\n\nns(resid_walkscore, knots = c(0.5))2\n−18.348\n−85.986\n49.290\n−0.538\n0.592\n\n\nsquarefeet\n145.196\n122.380\n168.012\n12.632\n0.000\n\n\nbedgroup3 beds\n7.596\n−9.282\n24.474\n0.893\n0.374\n\n\nbedgroup4+ beds\n−28.638\n−55.632\n−1.644\n−2.106\n0.038\n\n\nzip1062\n−30.041\n−50.154\n−9.928\n−2.965\n0.004\n\n\n\n\n\n\n\nOf course, a fourth option is to manually perform the t test, which might make sense if we have a complex null or would like to do a one-sided test. Below is the general code for this:\n\nbeta_hats &lt;- coef(gentoo_fit)\na &lt;- c(0, 0, 0, 1) # beta_3 = 0\n\nt_stat &lt;- sum(a * beta_hats) /\n  sqrt(a %*% vcov(gentoo_fit) %*% a)[1,1]\n\np_value &lt;- pt(t_stat, gentoo_fit$df.residual,\n              lower.tail = FALSE)\n\np_value * 2 # if two-sided\n\n\n\n\nThe anova() function can be used to conduct F tests for nested models. For instance, we can test whether there was a significant association between residual walk score and estimated home price in 2007 (in practice, probably not a good idea since residual walk score is random):\n\n\nShow the code\n# create a model without the predictor\nfit_no_resid_ws &lt;- lm(price2007 ~ distance + \n                squarefeet + bedgroup + zip, data = rail_trail_modeling)\n\nanova(fit_no_resid_ws, fit_new)\n\n\nAnalysis of Variance Table\n\nModel 1: price2007 ~ distance + squarefeet + bedgroup + zip\nModel 2: price2007 ~ distance + ns(resid_walkscore, knots = c(0.5)) + \n    squarefeet + bedgroup + zip\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)  \n1     98 170673                             \n2     96 160689  2    9983.3 2.9821 0.0554 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe parameters of an F-statistic are q and n - p. Df is the outputted table corresponds to q and the second row of Res.Df corresponds to n - p. E.g, we have F(2, 96) as our null distribution in this example.\n\n\n\n\nIn a report, we just include full interpretations for the hypothesis tests and/or coefficient estimates that relevant to the practical problem at hand!\n\n\nShow the code\nlibrary(palmerpenguins)\n\npenguin_fit &lt;- lm(\n  bill_length_mm ~ flipper_length_mm + species +\n    flipper_length_mm:species,\n  data = penguins\n)\ntbl_regression(penguin_fit)\n\n\n\n\nTable 1: ADD\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nBeta\n95% CI\np-value\n\n\n\n\nflipper_length_mm\n0.13\n0.07, 0.20\n&lt;0.001\n\n\nspecies\n\n\n\n\n\n\n\n\n    Adelie\n—\n—\n\n\n\n\n    Chinstrap\n-8.0\n-29, 13\n0.4\n\n\n    Gentoo\n-34\n-54, -15\n&lt;0.001\n\n\nflipper_length_mm * species\n\n\n\n\n\n\n\n\n    flipper_length_mm * Chinstrap\n0.09\n-0.02, 0.19\n0.10\n\n\n    flipper_length_mm * Gentoo\n0.18\n0.09, 0.28\n&lt;0.001\n\n\n\nAbbreviation: CI = Confidence Interval\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe intercept \\beta_0 is the mean value of when all other regressors are 0:\n\\mathbb{E}[Y \\mid X_1 = 0, \\dots, X_q = 0] Interpreting the intercept, quoting confidence intervals, and conducting hypothesis tests for its value may only be useful when the intercept has a substantive meaning and we have observed X values nearby.\n\n\n\nNote these are the general parts of an interpretation for a continuous predictor in a linear regression:\n\nDifference in the mean value of Y\nAssociated with a one-unit increase in X_1\nHolding all other regressors constant\n\nExact interpretation depends on if the predictor is continuous or a factor level:\n\nTest for a factor coefficient: For a given flipper length, gentoo penguins have a smaller mean bill length (M = 47.5 mm, SD = 3.1) than Adelie penguins (M = 38.8 mm, SD = 2.7), t(336) = -3.5, p = 0.001.\n\nIn part calculated using group_by():\n\n\nShow the code\npenguins |&gt;\n  group_by(species)|&gt;\n  summarize(mean_bill_length =\n              mean(bill_length_mm,\n                 na.rm = TRUE)) |&gt;\n  gt() |&gt;\n  fmt_number(decimals = 1)\n\n\n\n\n\n\n\n\nspecies\nmean_bill_length\n\n\n\n\nAdelie\n38.8\n\n\nChinstrap\n48.8\n\n\nGentoo\n47.5\n\n\n\n\n\n\n\n\nConfidence interval for a factor coefficient: For a given flipper length, gentoo penguin bills are shorter than Adelie penguin bills by an average of 34.3 mm (95% CI [15.01, 15.64]).\nTest for slope: In Adelie penguins, there was a statistically significant association between flipper length and bill length, \\hat \\beta = 0.13, t(336) = 4.17, p &lt; 0.001.\nDescribe a slope: Among Adelie penguins, each additional millimeter of flipper length is associated with 0.13 mm of additional bill length, on average (95% CI [0.07, 0.2]).\n\n\n\n\nDescribing an interaction coefficient: For the interaction term between flipper length and species from Table 1: For Adelie penguins (the baseline level), the association between bill length and flipper length is 0.13 mm of bill length per millimeter of flipper length, on average (95% CI [0.07, 0.20]). But for Chinstrap penguins, the association is 0.13 + 0.09 = 0.22 mm of bill length per millimeter of flipper length, on average (95% CI [0.11, 0.31]). [LOOK AT THEOREM 5.5 AND HW 3 TO CONFIRM]\nA helpful way to visualize interaction terms, polynomial terms, (or other complex relationships) is to use an effects plot:\n\npredict_response(penguin_fit,\n                 terms = c(\"flipper_length_mm\", \"species\")) |&gt;\n  plot() +\n  labs(x = \"Flipper length (mm)\", y = \"Bill length (mm)\",\n       color = \"Species\",\n       title = \"Flipper length effect plot\")\n\n\n\n\n\n\n\n\nIn an effects plot, non-focal predictors are set to their mean (numeric variables), reference level (factors), or “most common” value (mode) in case of character vectors.\n\n\n\nIf we fit a spline or a polynomial for our predictor-of-interest, we cannot determine the statistical significance or effect size from the outputted regression tables. Instead, we must use the following framework:\n\nDo a LRT/F-test for nested models to obtain the overall significance of the predictor-of-interest.\n\n\nanova(fit_new, fit_no_resid_ws)\n\nAnalysis of Variance Table\n\nModel 1: price2007 ~ distance + ns(resid_walkscore, knots = c(0.5)) + \n    squarefeet + bedgroup + zip\nModel 2: price2007 ~ distance + squarefeet + bedgroup + zip\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)  \n1     96 160689                             \n2     98 170673 -2   -9983.3 2.9821 0.0554 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nCreate an effects plot fort the variable of interest\n\n\npredict_response(fit_new,\n                 terms = c(\"resid_walkscore\")) |&gt;\n  plot()\n\n\n\n\n\n\n\n\n\nGive an example of an interpretation using the effects plot with specific values\n\n\npreds &lt;- predict_response(fit_new, terms = c(\"resid_walkscore\"))\nhead(preds)\n\n# Predicted values of price2007\n\nresid_walkscore | Predicted |         95% CI\n--------------------------------------------\n         -43.01 |    259.57 | 213.44, 305.71\n         -36.08 |    271.78 | 233.60, 309.96\n         -31.03 |    280.37 | 247.16, 313.58\n         -29.04 |    283.65 | 252.14, 315.16\n         -25.22 |    289.68 | 260.94, 318.43\n         -25.21 |    289.70 | 260.96, 318.44\n\nAdjusted for:\n*   distance =     1.11\n* squarefeet =     1.57\n*   bedgroup = 1-2 beds\n*        zip =     1060\n\n\nFor example, when the residual walk score is -43.01, the predicted house price (in 2007) is $259,570 in Northampton, MA (95% CI [\\$213,440, \\$305,710]) versus when the residual walk score is -36.08, the predicted house price (in 2007) is $271,780 in Northampton, MA (95% CI [\\$233,600, \\$309,960]) – holding all other covariates at their mean or reference level."
  },
  {
    "objectID": "model-types-2.html#assumptions",
    "href": "model-types-2.html#assumptions",
    "title": "Model Types and Diagnostics",
    "section": "",
    "text": "For the linear regression model \\mathbf{Y} = \\mathbf{X}\\beta + e, we have the following assumptions:\n\n\nErrors have mean 0: \\mathbb{E}[Y \\mid X] = 0\n\nThe error variance is constant: \\text{Var}[e \\mid \\mathbf{X}] = \\sigma^2\n\nThe errors are uncorrelated (the data points are iid): \\text{Var}[e \\mid \\mathbf{X}] = \\sigma^2\n\nThe errors are normally distributed"
  },
  {
    "objectID": "model-types-2.html#diagnostics-and-solutions",
    "href": "model-types-2.html#diagnostics-and-solutions",
    "title": "Model Types and Diagnostics",
    "section": "",
    "text": "Linearity: partial residual plots help us diagnose whether the true relationship between the outcome and some of the predictors is non-linear\n\n\nShow the code\nfit &lt;- lm(price2007 ~ distance + resid_walkscore + squarefeet + bedgroup + \n            zip, data = rail_trail_modeling)\n\nfit_new &lt;- lm(price2007 ~ distance + ns(resid_walkscore, knots = c(0.5)) + \n                squarefeet + bedgroup + zip, data = rail_trail_modeling)\n\np1 &lt;- partial_residuals(fit) |&gt;\n  mutate(.predictor_name = case_when(\n    .predictor_name == \"distance\" ~ \"Distance to Rail Trail\",\n    .predictor_name == \"walkscore\" ~ \"Walking Score\",\n    .predictor_name == \"bikescore\" ~ \"Biking Score\",\n    .predictor_name == \"squarefeet\" ~ \"Square Footage\",\n    .predictor_name == \"resid_walkscore\" ~ \"Residual Walk Score\",\n    TRUE ~ .predictor_name)) |&gt;\n  ggplot(aes(x = .predictor_value, y = .partial_resid)) +\n  geom_point(color = \"grey40\", size = 0.5) +\n  geom_smooth(color = \"blue\", fill = 'skyblue2',\n              linewidth = 0.7, linetype = \"twodash\") +\n  geom_line(aes(y = .predictor_effect), color = 'red') +\n  facet_wrap(vars(.predictor_name), scales = \"free\", ncol = 2) +\n  labs(x = \"Predictor value\", y = \"Partial residual\")+\n  ggtitle(\"(a) Partial Residual Plots,\\nPreliminary Linear Model\")+\n  theme_bw()+\n  theme(strip.background = element_rect(fill = \"white\", color = \"white\"),\n        strip.text = element_text(color = \"black\", face = \"bold\"),\n        axis.text = element_text(color = \"black\"),\n        plot.title = element_text(face = \"bold\", size = 11))\n\np2 &lt;- partial_residuals(fit_new) |&gt;\n  mutate(.predictor_name = case_when(\n    .predictor_name == \"distance\" ~ \"Distance to Rail Trail\",\n    .predictor_name == \"walkscore\" ~ \"Walking Score\",\n    .predictor_name == \"bikescore\" ~ \"Biking Score\",\n    .predictor_name == \"squarefeet\" ~ \"Square Footage\",\n    .predictor_name == \"resid_walkscore\" ~ \"Residual Walk Score\",\n    TRUE ~ .predictor_name)) |&gt;\n  ggplot(aes(x = .predictor_value, y = .partial_resid)) +\n  geom_point(color = \"grey40\", size = 0.5) +\n  geom_smooth(color = \"blue\", fill = 'skyblue2',\n              linewidth = 0.7, linetype = \"twodash\") +\n  geom_line(aes(y = .predictor_effect), color = 'red') +\n  facet_wrap(vars(.predictor_name), scales = \"free\", ncol = 2) +\n  labs(x = \"Predictor value\", y = \"Partial residual\")+\n  ggtitle(\"(b) Partial Residual Plots, Adding\\nNatural Splines (Knot at Median)\\nto Residual Walk Score\")+\n  theme_bw()+\n  theme(strip.background = element_rect(fill = \"white\", color = \"white\"),\n        strip.text = element_text(color = \"black\", face = \"bold\"),\n        axis.text = element_text(color = \"black\"),\n        plot.title = element_text(face = \"bold\", size = 11))\n\np1 + p2\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrom Figure 1(a), we see that residual walk score has a nonlinear relationship with estimated house price in 2007 – as indicated by the smoothed residual curve (in blue) diverging from the predicted effect line (in red). We would address this nonlinearity by fitting natural splines on the residual walk score, with one knot at the median. Figure 1(b) provides the updated partial residual plots when we add natural splines to the residual walk score. We see now that the predicted effect line (in red) tracks closely to the smoothed residual curve (in blue) without over-fitting to model to the turns in the smoothed residual curve.\nLineups: If we aren’t sure whether the true relationship between the outcome and a given predictor is non-linear, we can conduct a partial residual visual lineup.\nFor instance, it is not entirely clear whether distance has a non-linear relationship to estimated house price in 2007 or if this is just due to the high-leverage home with the largest distance to the rail trail. Figure 2 shows the partial residual line-up. If we cannot pick out the partial residual plot corresponding to the real data from the simulated data, we can conclude that there is insufficient evidence of a non-linear relationship between distance to rail trail and estimated house price in 2007 at a significance level of 0.05.\n\n\nShow the code\nset.seed(123)\nmodel_lineup(fit_new, fn = partial_residuals, nsim = 20) |&gt;\n  filter(.predictor_name == \"distance\") |&gt;\n  mutate(.sample_new = paste0(\"Position \", .sample),\n         .sample_new = fct_reorder(.sample_new, .sample)) |&gt;\n  ggplot(aes(x = .predictor_value, y = .partial_resid))+\n  geom_point(color = \"grey40\", size = 0.5) +\n  geom_line(aes(y = .predictor_effect), color = 'red') +\n  geom_smooth(color = \"blue\", fill = 'skyblue2',\n              linewidth = 0.7, linetype = \"twodash\") +\n  facet_wrap(~.sample_new)+\n  labs(x = \"Distance to Rail Trail (in Miles)\", y = \"Partial residual\")+\n  theme_bw()+\n  theme(strip.background = element_rect(fill = \"white\", color = \"white\"),\n        strip.text = element_text(color = \"black\", face = \"bold\"),\n        axis.text = element_text(color = \"black\"),\n        plot.title = element_text(face = \"bold\", size = 11))\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can decrypt (show the the position of our actual data) by running the command, which is given at the end of the line-up call (dycrypt()):\n\ndecrypt(\"QUg2 qFyF Rx 8tLRyRtx ZP\")\n\n[1] \"True data in position  16\"\n\n\n\n\n\nAdd polynomial terms to the model\n\nSimplest way to add polynomial terms is with I():\n\nfit_new &lt;- lm(price2007 ~ distance + resid_walkscore + I(resid_walkscore^2) +\n                squarefeet + bedgroup + zip, data = rail_trail_modeling)\n\nIf there is numerical issues (e.g., from the predictor having large values), we can use the poly()function which constructs the design matric using an orthogonal polynomial basis.\nA downside is that the coefficients are no longer is terms of the interpretable regressor terms. The fitted values though wil be the same between the two approaches for adding polynomial terms.\n\nfit_new &lt;- lm(price2007 ~ distance + poly(resid_walkscore, degree = 2) + squarefeet + \n                bedgroup + zip, data = rail_trail_modeling)\n\n\nAdd regression splines to the model\n\nRegression splines model relationships as being piecewise polynomial. They require us to choose knots, which are the fixed points between which the function is polynomial. Two common approaches are natural splines and B-spline basis. Statisticians almost universally use cubic splines. Cubic splines are continuous in their first and second derivatives, making the knots almost visually imperceptible in plots of the spline.\nFor knot selection, a typical approach is to use quantiles of the data as knots – erring on the side of less knots (especially if there is few observations).\n\nfit_new &lt;- lm(price2007 ~ distance + ns(resid_walkscore, knots = c(0.5)) + \n                squarefeet + bedgroup + zip, data = rail_trail_modeling)\n\n\n\n\n\nErrors with non-constant variance are called heteroskedastic. To detect heteroskedasticity, we can examine the residuals plotted against the fitted values.\n\n\nShow the code\nfit_new |&gt;\n  augment() |&gt;\n  ggplot(aes(x = .fitted, y = .resid))+\n  geom_point(size = 0.5)+\n  geom_hline(yintercept = 0)+\n  geom_smooth(fill = 'skyblue2')+\n  labs(x = \"Fitted Values\", y = \"Residuals\")+\n  theme_bw()+\n  theme(axis.text = element_text(color = \"black\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn Figure 3, we have non-constant error variance. Indeed, there appears to be an approximately quadratic relationship between our fitted values and our residual terms.\nWe can remedy non-constant error variance by using the sandwich estimator for the variance of our estimators in all inference.\n\n# uses sandwich estimator with HC3 estimator for Omega\nConfint(fit_new, vcov = vcovHC(fit_new))\n\nStandard errors computed by vcovHC(fit_new) \n\n\n                                       Estimate      2.5 %     97.5 %\n(Intercept)                           48.607106 -15.216646 112.430857\ndistance                             -14.779103 -23.768308  -5.789898\nns(resid_walkscore, knots = c(0.5))1  93.909271 -17.234103 205.052646\nns(resid_walkscore, knots = c(0.5))2 -18.347598 -85.985661  49.290465\nsquarefeet                           145.196077 122.379951 168.012202\nbedgroup3 beds                         7.595663  -9.282322  24.473649\nbedgroup4+ beds                      -28.637850 -55.631857  -1.643844\nzip1062                              -30.040888 -50.154050  -9.927726\n\n\n\n\n\nWe can check for non-normality of errors using residual Q-Q plots. Using the standardized residuals, we can detect gross deviations from normality while ignoring small deviations in non-normality. An example of a Q-Q plot for linear regression is in Figure 4. Since we are doing linear regression, the theoretical quantiles correspond to a normal distribution with mean 0 and variance 1.\n\n\nShow the code\naugment(fit_new) |&gt;\n  ggplot(aes(sample = .std.resid)) +\n  geom_qq_line(linewidth = 0.4) +\n  geom_qq(size = 0.75)+\n  labs(x = 'Theoretical Quantiles',\n       y = 'Observed Quantiles')+\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\n\n\n\nLike with partial residuals, we can use a lineup to detect whether the non-normality in errors is statistically significant (at a significance level of 0.05).\n\n\nShow the code\nset.seed(123)\nmodel_lineup(fit_new) |&gt; \n  mutate(.sample_new = paste0(\"Position \", .sample),\n         .sample_new = fct_reorder(.sample_new, .sample)) |&gt; \n  ggplot(aes(sample = .std.resid)) +\n  geom_qq_line(linewidth = 0.4) +\n  geom_qq(size = 0.5) +\n  facet_wrap(vars(.sample_new)) +\n  labs(x = \"Theoretical Quantiles\",\n       y = \"Observed Quantiles\")+\n  theme_bw()+\n  theme(strip.text = element_text(face = \"bold\"),\n        strip.background = element_rect(fill = \"white\", color = \"white\"),\n        text = element_text(color = \"black\"),\n        axis.text = element_text(color = \"black\"))\n\n\n\n\n\n\n\n\n\n\n\n[1] \"True data in position  16\"\n\n\nNote: if the sample size is sufficiently large, then violating non-normality of errors is not a serious concern. Otherwise, if the assumptions linearity and constant error variance hold, then interpret coefficient estimates with caution (since coefficient estimates would not be approximately normal)."
  },
  {
    "objectID": "model-types-2.html#contaminated-errors-and-outliers",
    "href": "model-types-2.html#contaminated-errors-and-outliers",
    "title": "Model Types and Diagnostics",
    "section": "",
    "text": "Outliers: Not all outliers have a major effect on the regression line. So, their presence may not be a problem. We hence need a way to detect outliers and characterize their influence on the regression line.\n\nIf we can determine the outlier is due to some kind of measurement or recording error, we can correct the error.\nIf we cannot, we must make the difficult decision of whether to keep the outlier, and acknowledge that it may significantly influence our estimates, or to remove it and risk throwing away good information\n\nOne way to measure the influence of an observation on the regression is to quantify how its inclusion changes our coefficient estimates via Cook’s Distance.\n\nLook for Cook’s Distance (D_i), where D_i \\geq 1, though this is again a matter of judgment, to indicate that a particular observation substantially changes the regression fit.\n\nFigure 5 shows that all observations have Cook’s Distances well below the standard cut-off of 1, so there does not appear that any of the homes in the data have a large influence on our regression model.\n\n\nShow the code\nmax_price &lt;- augment(fit_new) |&gt;\n  arrange(desc(price2007)) |&gt;\n  slice_head(n = 1)\n\naugment(fit_new) |&gt;\n  ggplot(aes(y = .cooksd, x = price2007))+\n  geom_point(size = 0.7, alpha = 0.7)+\n  geom_point(data = max_price, aes(y = .cooksd, x = price2007),\n             size = 2, color = \"red\")+\n  labs(y = \"Cook's Distance\", x = \"Estimated House Price (2007)\")+\n  #geom_hline(yintercept = 1, linetype = \"dashed\")+\n  theme_bw()+\n  theme(plot.title = element_blank(),\n        text = element_text(color = \"black\"),\n        axis.text = element_text(color = \"black\"),\n        axis.title = element_text(color = \"black\"))"
  },
  {
    "objectID": "model-types-2.html#collinearity",
    "href": "model-types-2.html#collinearity",
    "title": "Model Types and Diagnostics",
    "section": "",
    "text": "When the model matrix X is not full column rank, its columns span a lower-dimensional space. There is still a unique perpendicular projection of Y onto this space, and hence a unique \\hat Y. But the coordinates of that fit are not unique, meaning that there are infinite values of \\hat \\beta that correspond to the same squared error/prediction. If we care about inference on \\hat \\beta this is a serious problem.\n\n\nShow the code\nloess_wrapper &lt;- function(data, mapping, ...){\n      p &lt;- ggplot(data = data, mapping = mapping) + \n      geom_point(size = 0.35, color = \"skyblue2\") + \n      geom_smooth(linetype = \"dashed\", color = \"black\", linewidth = 0.6)\n      return(p)\n}\n\nrail_trail_modeling |&gt;\n  select(price2007, distance, walkscore, squarefeet) |&gt;\n  rename(`2007 Est.\\nHouse Price\\n(thousands of $)` = price2007,\n         `Distance to\\nRail Trail (miles)` = distance,\n         `Walk\\nScore` = walkscore,\n         `Square\\nFootage` = squarefeet) |&gt;\n  ggpairs(upper = list(continuous = wrap(\"cor\", color = \"black\")),\n          lower = list(continuous = loess_wrapper,\n                       combo = wrap(\"barDiag\", size=0.2,\n                                     alpha = 0.9))) +\n  scale_x_continuous(breaks = scales::pretty_breaks(n = 4))+\n  scale_y_continuous(breaks = scales::pretty_breaks(n = 4))+\n  theme_bw()+\n  theme(strip.background = element_rect(fill = \"white\", color = \"white\"),\n        strip.text = element_text(color = \"black\", face = \"bold\", size = 6.5),\n        axis.text = element_text(color = \"black\", size = 6.5))\n\n\n\n\n\n\n\n\n\n\n\nCondition Number:\nHowever, multiple variables can be collinear even when individual pairs of variables have low correlation. One way to detect non-pairwise collinearity is to use the condition number, where we compare the largest and smallest eigenvalues of X^\\intercal X:\n\nX &lt;- model.matrix(price2007 ~ distance + resid_walkscore + \n                squarefeet + bedgroup + zip - 1, data = rail_trail_modeling)\n# first step is to scale the model matrix X\nscaled_x &lt;- scale(X, center = FALSE, scale = TRUE)\n  \n# Then, we can use the kappa() function on X'X\nkappa(t(scaled_x) %*% scaled_x)\n\n[1] 123.3692\n\n\nIdeally, we want the condition number to be small. The general cut-off is condition numbers larger than 50 or 100. So, from the results above, we see that there is potentially concerning levels of collinearity among our predictors, which could influence the accuracy of our inference.\nVariance Inflation Factors:\nAnother way to look at collinearity is with variance inflation factors (VIF), which can be interpreted as showing how much the variance of \\hat \\beta_j is inflated relative to a model where there is no collinearity (and all columns are orthogonal).\n\n# consider using type = 'predictor' for interactions (or ignoring VIFs since compared to main effecs)\ncar::vif(lm(price2007 ~ distance + ns(resid_walkscore, knots = c(0.5)) + \n                squarefeet + bedgroup + zip, data = rail_trail_modeling))\n\n                                        GVIF Df GVIF^(1/(2*Df))\ndistance                            1.379898  1        1.174691\nns(resid_walkscore, knots = c(0.5)) 1.409755  2        1.089647\nsquarefeet                          1.951119  1        1.396825\nbedgroup                            1.725889  2        1.146181\nzip                                 1.342293  1        1.158574\n\n\nA general rule of thumb is that VIF values exceeding 5 warrant further investigation, and VIFs exceeding 10 are signs of serious multicollinearity requiring correction.\nGeneral Notes: If we are confronted with collinearity, then, the question to ask is:\n\n“Have we chosen the correct predictors for the research question?”\n\nIf we have, there is little to be done; if we have not, we can reconsider our choice of predictors and perhaps eliminate the collinear ones.\nIf we are interested in prediction rather than in the coefficients, collinearity is a problem insofar as it creates high prediction variance, and we might reconsider our model and use a penalization model to reduce the prediction variance.s"
  },
  {
    "objectID": "model-types-2.html#interactions",
    "href": "model-types-2.html#interactions",
    "title": "Model Types and Diagnostics",
    "section": "",
    "text": "An interaction allows one predictor’s association with the outcome to depend on values of another predictor.\nWhen an interaction is present, the normal interpretation of coefficients as slopes no longer holds for the predictors involved in the interaction.\nSee Example 7.4"
  },
  {
    "objectID": "model-types-2.html#hypothesis-tests",
    "href": "model-types-2.html#hypothesis-tests",
    "title": "Model Types and Diagnostics",
    "section": "",
    "text": "Many useful null hypotheses can be written in terms of linear combinations of the coefficients. For example, consider a linear model with a continuous predictor X_1 and dummy-coded regressor X_2 \\in \\{0,1 \\}:\nY = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_1 X_2 We could think of several null hypotheses to test:\n\n\\beta_2 = 0: the two factor levels have identical intercepts\n\\beta_3 = 0: the two factor levels have identical slopes\n\\beta_1 = c: the slope when X_2 = 0 is some value c predicted by a theory we are testing (usually c = 0 in linear regression, which corresponds to no association)\n\\beta_1 + \\beta_3 = 0: when X_2 = 1, there is no association between X_1 and Y\n\\beta_2 = \\beta_3 = 0: the two factor levels have identical relationships between X_1 and Y\n\nFor all but the last listed hypothesis, we can use a t test. R conducts a t test for every coefficient by default, with c = 0.\nThe degrees of freedom for a t test is n - p, where p is the number of parameters in our model. We can get this via fit$df.residual. E.g., for our rail trail linear model example we have df = 96.\nThe function tidy() does the test for c = 0 with every other predictor held constant in the model.\n\n\nShow the code\ngt(tidy(fit_new)) |&gt;\n  fmt_number(decimals = 3)\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n48.607\n27.295\n1.781\n0.078\n\n\ndistance\n−14.779\n5.039\n−2.933\n0.004\n\n\nns(resid_walkscore, knots = c(0.5))1\n93.909\n43.190\n2.174\n0.032\n\n\nns(resid_walkscore, knots = c(0.5))2\n−18.348\n22.890\n−0.802\n0.425\n\n\nsquarefeet\n145.196\n10.082\n14.401\n0.000\n\n\nbedgroup3 beds\n7.596\n12.269\n0.619\n0.537\n\n\nbedgroup4+ beds\n−28.638\n15.037\n−1.905\n0.060\n\n\nzip1062\n−30.041\n9.438\n−3.183\n0.002\n\n\n\n\n\n\n\nWe can also use tbl_regression() which gives us the confidence intervals and p-values:\n\n\nShow the code\ntbl_regression(fit_new)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nBeta\n95% CI\np-value\n\n\n\n\ndistance\n-15\n-25, -4.8\n0.004\n\n\nns(resid_walkscore, knots = c(0.5))\n\n\n\n\n\n\n\n\n    ns(resid_walkscore, knots = c(0.5))1\n94\n8.2, 180\n0.032\n\n\n    ns(resid_walkscore, knots = c(0.5))2\n-18\n-64, 27\n0.4\n\n\nsquarefeet\n145\n125, 165\n&lt;0.001\n\n\nbedgroup\n\n\n\n\n\n\n\n\n    1-2 beds\n—\n—\n\n\n\n\n    3 beds\n7.6\n-17, 32\n0.5\n\n\n    4+ beds\n-29\n-58, 1.2\n0.060\n\n\nzip\n\n\n\n\n\n\n\n\n    1060\n—\n—\n\n\n\n\n    1062\n-30\n-49, -11\n0.002\n\n\n\nAbbreviation: CI = Confidence Interval\n\n\n\n\n\n\n\n\nIf we have non-constant error variance and thus use the sandwich estimator, we can use the following code to conduct t tests and present our results:\n\np.values &lt;- coeftest(fit_new, vcov = vcovHC(fit_new)) |&gt;\n  tidy() |&gt; _$p.value\n\nt.stats &lt;- coeftest(fit_new, vcov = vcovHC(fit_new)) |&gt;\n  tidy() |&gt; _$statistic\n\ndata_table &lt;- as.data.frame(Confint(fit_new, vcov = vcovHC(fit_new))) |&gt;\n  cbind(t.stats) |&gt;\n  cbind(p.values) |&gt;\n  rownames_to_column(var = \"Characteristic\") |&gt;\n  rename(`p-value` = p.values,\n         `t-stat` = t.stats)\n\nStandard errors computed by vcovHC(fit_new) \n\ngt(data_table) |&gt;\n  fmt_number(decimals = 3)\n\n\n\n\n\n\n\nCharacteristic\nEstimate\n2.5 %\n97.5 %\nt-stat\np-value\n\n\n\n\n(Intercept)\n48.607\n−15.217\n112.431\n1.512\n0.134\n\n\ndistance\n−14.779\n−23.768\n−5.790\n−3.264\n0.002\n\n\nns(resid_walkscore, knots = c(0.5))1\n93.909\n−17.234\n205.053\n1.677\n0.097\n\n\nns(resid_walkscore, knots = c(0.5))2\n−18.348\n−85.986\n49.290\n−0.538\n0.592\n\n\nsquarefeet\n145.196\n122.380\n168.012\n12.632\n0.000\n\n\nbedgroup3 beds\n7.596\n−9.282\n24.474\n0.893\n0.374\n\n\nbedgroup4+ beds\n−28.638\n−55.632\n−1.644\n−2.106\n0.038\n\n\nzip1062\n−30.041\n−50.154\n−9.928\n−2.965\n0.004\n\n\n\n\n\n\n\nOf course, a fourth option is to manually perform the t test, which might make sense if we have a complex null or would like to do a one-sided test. Below is the general code for this:\n\nbeta_hats &lt;- coef(gentoo_fit)\na &lt;- c(0, 0, 0, 1) # beta_3 = 0\n\nt_stat &lt;- sum(a * beta_hats) /\n  sqrt(a %*% vcov(gentoo_fit) %*% a)[1,1]\n\np_value &lt;- pt(t_stat, gentoo_fit$df.residual,\n              lower.tail = FALSE)\n\np_value * 2 # if two-sided\n\n\n\n\nThe anova() function can be used to conduct F tests for nested models. For instance, we can test whether there was a significant association between residual walk score and estimated home price in 2007 (in practice, probably not a good idea since residual walk score is random):\n\n\nShow the code\n# create a model without the predictor\nfit_no_resid_ws &lt;- lm(price2007 ~ distance + \n                squarefeet + bedgroup + zip, data = rail_trail_modeling)\n\nanova(fit_no_resid_ws, fit_new)\n\n\nAnalysis of Variance Table\n\nModel 1: price2007 ~ distance + squarefeet + bedgroup + zip\nModel 2: price2007 ~ distance + ns(resid_walkscore, knots = c(0.5)) + \n    squarefeet + bedgroup + zip\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)  \n1     98 170673                             \n2     96 160689  2    9983.3 2.9821 0.0554 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe parameters of an F-statistic are q and n - p. Df is the outputted table corresponds to q and the second row of Res.Df corresponds to n - p. E.g, we have F(2, 96) as our null distribution in this example."
  },
  {
    "objectID": "model-types-2.html#coefficient-interpretation",
    "href": "model-types-2.html#coefficient-interpretation",
    "title": "Model Types and Diagnostics",
    "section": "",
    "text": "In a report, we just include full interpretations for the hypothesis tests and/or coefficient estimates that relevant to the practical problem at hand!\n\n\nShow the code\nlibrary(palmerpenguins)\n\npenguin_fit &lt;- lm(\n  bill_length_mm ~ flipper_length_mm + species +\n    flipper_length_mm:species,\n  data = penguins\n)\ntbl_regression(penguin_fit)\n\n\n\n\nTable 1: ADD\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nBeta\n95% CI\np-value\n\n\n\n\nflipper_length_mm\n0.13\n0.07, 0.20\n&lt;0.001\n\n\nspecies\n\n\n\n\n\n\n\n\n    Adelie\n—\n—\n\n\n\n\n    Chinstrap\n-8.0\n-29, 13\n0.4\n\n\n    Gentoo\n-34\n-54, -15\n&lt;0.001\n\n\nflipper_length_mm * species\n\n\n\n\n\n\n\n\n    flipper_length_mm * Chinstrap\n0.09\n-0.02, 0.19\n0.10\n\n\n    flipper_length_mm * Gentoo\n0.18\n0.09, 0.28\n&lt;0.001\n\n\n\nAbbreviation: CI = Confidence Interval\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe intercept \\beta_0 is the mean value of when all other regressors are 0:\n\\mathbb{E}[Y \\mid X_1 = 0, \\dots, X_q = 0] Interpreting the intercept, quoting confidence intervals, and conducting hypothesis tests for its value may only be useful when the intercept has a substantive meaning and we have observed X values nearby.\n\n\n\nNote these are the general parts of an interpretation for a continuous predictor in a linear regression:\n\nDifference in the mean value of Y\nAssociated with a one-unit increase in X_1\nHolding all other regressors constant\n\nExact interpretation depends on if the predictor is continuous or a factor level:\n\nTest for a factor coefficient: For a given flipper length, gentoo penguins have a smaller mean bill length (M = 47.5 mm, SD = 3.1) than Adelie penguins (M = 38.8 mm, SD = 2.7), t(336) = -3.5, p = 0.001.\n\nIn part calculated using group_by():\n\n\nShow the code\npenguins |&gt;\n  group_by(species)|&gt;\n  summarize(mean_bill_length =\n              mean(bill_length_mm,\n                 na.rm = TRUE)) |&gt;\n  gt() |&gt;\n  fmt_number(decimals = 1)\n\n\n\n\n\n\n\n\nspecies\nmean_bill_length\n\n\n\n\nAdelie\n38.8\n\n\nChinstrap\n48.8\n\n\nGentoo\n47.5\n\n\n\n\n\n\n\n\nConfidence interval for a factor coefficient: For a given flipper length, gentoo penguin bills are shorter than Adelie penguin bills by an average of 34.3 mm (95% CI [15.01, 15.64]).\nTest for slope: In Adelie penguins, there was a statistically significant association between flipper length and bill length, \\hat \\beta = 0.13, t(336) = 4.17, p &lt; 0.001.\nDescribe a slope: Among Adelie penguins, each additional millimeter of flipper length is associated with 0.13 mm of additional bill length, on average (95% CI [0.07, 0.2]).\n\n\n\n\nDescribing an interaction coefficient: For the interaction term between flipper length and species from Table 1: For Adelie penguins (the baseline level), the association between bill length and flipper length is 0.13 mm of bill length per millimeter of flipper length, on average (95% CI [0.07, 0.20]). But for Chinstrap penguins, the association is 0.13 + 0.09 = 0.22 mm of bill length per millimeter of flipper length, on average (95% CI [0.11, 0.31]). [LOOK AT THEOREM 5.5 AND HW 3 TO CONFIRM]\nA helpful way to visualize interaction terms, polynomial terms, (or other complex relationships) is to use an effects plot:\n\npredict_response(penguin_fit,\n                 terms = c(\"flipper_length_mm\", \"species\")) |&gt;\n  plot() +\n  labs(x = \"Flipper length (mm)\", y = \"Bill length (mm)\",\n       color = \"Species\",\n       title = \"Flipper length effect plot\")\n\n\n\n\n\n\n\n\nIn an effects plot, non-focal predictors are set to their mean (numeric variables), reference level (factors), or “most common” value (mode) in case of character vectors.\n\n\n\nIf we fit a spline or a polynomial for our predictor-of-interest, we cannot determine the statistical significance or effect size from the outputted regression tables. Instead, we must use the following framework:\n\nDo a LRT/F-test for nested models to obtain the overall significance of the predictor-of-interest.\n\n\nanova(fit_new, fit_no_resid_ws)\n\nAnalysis of Variance Table\n\nModel 1: price2007 ~ distance + ns(resid_walkscore, knots = c(0.5)) + \n    squarefeet + bedgroup + zip\nModel 2: price2007 ~ distance + squarefeet + bedgroup + zip\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)  \n1     96 160689                             \n2     98 170673 -2   -9983.3 2.9821 0.0554 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nCreate an effects plot fort the variable of interest\n\n\npredict_response(fit_new,\n                 terms = c(\"resid_walkscore\")) |&gt;\n  plot()\n\n\n\n\n\n\n\n\n\nGive an example of an interpretation using the effects plot with specific values\n\n\npreds &lt;- predict_response(fit_new, terms = c(\"resid_walkscore\"))\nhead(preds)\n\n# Predicted values of price2007\n\nresid_walkscore | Predicted |         95% CI\n--------------------------------------------\n         -43.01 |    259.57 | 213.44, 305.71\n         -36.08 |    271.78 | 233.60, 309.96\n         -31.03 |    280.37 | 247.16, 313.58\n         -29.04 |    283.65 | 252.14, 315.16\n         -25.22 |    289.68 | 260.94, 318.43\n         -25.21 |    289.70 | 260.96, 318.44\n\nAdjusted for:\n*   distance =     1.11\n* squarefeet =     1.57\n*   bedgroup = 1-2 beds\n*        zip =     1060\n\n\nFor example, when the residual walk score is -43.01, the predicted house price (in 2007) is $259,570 in Northampton, MA (95% CI [\\$213,440, \\$305,710]) versus when the residual walk score is -36.08, the predicted house price (in 2007) is $271,780 in Northampton, MA (95% CI [\\$233,600, \\$309,960]) – holding all other covariates at their mean or reference level."
  },
  {
    "objectID": "model-types-2.html#assumptions-1",
    "href": "model-types-2.html#assumptions-1",
    "title": "Model Types and Diagnostics",
    "section": "2.1 Assumptions",
    "text": "2.1 Assumptions\nLogistic regression makes two basic assumptions about the population relationship:\n\nThe log-odds is linearly related to the regressors: \\log(\\text{odds}(Y = 1 \\mid X = x)) = \\beta^\\intercal x\nThe observation Y_i are conditionally independent given the covariates X_i"
  },
  {
    "objectID": "model-types-2.html#linearity-diagnostics",
    "href": "model-types-2.html#linearity-diagnostics",
    "title": "Model Types and Diagnostics",
    "section": "2.2 Linearity Diagnostics",
    "text": "2.2 Linearity Diagnostics\nEmpirical Link Plot (EDA):\nExample: Figure 6 shows that TWA core temperature during surgery appears to have an approximately linear relationship with the log-odds of superficial post-op infection but possibly a non-linear relationship with log-odds of a serious post-op infection.\n\n\nShow the code\ncore_temperature &lt;- read_csv(\"blogs/DA-materials/data/core-temperature.csv\") |&gt;\n  select(-SurgeryType) |&gt;\n  filter(DEAD == 0 | DurationHosp &gt;= 30)\n\np1 &lt;- core_temperature |&gt;\n  bin_by_quantile(TWATemp, breaks = 10) |&gt;\n  summarize(\n    mean_temp = mean(TWATemp),\n    prob = mean(SeriousInfection),\n    log_odds = empirical_link(\n      SeriousInfection,\n      family = binomial(link = \"logit\")),\n    `Patient Count` = n()\n  ) |&gt;\n  ggplot(aes(x = mean_temp, y = log_odds)) +\n  geom_point(aes(size = `Patient Count`))+\n  geom_smooth(fill = \"skyblue3\")+\n  scale_size_continuous(range = c(1,3))+\n  labs(title = \"(A) Serious Infection\",\n       y = \"Log-odds of Infection\",\n       x = \"TWA Core Temperature during\\nSurgery (in Celsius)\")+\n  theme_minimal()+\n  theme(legend.position = \"None\",\n        plot.title = element_text(face = \"bold\", size = 11, hjust = 0),\n        axis.text = element_text(color = \"black\"))\n\np2 &lt;- core_temperature |&gt;\n  bin_by_quantile(TWATemp, breaks = 10) |&gt;\n  summarize(\n    mean_temp = mean(TWATemp),\n    prob = mean(SuperficialInfection),\n    log_odds = empirical_link(\n      SuperficialInfection,\n      family = binomial(link = \"logit\")),\n    `Patient Count` = n()\n  ) |&gt;\n  ggplot(aes(x = mean_temp, y = log_odds)) +\n  geom_point(aes(size = `Patient Count`))+\n  geom_smooth(fill = \"skyblue3\")+\n  scale_size_continuous(range = c(1,3))+\n  labs(title = \"(B) Superficial Infection\",\n       y = \"Log-odds of Infection\",\n      x = \"TWA Core Temperature during\\nSurgery (in Celsius)\")+\n  theme_minimal()+\n  theme(plot.title = element_text(face = \"bold\", size = 11, hjust = 0),\n        axis.text = element_text(color = \"black\"))\n\np1 + p2\n\n\n\n\n\n\n\n\n\n\n\n\n\nPartial Residuals:\nAnother method is to use partial residuals, just like we did for linear regression.\nExample: Figure 7 shows that surgery duration and TWA core temperature both appear to have non-linear relationships with the log-odds of serious infection, which confirms what we see in the empirical link plot.\n\n\nShow the code\ncore_temperature_model &lt;- core_temperature |&gt;\n  filter(DEAD == 0 | DurationHosp &gt;= 30) |&gt;\n  mutate(WeightLoss = ifelse(WGHTLOSS == 1, \"Yes\", \"No\"),\n         SteroidUsage = ifelse(SteroidHx == 1, \"Yes\", \"No\"),\n         SurgeryDuration = SurgDuration)\n\nprelim_fit &lt;- glm(SeriousInfection ~ TWATemp + WeightLoss +\n                    SurgeryDuration + SteroidUsage,\n    data = core_temperature_model,\n    family = binomial()) \n\npartial_residuals(prelim_fit) |&gt;\n    mutate(.predictor_name = case_when(\n    .predictor_name == \"Age\" ~ \"Age (in Years)\",\n    .predictor_name == \"BMI\" ~ \"Body Mass Index (kg/m^2)\",\n    .predictor_name == \"SurgeryDuration\" ~ \"Surgery Duration (in Minutes)\",\n    .predictor_name == \"TWATemp\" ~ \"TWA Core Temperature (in Celsius)\",\n    TRUE ~ .predictor_name\n  )) |&gt;\n  ggplot((aes(x = .predictor_value, y = .partial_resid)))+\n  geom_point(size = 0.3, color = 'grey50')+\n  geom_line(aes(y = .predictor_effect), linewidth = 0.3)+\n  geom_smooth(method = \"loess\", linetype = \"twodash\", fill = \"skyblue3\")+\n  facet_wrap(~.predictor_name, scales = \"free\")+\n  labs(y = \"Partial Residual\",\n       x = \"Predictor Value\")+\n  theme_minimal()+\n  theme(strip.text = element_text(face = \"bold\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe address the non-linearity in Figure 7 by fitting natural splines to both surgical duration and TWA core temperature.\n\n\nShow the code\nupdated_fit &lt;- glm(SeriousInfection ~ splines::ns(TWATemp, knots = c(35.7, 36, 36.4)) + WeightLoss + ns(SurgeryDuration, knots = c(550)) + SteroidUsage,\n    data = core_temperature_model,\n    family = binomial()) \n\npartial_residuals(updated_fit) |&gt;\n  mutate(.predictor_name = case_when(\n    .predictor_name == \"Age\" ~ \"Age (in Years)\",\n    .predictor_name == \"BMI\" ~ \"Body Mass Index (kg/m^2)\",\n    .predictor_name == \"SurgeryDuration\" ~ \"Surgery Duration (in Minutes)\",\n    .predictor_name == \"TWATemp\" ~ \"TWA Core Temperature (in Celsius)\",\n    TRUE ~ .predictor_name\n  )) |&gt;\n  ggplot((aes(x = .predictor_value, y = .partial_resid)))+\n  geom_point(size = 0.3, color = 'grey50')+\n  geom_line(aes(y = .predictor_effect), linewidth = 0.3)+\n  geom_smooth(method = \"loess\", linetype = \"twodash\", fill = \"skyblue3\")+\n  facet_wrap(~.predictor_name, scales = \"free\")+\n  labs(y = \"Partial Residual\",\n       x = \"Predictor Value\")+\n  theme_minimal()+\n  theme(strip.text = element_text(face = \"bold\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\nRandomized Quantile Residuals:\nA third method is to use randomized quantile residuals. We can use plots of randomized quantile residuals against predictors and/or fitted values, as with any other residuals, to check the overall fit of our model.\n\nIf the model is correctly specified, we would want the conditional mean of the randomized quantile residuals to be approximately 0.5.\n\nIt is also useful to check that their distribution is indeed uniform (e.g. with a Q-Q plot). When the model is incorrectly specified, the distribution will not be uniform, producing patterns on the residual plots that can be interpreted.\nExample: Below is an example with Pima women data from the MASS package.\n\n\nShow the code\nlibrary(MASS)\nPima.tr$pregnancy &lt;- factor(\n  ifelse(Pima.tr$npreg &gt; 0, \"Yes\", \"No\")\n)\n\npima_fit &lt;- glm(type ~ pregnancy + bp, data = Pima.tr,\n                family = binomial())\n\npima_aug &lt;- augment_quantile(pima_fit)\n\np1 &lt;- pima_aug |&gt;\n  ggplot(aes(x = .fitted, y = .quantile.resid))+\n  geom_point()+\n  geom_hline(yintercept = 0.5)+\n  geom_smooth(fill = 'skyblue2')+\n  labs(x = 'Fitted Value', y = 'Randomized Quantile Residual')+\n  theme_bw()\n\np2 &lt;- pima_aug |&gt;\n  ggplot(aes(sample = .quantile.resid))+\n  geom_qq_line(linewidth = 0.4, distribution = stats::qunif) +\n  geom_qq(size = 0.75, distribution = stats::qunif)+\n  labs(x = 'Theoretical Quantiles',\n       y = 'Observed Quantiles')+\n  theme_bw()\n\np1 + p2"
  },
  {
    "objectID": "model-types-2.html#calibration-plots",
    "href": "model-types-2.html#calibration-plots",
    "title": "Model Types and Diagnostics",
    "section": "2.3 Calibration Plots",
    "text": "2.3 Calibration Plots\nRoughly speaking, a calibrated model is one whose predicted probabilities are accurate. For example, if the model predicts \\text{Pr}(Y = 1 \\mid X = x) = 0.8 for a particular x, and we observe many responses with that x, about 80\\% of those responses should be 1 and 20\\% should be 0.\n\n\nShow the code\ncalibration_data &lt;- data.frame(\n  x = predict(pima_fit, type = \"response\"),\n  y = ifelse(Pima.tr$type == \"Yes\", 1, 0)\n)\n\nggplot(calibration_data, aes(x = x, y = y)) +\n  geom_point() +\n  geom_smooth(se = FALSE) +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\") +\n  labs(x = \"Predicted probability\", y = \"Observed fraction\") +\n  ylim(0, 1)+\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe start by converting the response from a factor to 0 or 1, so the smoother can take the average. Figure 9 shows that the calibration looks good across most of the range in our data, but as the predicted probability gets over 0.6, we see odd behavior; examining the plotted points, it appears this is due to a few observations with high predicted probability but Y = 0. We should not get too worried over two or three observations, so this is not particularly concerning.\nBut perfect calibration does not make a good model. Calibration hence should be used together with other measures of the adequacy of the model fit, not on its own!"
  },
  {
    "objectID": "model-types-2.html#hypothesis-tests-1",
    "href": "model-types-2.html#hypothesis-tests-1",
    "title": "Model Types and Diagnostics",
    "section": "2.4 Hypothesis Tests",
    "text": "2.4 Hypothesis Tests\nWald tests are what tidy() defaults to for GLMs. Wald tests use a z test statistic and, by default tidy() tests \\beta = 0.\n\n2.4.1 Wald Tests\nFirst, we can find a confidence interval for a probability for a specific set of regressors:\n\n\nShow the code\nilogit &lt;- function(x) 1 / (1 + exp(-x))\n\nx &lt;- data.frame(bp = 80,\n                pregnancy = \"Yes\")\n\npred_lp &lt;- predict(pima_fit, newdata = x,\n                   type = \"link\", se.fit = TRUE)\n\nilogit(pred_lp$fit + (\n  qnorm(p = c(0.025, 0.975)) *\n    pred_lp$se.fit\n))\n\n\n[1] 0.3119114 0.4923628\n\n\n\n\n2.4.2 Confidence Intervals\nWe can also derive the confidence intervals for the coefficients in our model using Wald tests just using confint()\n\n\nShow the code\nexp(confint(pima_fit)) # exponentiate puts it of the odds scale\n\n\n                   2.5 %    97.5 %\n(Intercept)  0.004724633 0.3286171\npregnancyYes 0.271584007 1.4697789\nbp           1.013722770 1.0710464\n\n\nExample interpretation: A 95% confidence interval for the odds ratio associated with having a pregnancy is [0.272, 1.47]. Since the confidence interval overlaps with 1, we cannot conclusively say whether prior pregnancy is associated with an increase or decrease in odds.\n\n\n2.4.3 Deviance Tests\nDeviance tests are used for nested models and thus equivalent to an F test in linear regression. Specifically:\n\\text{Dev}_{\\text{reduced}} - \\text{Dev}_{\\text{full}} \\overset{\\text{d}}{\\rightarrow} \\chi^2_q, \\quad \\text{where } q \\text{ is the diff in the degrees of freedom}\nExample code:\n\n\nShow the code\npima_larger_fit &lt;- glm(type ~ pregnancy + bp + age + glu,\n                       data = Pima.tr, family = binomial())\n\nanova(pima_fit, pima_larger_fit, test = \"Chisq\") #chisq is same as LRT\n\n\nAnalysis of Deviance Table\n\nModel 1: type ~ pregnancy + bp\nModel 2: type ~ pregnancy + bp + age + glu\n  Resid. Df Resid. Dev Df Deviance  Pr(&gt;Chi)    \n1       197     246.37                          \n2       195     194.34  2   52.031 5.032e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nHere, q = 2. Since the p-value is significant we reject the null hypothesis that both age and glucose status have coefficients equal to one (meaning at least one is associated with a change in the odds of diabetes status), at a significance level of 0.05."
  },
  {
    "objectID": "model-types-2.html#coefficient-interpretation-1",
    "href": "model-types-2.html#coefficient-interpretation-1",
    "title": "Model Types and Diagnostics",
    "section": "2.5 Coefficient Interpretation",
    "text": "2.5 Coefficient Interpretation\n\n\nShow the code\ngt(tidy(pima_fit)) |&gt; \n  fmt_number(decimals = 3) |&gt;\n  cols_align_decimal(c(estimate, p.value))\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n−3.165\n1.077\n−2.939\n0.003\n\n\npregnancyYes\n−0.468\n0.427\n−1.097\n0.273\n\n\nbp\n 0.040\n0.014\n2.886\n0.004\n\n\n\n\n\n\n\nNote the above is on the log-odds scale\n\n\nShow the code\ntbl_regression(pima_fit, exponentiate = TRUE) |&gt;\n  as_gt() |&gt;\n  cols_align_decimal(c(estimate, p.value))\n\n\n\n\nTable 2: ADD\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nOR\n95% CI\np-value\n\n\n\n\npregnancy\n\n\n\n\n\n\n\n\n    No\n—\n—\n\n\n\n\n    Yes\n0.63\n0.27, 1.47\n0.3  \n\n\nbp\n1.04\n1.01, 1.07\n0.004\n\n\n\nAbbreviations: CI = Confidence Interval, OR = Odds Ratio\n\n\n\n\n\n\n\n\n\n\n\nExample interpretation: Table 2 gives the results of the logistic regression fit. Women with prior pregnancies were less likely to have diabetes (OR = 0.63, 95% CI [0.27, 1.5]), but this result was not statistically significant (z = -1.1, p = 0.27). A larger sample may be necessary to determine if a relationship exists in the population.\nAnother one: Each unit of increase in blood pressure (measured in mm Hg) is associated with an increase in the log-odds of diabetes of 0.04, or a multiplication in the odds of diabetes by 1.04 (95% CI [1.01, 1.07])."
  },
  {
    "objectID": "model-types-2.html#outliers",
    "href": "model-types-2.html#outliers",
    "title": "Model Types and Diagnostics",
    "section": "2.6 Outliers",
    "text": "2.6 Outliers\nWe can approximate Cook’s distance fairly well to estimate the influence of each observation on our coefficient estimates, using augment just like before.\nWe could also likely remove the outliers and see if it qualitatively changes our findings if we aren’t sure."
  },
  {
    "objectID": "model-types-2.html#assumptions-2",
    "href": "model-types-2.html#assumptions-2",
    "title": "Model Types and Diagnostics",
    "section": "3.1 Assumptions",
    "text": "3.1 Assumptions\nWhen we fit a generalized linear model (such as Binomial and Poisson models), we make three key assumptions:\n\nThe observations are conditionally independent given X\nThe response variable follows the chosen distribution\nThe mean of the response is related to the predictors through the chosen link function and functional form\n\nResidual diagnostics can be used to check the latter two assumptions."
  },
  {
    "objectID": "model-types-2.html#diagnostics",
    "href": "model-types-2.html#diagnostics",
    "title": "Model Types and Diagnostics",
    "section": "3.2 Diagnostics",
    "text": "3.2 Diagnostics\nWe can use similar diagnostics to what we used for logistic regression. A good precursor to these is an empirical link plot, which would be done in the EDA section.\nPartial Residuals:\n\n\nShow the code\nants &lt;- read.csv(\"blogs/DA-materials/data/ants.csv\")\n\nants_fit &lt;- glm(Srich ~ Latitude + Elevation + Habitat, data = ants,\n                family = poisson())\n\npartial_residuals(ants_fit) |&gt;\n  ggplot(aes(x = .predictor_value, y = .partial_resid)) +\n  geom_point(color = \"grey40\", size = 0.5) +\n  geom_smooth(color = \"blue\", fill = 'skyblue2',\n              linewidth = 0.7, linetype = \"twodash\") +\n  geom_line(aes(y = .predictor_effect), color = 'red') +\n  facet_wrap(vars(.predictor_name), scales = \"free\", ncol = 2) +\n  labs(x = \"Predictor value\", y = \"Partial residual\")+\n  theme_bw()+\n  theme(strip.background = element_rect(fill = \"white\", color = \"white\"),\n        strip.text = element_text(color = \"black\", face = \"bold\"),\n        axis.text = element_text(color = \"black\"),\n        plot.title = element_text(face = \"bold\", size = 11))\n\n\n\n\n\n\n\n\n\nRandomized Quantile Residuals:\n\n\nShow the code\nlibrary(agridat)\n\nseed_fit_1 &lt;- glm(cbind(germ, n - germ) ~\n                    extract + gen,\n                  data = crowder.seeds, family = binomial())\n\nseed_aug &lt;- augment_quantile(seed_fit_1)\n\np1 &lt;- seed_aug |&gt;\n  ggplot(aes(x = .fitted, y = .quantile.resid))+\n  geom_point()+\n  geom_hline(yintercept = 0.5)+\n  geom_smooth(fill = 'skyblue2')+\n  labs(x = 'Fitted Value', y = 'Randomized Quantile Residual')+\n  theme_bw()\n\np2 &lt;- seed_aug |&gt;\n  ggplot(aes(sample = .quantile.resid))+\n  geom_qq_line(linewidth = 0.4, distribution = stats::qunif) +\n  geom_qq(size = 0.75, distribution = stats::qunif)+\n  labs(x = 'Theoretical Quantiles',\n       y = 'Observed Quantiles')+\n  theme_bw()\n\np1 + p2"
  },
  {
    "objectID": "model-types-2.html#binomial-regresson",
    "href": "model-types-2.html#binomial-regresson",
    "title": "Model Types and Diagnostics",
    "section": "3.3 Binomial Regresson",
    "text": "3.3 Binomial Regresson\nThe binomial distribution is a common response distribution whenever outcomes are binary, or whenever we count a certain binary outcome out of a total number of trials.\nThe binomial distribution is suitable when there is a fixed and known total number of trials. Each observation hence consists of a number of successes and a total number of trials; the number of trials may differ between observations.\nThe model is (with a logit-link function, the default):\nn_iY_i \\mid X_i = x_i \\sim \\text{Binomial}(n_i, \\text{logit}(\\beta^\\intercal x_i)) Hence, the sample proportion Y_i should be proportional to \\text{logit}(\\beta^\\intercal x_i).\n\nY_i is the rate/probability of success!\n\nIn R, a binomial response variable for n &gt; 1 can be provided as a a two-column matrix with the number of successes and the number of failures.\n\nlibrary(agridat)\n\nseed_fit_1 &lt;- glm(cbind(germ, n - germ) ~\n                    extract + gen,\n                  data = crowder.seeds, family = binomial())\nseed_fit_2 &lt;- glm(cbind(germ, n - germ) ~\n                    extract * gen,\n                  data = crowder.seeds, family = binomial())\n\n\n3.3.1 Inference and Predictions\nCrowder Seed Example:\n\n\nShow the code\ncrowder.seeds |&gt;\n  mutate(germ_rate = germ/n) |&gt;\n  ggplot(aes(x = gen, y = germ_rate, color = extract))+\n  geom_boxplot()+\n  labs(y = 'Germination Rate', x = 'Seed Variety')+\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nseed_test &lt;- anova(seed_fit_1, seed_fit_2, test = \"Chisq\")\nseed_test\n\n\nAnalysis of Deviance Table\n\nModel 1: cbind(germ, n - germ) ~ extract + gen\nModel 2: cbind(germ, n - germ) ~ extract * gen\n  Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi)  \n1        18     39.686                       \n2        17     33.278  1   6.4081  0.01136 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nBased on the analysis of deviance table, we conclude that that extract differs by side type, \\chi^2(1) = 6.41, p = 0.011.\n\nLike with logistic regression, we can use tbl_regression() to compute the coefficients in each model. E.g., for the interaction model:\n\n\ntbl_regression(seed_fit_2, exponentiate = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nOR\n95% CI\np-value\n\n\n\n\nextract\n\n\n\n\n\n\n\n\n    bean\n—\n—\n\n\n\n\n    cucumber\n1.72\n1.05, 2.81\n0.031\n\n\ngen\n\n\n\n\n\n\n\n\n    O73\n—\n—\n\n\n\n\n    O75\n0.86\n0.56, 1.34\n0.5\n\n\nextract * gen\n\n\n\n\n\n\n\n\n    cucumber * O75\n2.18\n1.19, 3.97\n0.011\n\n\n\nAbbreviations: CI = Confidence Interval, OR = Odds Ratio\n\n\n\n\n\n\n\n\n\nWe can also compute the predictions and the standard errors. From the table, we see that the ’75 seeds with cucumber extract have the highest predicted probability of germination.\n\n\n# first we create a grid of all possible versions of the relevant regressors we want to get predictions for:\nxs &lt;- expand.grid(gen = c(\"O75\", \"O73\"),\n                  extract = c(\"bean\", \"cucumber\"))\n\npredictions &lt;- predict(seed_fit_2, newdata = xs,\n                       type = \"response\", se.fit = TRUE) # response = rate scale\n\ngt(data.frame(gen = xs$gen, extract = xs$extract,\n           pred_germ_rate = predictions$fit, se = predictions$se.fit)) |&gt;\n  fmt_number(decimals = 3)\n\n\n\n\n\n\n\ngen\nextract\npred_germ_rate\nse\n\n\n\n\nO75\nbean\n0.364\n0.029\n\n\nO73\nbean\n0.398\n0.044\n\n\nO75\ncucumber\n0.681\n0.027\n\n\nO73\ncucumber\n0.532\n0.042\n\n\n\n\n\n\n\n\n3.3.1.1 Interpretation\n\n\nShow the code\nlibrary(Sleuth3)\n\nisland_fit &lt;- glm(cbind(Extinct, AtRisk - Extinct) ~ log10(Area), data = case2101,\n    family = binomial())\n\nexp(coef(island_fit))\n\n\n(Intercept) log10(Area) \n  0.3023423   0.5045408 \n\n\nShow the code\nci &lt;- exp(confint(island_fit)['log10(Area)',])\nci\n\n\n    2.5 %    97.5 % \n0.3910622 0.6422837 \n\n\nInterpretation: a ten-fold increase in island size (i.e., an island that is 10 times larger) is associated with the odds of extinct being multiplied by 0.5 (95% CI [0.39, 0.64]) (if not on log scale, would be a one-unit increase in X)."
  },
  {
    "objectID": "model-types-2.html#poisson-regression",
    "href": "model-types-2.html#poisson-regression",
    "title": "Model Types and Diagnostics",
    "section": "3.4 Poisson Regression",
    "text": "3.4 Poisson Regression\nIf a certain event occurs with a fixed rate, and the events are independent (so that the occurrence of one event does not make another more or less likely), then the count of events over a fixed period of time will be Poisson-distributed. This makes Poisson GLMs well-suited for response variables that are .\nThe canonical link for a Poisson distribution is the log-link, meaning that under the Poisson model, we have:\n\\log(\\mathbb{E}[Y \\mid X = x]) = \\beta^\\intercal x which is equivalent to:\n\\mathbb{E}[Y \\mid X = x] = \\exp(\\beta^\\intercal x)\nThus, the variance of Y depends on X in Poisson GLMs!\n\n3.4.1 Inference and Predictions\nAnt Example:\n\n\nShow the code\n# omitted EDA but idea is to see which covariates are associated with the ant species counts in the data\n\nants_fit &lt;- glm(Srich ~ Latitude + Elevation + Habitat, data = ants,\n                family = poisson())\n\ntbl_regression(ants_fit, exponentiate = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nIRR\n95% CI\np-value\n\n\n\n\nLatitude\n0.79\n0.70, 0.89\n&lt;0.001\n\n\nElevation\n1.00\n1.00, 1.00\n0.002\n\n\nHabitat\n\n\n\n\n\n\n\n\n    Bog\n—\n—\n\n\n\n\n    Forest\n1.89\n1.50, 2.39\n&lt;0.001\n\n\n\nAbbreviations: CI = Confidence Interval, IRR = Incidence Rate Ratio\n\n\n\n\n\n\n\n\nExample Interpretation: One additional degree in latitude is associated with the mean number of ant species being multiplied by 0.79, holding all other covariates in the model constant.\nWe can also find confidence intervals by exponentiating the confidence interval based on the profile likelihood:\n\n\n                    2.5 %       97.5 %\n(Intercept)   999.8169873 2.941911e+07\nLatitude        0.6979144 8.890260e-01\nElevation       0.9981167 9.995855e-01\nHabitatForest   1.4972591 2.393810e+00\n\n\nAnother example (Howard the Duck): Increasing the fraction of time a duck spends at the park by 0.1 is associated with the mean number of hospitalizations being multiplied by 1.09 (95% CI [1.06, 1.13]), holding all other covariates in the model constant.\n\n\n3.4.2 Offsets\nOffsets are useful when the observed counts recorded for different population sizes or time periods.\nSmoking Example:\n\n\nShow the code\nsmokers &lt;- read.csv(\"blogs/DA-materials/data/smokers.csv\")\n\nsmokers |&gt;\n  group_by(age, smoke) |&gt;\n  summarize(Deaths = sum(deaths),\n            `Person-years` = sum(py)) |&gt;\n  ungroup() |&gt;\n  gt()\n\n\n\n\n\n\n\n\nage\nsmoke\nDeaths\nPerson-years\n\n\n\n\n40\nno\n2\n18790\n\n\n40\nyes\n32\n52407\n\n\n50\nno\n12\n10673\n\n\n50\nyes\n104\n43248\n\n\n60\nno\n28\n5710\n\n\n60\nyes\n206\n28612\n\n\n70\nno\n28\n2585\n\n\n70\nyes\n186\n12663\n\n\n80\nno\n31\n1462\n\n\n80\nyes\n102\n5317\n\n\n\n\n\n\n\nThe person-years column indicates that people in the smoking study were observed for different number of years and/or there were different numbers of people in the study who were in each age range.\nIf we are interested in death rate, we could model the rate directly as some function of our covariates.\n\n\nShow the code\nsmokers |&gt;\n  mutate(death_rate = deaths/py) |&gt;\n  ggplot(aes(x = age, y = death_rate))+\n  geom_point(aes(color = smoke, shape = smoke))+\n  theme_bw()\n\n\n\n\n\n\n\n\n\nIn turn, we could model the outcomes (deaths in our case) as approximately Poisson, given the death rate f(\\beta^\\intercal X):\nY \\sim \\text{Poisson}(\\exp(\\beta^\\intercal X) + \\log(P)) where P is the offset, i.e., a term in our model that is fixed to have coefficient one rather than a slope that is estimated.\nNote for deaths, we are technically approximately Poisson as the upper-bound on deaths is the number of people in our study. That said, so long as the death rate is low, the Poisson distribution will assign very little probability on impossible death numbers\nIf the model with the offset is true, then we would expect a linear relationship between \\log(Y/P) and X. We can use the offset argument in the glm() function to specify our offset:\n\nsmoke_fit &lt;- glm(deaths ~ (age + I(age^2)) * smoke,\n                 offset = log(py), #offset!\n                 data = smokers, family = poisson(link = \"log\"))\n\nFurther, we can use deviance tests with offsets\n\n\nAnalysis of Deviance Table\n\nModel 1: deaths ~ age + smoke\nModel 2: deaths ~ (age + I(age^2)) * smoke\n  Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi)    \n1         7     69.182                         \n2         4      1.246  3   67.936 1.18e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nRisk Ratio Calculation:\nNow suppose we want to compare smokers and non-smokers at age 40, calculating a risk ratio: the ratio of risk of death for smokers versus non-smokers. A simple way would be to make a prediction and take the ratio. To predict the rate of death, we can predict the mean number of deaths for one person-year:\n\n\nShow the code\nnew_smokers &lt;- data.frame(smoke = c(\"no\", \"yes\"),\n                          age = c(40, 40),\n                          py = c(1, 1))\npreds &lt;- predict(smoke_fit, newdata = new_smokers, type = \"response\")\npreds[2] / preds[1]\n\n\n       2 \n3.927567 \n\n\nShow the code\n# or with:\ncoef_vec &lt;- c(0, 0, 0, 1, 40, 1600)\n\nexp(sum(coef_vec * coef(smoke_fit)))\n\n\n[1] 3.927567\n\n\nThe standard errors are not independent. We can create the Wald confidence interval by leveraging the fact that the variance and mean are equal in Poisson models.\n\n\nShow the code\nse &lt;- sqrt(t(coef_vec) %*% vcov(smoke_fit) %*% coef_vec)[1,1]\n\nbounds &lt;- sum(coef_vec * coef(smoke_fit)) + c(-2, 2) * se\nexp(bounds)\n\n\n[1]  1.49177 10.34059\n\n\nThe quite a large confidence interval is indicative of the difficulty in estimating risk ratios when the underlying rate of events is so low."
  },
  {
    "objectID": "model-types-2.html#overdispersion",
    "href": "model-types-2.html#overdispersion",
    "title": "Model Types and Diagnostics",
    "section": "3.5 Overdispersion",
    "text": "3.5 Overdispersion\nOverdispersion is when there is more variance in Y than the response distribution would predict. This could be due to:\n\nInsufficient predictors. That is, there might be other factors associated with the expected value of Y that we do not observe\nThere might be correlations we did not account for (e.g., a binomial distribution assumes the n trials are independent but what if success in one is correlated with increased success in the others?)\n\nA remedy for overdispersion is to use quasi-likleihood. These can be fit using quasibinomial() and quasipoisson() families in the glm() function. Compared to non-quasi models, the estimates of the coefficients will be the same, only the confidence intervals differ, witht heir width expanding by a constant factor\nWe can check for overdispersion using Q-Q plot of randomized quantile residuals\nExample: returning to the seed data:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nOR\n95% CI\np-value\n\n\n\n\nextract\n\n\n\n\n\n\n\n\n    bean\n—\n—\n\n\n\n\n    cucumber\n1.72\n0.88, 3.37\n0.13\n\n\ngen\n\n\n\n\n\n\n\n\n    O73\n—\n—\n\n\n\n\n    O75\n0.86\n0.48, 1.58\n0.6\n\n\nextract * gen\n\n\n\n\n\n\n\n\n    cucumber * O75\n2.18\n0.96, 4.94\n0.080\n\n\n\nAbbreviations: CI = Confidence Interval, OR = Odds Ratio\n\n\n\n\n\n\n\n\nImportant Note: since overdispersed models no longer specify a distribution of the outcome conditional on the covariates, tests based on the log-likelihood are no longer valid and we can no longer use randomized quantile residuals or do line-ups.\nWe can still still do Wald Tests and interpret coefficients using the new Confidence Interval\nEstimating the dispersion parameter…\n\nsum(residuals(ants_fit, type = 'deviance')^2)/ants_fit$df.residual\n\n[1] 1.017261"
  },
  {
    "objectID": "model-types-2.html#outliers-1",
    "href": "model-types-2.html#outliers-1",
    "title": "Model Types and Diagnostics",
    "section": "3.6 Outliers",
    "text": "3.6 Outliers\nWe can approximate Cook’s distance fairly well to estimate the influence of each observation on our coefficient estimates, using augment just like before.\nWe could also likely remove the outliers and see if it qualitatively changes our findings if we aren’t sure.\n\n\nShow the code\naugment(ants_fit) |&gt;\n  ggplot(aes(x = .fitted, y = .cooksd))+\n  geom_point()+\n  theme_bw()"
  },
  {
    "objectID": "model-types-2.html#ridge-regression",
    "href": "model-types-2.html#ridge-regression",
    "title": "Model Types and Diagnostics",
    "section": "5.1 Ridge Regression",
    "text": "5.1 Ridge Regression\nRidge Regression penalizes the L-2 norm of the covariates. We can implement Ridge Regression using the glmnet package, which requires us to provide X as a matrix and Y as a vector of responses. Note that glmnet() standardizes each column to have variance 1 automatically.\n[For Ridge Regression, \\alpha = 0]\n\n\nShow the code\nlibrary(mvtnorm)\n\nsparse_pop &lt;- population(\n  x = predictor(rmvnorm,\n                mean = rep(0, 100),\n                sigma = diag(100)),\n  y = response(4 + 0.2 * x1 - 5 * x2 + x3 + 0.4 * x4 + 8 * x5,\n               error_scale = 1)\n)\n\nsparse_samp &lt;- sparse_pop |&gt;\n  sample_x(n = 90) |&gt;\n  sample_y()\n\nx &lt;- model.matrix(~ . - 1 - y, data = sparse_samp)\n\nridge_fit &lt;- glmnet(x, sparse_samp$y, alpha = 0)\n\nplot(ridge_fit)\n\n\n\n\n\n\n\n\n\nTo determine which penalty parameter to select, we can use cross-validation via cv_glmnet() (which by default does 10-fold CV).\n\n\nShow the code\ncv_results &lt;- cv.glmnet(x, sparse_samp$y, alpha = 0)\ncv_results$lambda.min\n\n\n[1] 82.11175\n\n\nShow the code\n# if we want to plot to see what is happening:\nplot(cv_results)\n\n\n\n\n\n\n\n\n\nWhile the coefficients of Ridge Regression are not fully interpretable, we can look at which predictors have the largest magnitudes to get a sense of the relative effect size between different covariates and our outcome variable.\n\n\nShow the code\ncoefs &lt;- coef(ridge_fit, s = cv_results$lambda.min)\ncoefs[order(abs(coefs[, 1]), decreasing = TRUE)[1:5], ]\n\n\n(Intercept)          x5          x2         x82         x29 \n  3.8773665   0.8550939  -0.5844830  -0.2314598  -0.2218641 \n\n\nRidge Regression is particularly helpful when there are collinear predictors – which result in high prediction variance (encourages the effects to be ``shared” between collinear predictors). So, Ridge Regression reduces the variance of predictions."
  },
  {
    "objectID": "model-types-2.html#lasso",
    "href": "model-types-2.html#lasso",
    "title": "Model Types and Diagnostics",
    "section": "5.2 Lasso",
    "text": "5.2 Lasso\nLasso is another penalized regression model. Its most useful propoerty is that (depending on the value of the penalization parameter), it forces many of the coefficient estimates to be exactly zero. That is, Lasso promotes sparsity, which matches real-world populations where we expect the true coefficients to be sparse (think: genetics)\nWe can implement Lasso with glmnet (the only difference is now we set \\alpha = 1).\n\n\nShow the code\nlasso_fit &lt;- glmnet(x, sparse_samp$y, alpha = 1)\n\nplot(lasso_fit)\n\n\n\n\n\n\n\n\n\nLike before, we can also cross-validate:\n\n\nShow the code\ncv_results &lt;- cv.glmnet(x, sparse_samp$y, alpha = 1)\ncv_results$lambda.min\n\n\n[1] 0.1434926\n\n\nShow the code\n# and plot if we want a bitter idea of what is happening\nplot(cv_results)\n\n\n\n\n\n\n\n\n\nand, we also can look at the coefficients with the largest magnitude like before:\n\n\nShow the code\ncoefs &lt;- coef(lasso_fit, s = cv_results$lambda.min)\ncoefs[order(abs(coefs[, 1]), decreasing = TRUE)[1:15], ]\n\n\n         x5          x2 (Intercept)          x3         x63         x94 \n 7.96584001 -5.13112348  3.97693917  0.77328053 -0.12880254 -0.12133145 \n        x98          x4         x20         x91         x78         x44 \n 0.10029135  0.08173381  0.06660953 -0.06330502 -0.06226934 -0.05644002 \n        x99         x15          x9 \n 0.04719759  0.04191703 -0.03643751 \n\n\nImportantly, it can be shown that if we know the right penalization value, the model selection by Lasso is model selection consistent (if chooses all and only the true non-zero coefficient with probability 1 as the sample size tends to infinity).\nWhile we choose our penalization value based on cross-validation, so model selection consistency might not hold."
  },
  {
    "objectID": "model-types-2.html#the-elastic-net",
    "href": "model-types-2.html#the-elastic-net",
    "title": "Model Types and Diagnostics",
    "section": "5.3 The Elastic Net",
    "text": "5.3 The Elastic Net\nElastic net generalizes the case of Ridge Regression and Lasso by allowing both \\alpha and \\lambda to be hyperparameters, the combination of which trades off the benefits of Ridge Regression and Lasso. That is, with a \\alpha closer to one, more sparsity is induced, but with a \\alpha closer to 0, variance from collinearity is reduced more.\nTo cross-validate over both lambda and alpha, you have to manually loop over alpha values yourself, and run cv.glmnet() inside that loop."
  },
  {
    "objectID": "model-types-2.html#lasso-model-example-on-genetic-data",
    "href": "model-types-2.html#lasso-model-example-on-genetic-data",
    "title": "Model Types and Diagnostics",
    "section": "5.4 Lasso Model Example (on Genetic Data)",
    "text": "5.4 Lasso Model Example (on Genetic Data)\n\nset.seed(47)\ngenedat &lt;- read_csv('blogs/DA-materials/data/genedat-exam.csv')\n\nx &lt;- model.matrix(Disease ~ . -1 -CaseId -X, data = genedat) \nY &lt;- if_else(genedat$Disease == 'control', 0, 1)\n\ncv_genedat &lt;- cv.glmnet(x, Y, alpha = 1, family = \"binomial\") \n#ideally would have done group cv here\n\nfit_genedat &lt;- glmnet(x, Y, family = \"binomial\", alpha = 1)\n\ncoefs &lt;- coef(fit_genedat, s = cv_genedat$lambda.min)\n\nresults_df &lt;- data.frame(coeff_est = coefs[,1]) |&gt;\n  rownames_to_column('covariate') |&gt;\n  filter(coeff_est != 0, !grepl('Intercept', covariate)) |&gt;\n  arrange(desc(abs(coeff_est))) |&gt;\n  slice_head(n = 10) |&gt; \n  gt() |&gt;\n  fmt_number(decimals = 2) |&gt;\n  cols_align_decimal(coeff_est)\n  \n# ignoring within person correlation for now, but would likely want to  subset data to just one cortex area especially since there isn't two observations (could also do mixed effects approach as a future step). Also, if goal is prediction, we would want to have test and train split sets -- probably by group to avoid data leakage\n\n\nset.seed(123)\nfolds &lt;- group_vfold_cv(genedat, group = CaseId, v = 5)\nlasso_errors &lt;- sapply(folds$splits, function(split) {\n  train &lt;- analysis(split)\n  train_x &lt;- model.matrix(~ . -1 -X -Disease -CaseId, data = train)\n  \n  test &lt;- assessment(split)\n  test_x &lt;- model.matrix( ~ . -1 -X -Disease -CaseId, data = test)\n  \n  fit_genedat &lt;- glmnet(train_x, train$Disease, family = \"binomial\", alpha = 1)\n  \n  pred &lt;- predict(fit_genedat, newx = test_x, s = cv_genedat$lambda.min,\n                   type = 'response')\n  \n  pred_class &lt;- if_else(pred &gt;= 0.5, 'autism', 'control')\n  \n  mean(pred_class == test$Disease)\n})\n\nmean(lasso_errors)\n\n[1] 0.7375425"
  },
  {
    "objectID": "model-types-2.html#splitting-the-data",
    "href": "model-types-2.html#splitting-the-data",
    "title": "Model Types and Diagnostics",
    "section": "6.1 Splitting the Data",
    "text": "6.1 Splitting the Data\n\n6.1.1 General Data Splitting\n\nset.seed(47)\ncar_split &lt;- initial_split(mtcars) #default is 3:1 training-test split\ntrain_data &lt;- training(car_split)\ntest_data &lt;- testing(car_split)\n\n\n\n6.1.2 With Stratification\nIncluding a strata is helpful, particularly if there is class imbalance in our covariates or outcome to the point where it is possible that training and/or test won’t have the same levels of the variables\n\ncar_split &lt;- initial_split(mtcars, strata = 'cyl')\ntrain_data &lt;- training(car_split)\ntest_data &lt;- testing(car_split)\n\n\n\n6.1.3 By Time\n\ncovid_hew &lt;- read_csv('blogs/DA-materials/data/covidcast-hew.csv')\n\ntraining &lt;- covid_hew |&gt;\n  mutate(year_value = year(time_value),\n         month_value = month(time_value)) |&gt;\n  filter(month_value == 1)\n\ntesting &lt;- covid_hew |&gt;\n  mutate(year_value = year(time_value),\n         month_value = month(time_value)) |&gt;\n  filter(month_value == 3)\n\n\n\n6.1.4 By Group\nHelpful if there is correlation between observations which would lead to data leakage concerns if we do not split the data by group.\n\ncar_split &lt;- group_initial_split(mtcars, group = cyl)\ntrain_data &lt;- training(car_split)\ntest_data &lt;- testing(car_split)"
  },
  {
    "objectID": "model-types-2.html#nested-cross-validation",
    "href": "model-types-2.html#nested-cross-validation",
    "title": "Model Types and Diagnostics",
    "section": "6.2 Nested Cross Validation",
    "text": "6.2 Nested Cross Validation\nOften, we might need to do cross-validation and want to keep a hold out set that we can test our model on later. Here would be a a framework for nested cross validation:\n\nSplit the full dataset into testing and training (we might do this randomly, by time, by group, etc.)\nOn the training data do k-fold cross validation to select any needed parameters (e.g., penalization parameters)\nFit the model on the full training data with the selected tuning parameter values\nAssess predictive performance on the test data"
  },
  {
    "objectID": "model-types-2.html#useful-metrics",
    "href": "model-types-2.html#useful-metrics",
    "title": "Model Types and Diagnostics",
    "section": "6.3 Useful Metrics",
    "text": "6.3 Useful Metrics\nReport on test data but could be interesting to compare test performance to how it performs on training data\n\nIf test data performance is same or better than training data, this indicates that the model is underfit/biased\nIf the test data performance is much lower than the training data, this might raise concerns about overfitting\n\nFor regression:\n\nMean-Squared Error (MSE)\nRoot Mean-Squared Error (RMSE): a measure of the average magnitude of the errors between predicted and actual values in a regression model\nActual versus Predicted Outcome Plot (ideal would be along the main diagonal)\n\nFor classification:\n\nCalibration plot\nConfusion matrix\n\nFalse Negative Rate, False Positive Rate, Specificity, Sensitivity\n\nROC Curve (AUC)\nAccuracy\n\nNote: when looking at accuracy, always report what the base incidence rate is (this would be the accuracy of a null model)."
  },
  {
    "objectID": "blogs/DA-exam-materials 2.html",
    "href": "blogs/DA-exam-materials 2.html",
    "title": "Data Analysis Exam Thoughts and Materials",
    "section": "",
    "text": "This Spring, at the end of my first-year, I took the Data Analysis (DA) exam, which is the “qualifying” exam for CMU’s Statistics Ph.D. program. This notebook includes the materials I made in preparation for the DA exam. The majority of these materials are based on the Fall 2024 Regression Analysis (36-707) lecture notes, which can be found here.1\nA little background on CMU’s DA Exam from the Statistics Graduate Student Handbook: “At the conclusion of each Spring Semester the Department administers the ‘Data Analysis Exam,’ which is designed to test students’ ability to apply statistical methods to address a substantive, real problem. Students are given eight hours to complete the exam, during which time they analyze the data and write a [ten-page] report to present their analysis and conclusions. The faculty are realistic as to what can be accomplished during the eight-hour period. In grading the exam, the faculty are looking for clear presentation of an appropriate analysis of the data. Emphasis is not placed on technical or mathematical sophistication,” (p. 13)."
  },
  {
    "objectID": "blogs/DA-exam-materials 2.html#footnotes",
    "href": "blogs/DA-exam-materials 2.html#footnotes",
    "title": "Data Analysis Exam Thoughts and Materials",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThanks to Alex Reinhart for compiling these!↩︎\nI did an Executive Summary + IMRAD (Introduction, Methods, Results, and Discussion) report for the exam.↩︎"
  },
  {
    "objectID": "recipes.html",
    "href": "recipes.html",
    "title": "Data Analysis Recipes",
    "section": "",
    "text": "For the linear regression model \\(\\mathbf{Y} = \\mathbf{X}\\beta + e\\), we have the following assumptions:\n\n\nErrors have mean 0: \\(\\mathbb{E}[Y \\mid X] = 0\\)\n\nThe error variance is constant: \\(\\text{Var}[e \\mid \\mathbf{X}] = \\sigma^2\\)\n\nThe errors are uncorrelated (the data points are iid): \\(\\text{Var}[e \\mid \\mathbf{X}] = \\sigma^2\\)\n\nThe errors are normally distributed\n\nSteps:\n\nFit model (consider interactions, non-linear patterns seen in EDA, confounding variables, transformations)\n\nPartial residual analysis to account for any remaining non-linearity, adjusting the model as needed [diagnostic for non-linearity]\n\nPlot residuals against fitted values; look for heteroskedasticity and use sandwich estimator for all inference if present [diagnostic for constant error variance and errors have mean zero]\n\nPlot Q-Q plot to check if there are any gross deviations from normality [diagnostic for normally distributed errors; not a big deal if the sample size is large]\nCheck for how outliers identified during EDA impact on coefficients via Cook’s Distance\n\nCheck for collinearity issues using EDA and VIF scores\n\nIf we included spline or polynomial terms, conduct an F test and create effect plots, with example interpretations that include confidence intervals\n\nOtherwise, for linear terms, conduct a t test and provide test statistic, p-value, and confidence interval\n\nIf sample size is small, make a statement about power\n\nLimitations:\n\nUnaccounted for correlation/repeated measures (lack of independence)\nUnmeasured confounders\nDichotomized confounders\nNon-random missingness\nSample demographics versus population of interest\nStatistical versus practical significance"
  },
  {
    "objectID": "recipes.html#linear-regression",
    "href": "recipes.html#linear-regression",
    "title": "Data Analysis Recipes",
    "section": "",
    "text": "For the linear regression model \\(\\mathbf{Y} = \\mathbf{X}\\beta + e\\), we have the following assumptions:\n\n\nErrors have mean 0: \\(\\mathbb{E}[Y \\mid X] = 0\\)\n\nThe error variance is constant: \\(\\text{Var}[e \\mid \\mathbf{X}] = \\sigma^2\\)\n\nThe errors are uncorrelated (the data points are iid): \\(\\text{Var}[e \\mid \\mathbf{X}] = \\sigma^2\\)\n\nThe errors are normally distributed\n\nSteps:\n\nFit model (consider interactions, non-linear patterns seen in EDA, confounding variables, transformations)\n\nPartial residual analysis to account for any remaining non-linearity, adjusting the model as needed [diagnostic for non-linearity]\n\nPlot residuals against fitted values; look for heteroskedasticity and use sandwich estimator for all inference if present [diagnostic for constant error variance and errors have mean zero]\n\nPlot Q-Q plot to check if there are any gross deviations from normality [diagnostic for normally distributed errors; not a big deal if the sample size is large]\nCheck for how outliers identified during EDA impact on coefficients via Cook’s Distance\n\nCheck for collinearity issues using EDA and VIF scores\n\nIf we included spline or polynomial terms, conduct an F test and create effect plots, with example interpretations that include confidence intervals\n\nOtherwise, for linear terms, conduct a t test and provide test statistic, p-value, and confidence interval\n\nIf sample size is small, make a statement about power\n\nLimitations:\n\nUnaccounted for correlation/repeated measures (lack of independence)\nUnmeasured confounders\nDichotomized confounders\nNon-random missingness\nSample demographics versus population of interest\nStatistical versus practical significance"
  },
  {
    "objectID": "recipes.html#logistic-regression",
    "href": "recipes.html#logistic-regression",
    "title": "Data Analysis Recipes",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nFor the logistic regression model \\(\\log(\\text{odds}(\\mathbf{Y})) = \\mathbf{X}\\beta\\), we have the following assumptions:\n\n\nLog-odds of \\(Y\\) are linearly related to the regressors \\(X\\)\n\nThe observations \\(Y_i\\) are conditionally independent of each other given the covariates \\(X_i\\)\n\nSteps\n\nFit model (consider interactions, non-linear patterns seen in EDA, confounding variables, transformations)\n\nPartial residual analysis to account for any remaining non-linearity with log-odds, adjusting the model as needed [diagnostic for non-linearity between log-odds and regressors]\n\nPlot randomized quantile residuals and corresponding Q-Q plot (with uniform distribution) for each predictor [second diagnostic for non-linearity between log-odds and regressors]\n\nOptionally, create a calibration plot and randomized quantile residuals against fitted values\n\nCheck for how outliers identified during EDA impact on coefficients via Cook’s Distance\n\nCheck for collinearity issues using EDA and VIF scores\n\nIf we included spline or polynomial terms, conduct a deviance test and create effect plots, with example interpretations that include confidence intervals (ensure it is on response scale, i.e., not log-odds)\n\nOtherwise, for linear terms, conduct a Wald test and provide test statistic (\\(z\\)), p-value, and confidence interval (exponentiate!)\n\nIf sample size is small, make a statement about power\n\nLimitations:\n\nUnaccounted for correlation/repeated measures (lack of independence)\nUnmeasured confounders\nDichotomized confounders\nNon-random missingness\nSample demographics versus population of interest\nStatistical versus practical significance\nIf cohort study think about which type: prospective, retrospective, case-control"
  },
  {
    "objectID": "recipes.html#binomial-regression",
    "href": "recipes.html#binomial-regression",
    "title": "Data Analysis Recipes",
    "section": "Binomial Regression",
    "text": "Binomial Regression\nFor the binomial regression model \\(n_iY_i \\mid X_i = x_i \\sim \\text{Binomial}(n_i, \\log(\\text{odds}(x_i\\beta)))\\). We use binomial regression when we have a fixed number of trials, with each observation consisting of a number of successes and total number of trials (so we can infer failures). In binomial regression, we have the following assumptions:\n(1)The observations are conditionally independent given \\(X\\)\n(2) The response variable follows a binomial distribution\n(3) The mean of the response variable is related to the predictors through the logit link and functional form [We want a linear relationship between the predictor and the log-odds of the rate, \\(Y_i\\)]:\n\\[\\log \\left( \\frac{rate}{1 - rate} \\right)\\]\nSteps:\n\nFit model (consider interactions, non-linear patterns seen in EDA, confounding variables, transformations)\n\nPartial residual analysis to account for any remaining non-linearity with log-odds, adjusting the model as needed [diagnostic for non-linearity between log-odds and regressors]\n\nPlot randomized quantile residuals and corresponding Q-Q plot (with uniform distribution) for each predictor [second diagnostic for non-linearity between log-odds and regressors]\n\nCheck for overdispersion using Q-Q plot from fitted. If there is evidence of overdispersion, move to quasi-GLM section\nCheck for how outliers identified during EDA impact on coefficients via Cook’s Distance\n\nCheck for collinearity issues using EDA and VIF scores\n\nIf we included spline or polynomial terms, conduct a deviance test and create effect plots, with example interpretations that include confidence intervals (ensure it is on response scale, i.e., not log-odds)\nOtherwise, for linear terms, conduct a Wald test and provide test statistic (\\(z\\)), p-value, and confidence interval (exponentiate!)\n\nIf sample size is small, make a statement about power\n\nLimitations:\n\nUnaccounted for correlation (spatiotemporal), dependence of successive trials (e.g., success in one, increases probability of success in another)\nUnmeasured confounders\nDichotomized confounders\nNon-random missingness\nSample demographics versus population of interest\nStatistical versus practical significance\nIf cohort study think about which type: prospective, retrospective, case-control"
  },
  {
    "objectID": "recipes.html#poisson-regression",
    "href": "recipes.html#poisson-regression",
    "title": "Data Analysis Recipes",
    "section": "Poisson Regression",
    "text": "Poisson Regression\nWhen certain event that occurs with a fixed rate, and the events are independent (so that the occurrence of one event does not make another more or less likely), then the count of events over a fixed period of time will be Poisson-distributed. For the Poisson regression model \\(\\mathbb{E}[Y \\mid X = x] = \\exp(\\beta^\\intercal x)\\), we have the following assumptions:\n(1)The observations are conditionally independent given \\(X\\)\n(2) The response variable follows a Poisson distribution\n(3) The mean of the response variable is related to the predictors through the log link and functional form [We want a linear relationship between the predictor and the log mean of the outcome, either rate or count]\nSteps:\n\nFit model (consider interactions, non-linear patterns seen in EDA, confounding variables, transformations)\n(1b) Consider whether an offset is needed. Used in cases where the counts are from different time period lengths or popualtion sizes\n\nPartial residual analysis to account for any remaining non-linearity with log mean outcome, adjusting the model as needed [diagnostic for non-linearity between log mean outcome and regressors]\n\nPlot randomized quantile residuals and corresponding Q-Q plot (with uniform distribution) for each predictor [second diagnostic for non-linearity between log mean outcome and regressors]\n\nCheck for overdispersion using Q-Q plot from fitted. If there is evidence of overdispersion, move to quasi-GLM section\n\nCheck for how outliers identified during EDA impact on coefficients via Cook’s Distance\n\nCheck for collinearity issues using EDA and VIF scores\n\nIf we included spline or polynomial terms, conduct a deviance test and create effect plots, with example interpretations that include confidence intervals (ensure it is on response scale, i.e., not log scale)\nOtherwise, for linear terms, conduct a Wald test and provide test statistic (\\(z\\)), p-value, and confidence interval (exponentiate!)\n\nIf sample size is small, make a statement about power\n\nLimitations:\n\nUnaccounted for correlation (spatiotemporal), dependence between events, and non-fixed rate of occurence\nInsufficient data to include an offset term\nUnreasonable approximation of count data as Poisson – could assign non-trivial probability to impossible counts\nUnmeasured confounders\nDichotomized confounders\nNon-random missingness\nSample demographics versus population of interest\nStatistical versus practical significance\nIf cohort study think about which type: prospective, retrospective, case-control"
  },
  {
    "objectID": "recipes.html#quasi-glm-regression",
    "href": "recipes.html#quasi-glm-regression",
    "title": "Data Analysis Recipes",
    "section": "Quasi-GLM Regression",
    "text": "Quasi-GLM Regression\nQuasi-GLM models are when there is overdispersion (or underdispersion) in the standard Binomial or Poisson regression model.\nNotably, overdispersion is when there is more variance in \\(Y\\) than the response distribution would predict. This could be due to:\n\nInsufficient predictors. That is, there might be other factors associated with the expected value of \\(Y\\) that we do not observe\nThere might be correlations we did not account for (e.g., a binomial distribution assumes the \\(n\\) trials are independent but what if success in one is correlated with increased success in the others?)\n\nSteps:\n\nFit the quasi-GLM model (using the regressors determined from the standard Binomial or Poisson regression model)\n\nConfirm linear relationships/goodness-of-fit with partial residual plot (should be same to above with perhaps wider variance bands)\n\nBased we no have a proper likelihood function, we cannot check our model with randomized quantile residuals\nCheck for how outliers identified during EDA impact on coefficients via Cook’s Distance\n\nCheck for collinearity issues using EDA and VIF scores\n\nIf we included spline or polynomial terms, we can no longer conduct deviance tests. We can still use effect plots with example interpretations that include confidence intervals (ensure it is on response scale, i.e., not log-odds or log scale)\nOtherwise, for linear terms, conduct a Wald test and provide test statistic (\\(z\\)), p-value, and confidence interval (exponentiate!); these coefficient estimates should be the same but the confidence interval should have a different width\n\nIf sample size is small, make a statement about power\n\nLimitations:\n\nCannot test for significance of spline or polynomial terms\nUnaccounted for correlation (spatiotemporal), dependence of successive trials (e.g., success in one, increases probability of success in another), dependence between events, and non-fixed rate of occurence\nInsufficient data to include an offset term\nUnreasonable approximation of count data as Poisson – could assign non-trivial probability to impossible counts\nUnmeasured confounders\nDichotomized confounders\nNon-random missingness\nSample demographics versus population of interest\nStatistical versus practical significance\nIf cohort study think about which type: prospective, retrospective, case-control"
  },
  {
    "objectID": "recipes.html#prediction",
    "href": "recipes.html#prediction",
    "title": "Data Analysis Recipes",
    "section": "Prediction",
    "text": "Prediction\nWe want to use ridge regression for high-dimensional data, where we have highly collinear predictors (since it promites sharing effects) and lasso for high-dimensional data, where we want sparsity.\nSteps:\n\nSplit the data into test and training sets trying as much as possible to avoid data leakage (e.g., keep repeated measures together if possible)\n\nCreate model matrix of covariates for testing and training data\n\nCross validate to select penalization parameter \\(\\lambda\\) using the training data\n\nFit model on full training data using cross-validated penalization parameter – make sure to specify the correct distribution family based on outcome. Note it is gaussian by default.\n(4b) If doing classification, pick the threshold for positive versus negative class using ROC curve where it is closest to top left\n\nPredict on the test data and calculate (R)MSE to assess overall predictive performance (if binary compare accuracy to incidence rate)\n\n\nRMSE: a measure of the average magnitude of the errors between predicted and actual values in a regression model\n\n\nIf classification, also compute sensitivity and specificity and/or AUC to show relative performance on positive and negative classes\n\nIf continuous outcome, plot actual versus predicted values (ideal predictive model would like along slope = 1, intercept = 0 line)"
  },
  {
    "objectID": "about 2.html",
    "href": "about 2.html",
    "title": "About",
    "section": "",
    "text": "I am a Ph.D. student in Carnegie Mellon’s Statistics and Data Science department. Before that, I attended Pomona College in Claremont, California, where I graduated with a double major in mathematics and philosophy in Spring 2024. At Pomona, I primarily conducted research with Jo Hardin, working on projects in both statistical genomics and data science ethics pedagogy.\nOutside of research, I really enjoy teaching. So far, I have been a graduate TA for two courses at CMU, and I will TA for the Carnegie Mellon Sports Analytics Summer Camp and co-instruct some lectures on data analysis with R as part of SURE 2025 this summer.\nAnother primary interest of mine is philosophy, particularly (feminist) philosophy of science, ethics, and (social) epistemology. I hope to stay engaged in philosophy and, with that, work to understand the ethical dimensions of statistical technologies while completing my Ph.D. at Carnegie Mellon."
  },
  {
    "objectID": "about 2.html#current-research-projects-may-2025",
    "href": "about 2.html#current-research-projects-may-2025",
    "title": "About",
    "section": "Current Research Projects (May 2025)",
    "text": "Current Research Projects (May 2025)\n\nPredicting Avoidance Ties in Avoidance Networks Leveraging only the Positive Network between the Same Individuals and Basic Node-Level Characteristics (Advanced Data Analysis project, in collaboration with Nynke Niezink and Eva Jaspers)\nSelecting Chip-Seq Normalization Methods from the Perspective of their Technical Conditions (in collaboration with Jo Hardin and Danae Schulz)\nAnalyzing Statistics Students’ Writing Before and After the Emergence of Large Language Models (in collaboration with my cohort-mate, Erin Franke)\nTeaching Data Cleaning and Wrangling with the data.table R Package (also with Erin Franke)"
  },
  {
    "objectID": "about 2.html#education",
    "href": "about 2.html#education",
    "title": "About",
    "section": "Education",
    "text": "Education\n Ph.D. in Statistics (in progress); Carnegie Mellon University\n B.A. in Mathematics and Philosophy (May 2024); Pomona College"
  },
  {
    "objectID": "about 2.html#contact",
    "href": "about 2.html#contact",
    "title": "About",
    "section": "Contact",
    "text": "Contact\n scolando@andrew.cmu.edu"
  },
  {
    "objectID": "teaching.html#education-related-research",
    "href": "teaching.html#education-related-research",
    "title": "Teaching",
    "section": "Education-Related Research",
    "text": "Education-Related Research\n\nAnalyzing Statistics Students’ Writing Before and After the Emergence of Large Language Models. Joint work with Erin Franke and Alex Reinhart. Presented work at the Research Satellite poster session during the United States Conference on Teaching Statistics, July 2025. We are currently working on the paper!\nTeaching Data Cleaning and Wrangling with R’s data.table Package. Joint work with Erin Franke. Presented at Posters and Beyond during United States Conference on Teaching Statistics, July 2025.\nPhilosophy within Data Science Ethics Courses. Joint work with Jo Hardin. Published in the Journal of Statistics and Data Science Education, 32(4), 2024."
  },
  {
    "objectID": "research/avoidance-ties/avoidance-ties.html",
    "href": "research/avoidance-ties/avoidance-ties.html",
    "title": "Why Do Students Avoid Each Other? Investigating Positive and Negative Network Effects",
    "section": "",
    "text": "Overview: Negative ties can greatly influence both positive network dynamics and the formation of beliefs and behaviors in social networks. Examples of negative ties include avoidance, dislike, bullying, and aggression. While there is a broad literature on bullying, less is known about what drives people in a group to avoid each other. Yet, avoidance is a particularly interesting type of relation, since it is often more frequent and less socially complex than aggression or bullying. In this project, we study structural dependencies in classroom avoidance networks and how a student’s tendency to avoid others and be avoided relates to their position in the friendship network using data from 193 Dutch classrooms. Fitting an exponential random graph model (ERGM), we find that avoidance networks are highly associated with friendship networks and exhibit strong reciprocity and degree-related endogenous effects. Finally, our conclusions based on the average marginal effects of ERGM parameters do not differ substantively from those based on the original ERGM parameter estimates, contrary to previous findings.\nWe are currently working to account for the heterogeneity in the structure of classroom avoidance networks in our dataset by using a Bayesian hierarchical ERGM, which allows each classroom to have its own ERGM parameters that deviate from the population-level parameters.\n  Advanced Data Analysis Report (December 2025)"
  }
]